
# Single node
Create the relative file to set up a single node using docker. It is for study only, do not use it for your production set up.
:::info
+ This architect is designed for single node using docker
+ Require to install docker
:::

## Config
### Dockerfile
In this Dockerfile, there are a few files to copy:
+ **slurm.conf.template**: The template for render `slurm.conf` that will be shared and consitent between all nodes.
+ **slurmdbd.conf**: The config on the slurm controller node, where it store the config of the slurmdbd for accessing and managing job via database
+ **run.sh**: The cmd to launch all service when docker image start


What is does it this:
+ Create a user called `river` will be used as normal user
+ Install slurmdbd, mysql for database
+ Munge for authentication between nodes
+ Optional: Install openssh server that can similar to login from a public network

```bash
FROM ubuntu:20.04
ENV DEBIAN_FRONTEND=noninteractive

# Install necessary packages
RUN apt-get update && apt-get install -y \
    munge slurm-wlm slurm-wlm-basic-plugins slurmdbd \
    mariadb-server mariadb-client \
    build-essential \
    openssh-server net-tools \
    python3 python3-dev python3-pip python3-venv \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Create user for the cluster and setup SSH
RUN useradd -m -s /bin/bash river && echo "river:password" | chpasswd && \
    usermod -aG sudo river

# Setup system directories
RUN mkdir -p /run/sshd /etc/munge /var/log/munge /var/lib/munge /etc/slurm /var/spool/slurmd \
    && chown -R munge:munge /etc/munge /var/log/munge /var/lib/munge \
    && chmod 0700 /etc/munge /var/log/munge /var/lib/munge \
    && /usr/sbin/create-munge-key \
    && mkdir -p /var/log/slurm /var/run/mysqld \
    && chown -R mysql:mysql /var/lib/mysql /var/run/mysqld

# Copy configuration files
COPY slurm.conf.template /etc/slurm-llnl/slurm.conf.template
COPY slurmdbd.conf /etc/slurm-llnl/slurmdbd.conf
COPY my.cnf /etc/mysql/my.cnf

# Modify system user slurm
RUN usermod -s /bin/bash slurm
COPY run.sh /opt
# Expose necessary ports
EXPOSE 22
CMD "/opt/run.sh"
```

### slurm.conf.template
:::info
+ This one is for single node, the dynamic config will be cpus and memory only
+ For multiple nodes, the config can be more customize on the patition, nodename
+ Include the worker nodes, 
:::

```
# slurm.conf for a single-node Slurm cluster with accounting
ClusterName=localcluster
SlurmctldHost=localhost
MpiDefault=none
ProctrackType=proctrack/linuxproc
ReturnToService=2
SlurmctldPidFile=/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/lib/slurm-llnl/slurmd
SlurmUser=slurm
StateSaveLocation=/var/lib/slurm-llnl/slurmctld
SwitchType=switch/none
TaskPlugin=task/none
AuthType=auth/munge

# TIMERS
InactiveLimit=0
KillWait=30
MinJobAge=300
SlurmctldTimeout=120
SlurmdTimeout=300
Waittime=0

# SCHEDULING
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core

# ACCOUNTING (uses slurmdbd)
AccountingStorageHost=localhost
AccountingStorageType=accounting_storage/slurmdbd
AccountingStoragePort=6819
JobCompType=jobcomp/none
JobAcctGatherType=jobacct_gather/linux
JobAcctGatherFrequency=30

# LOGGING
SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=/var/log/slurm/slurmd.log
# COMPUTE NODES (Single-node configuration)
NodeName=localhost CPUs=<<CPUS>> Sockets=1 CoresPerSocket=<<CPUS>> ThreadsPerCore=1 RealMemory=<<MEMORY>> State=UNKNOWN

# PARTITION CONFIGURATION
PartitionName=LocalQ Nodes=ALL Default=YES MaxTime=INFINITE State=UP
```

### slurmdbd.conf
:::info
+ It will store the secrets in  database with user and password
+ The secrets will be matched when we use `run.sh` to generate the realative database later
:::
```
PidFile=/run/slurmdbd.pid
LogFile=/var/log/slurm/slurmdbd.log
DebugLevel=error
AuthType=auth/munge
DbdHost=localhost
DbdPort=6819

# DB connection data
StorageType=accounting_storage/mysql
StorageHost=localhost
StoragePort=3306
StorageUser=slurm
StoragePass=slurm
StorageLoc=slurm_acct_db
SlurmUser=slurm
```

### run.sh
:::info
+ Replace the config cpus and memory in the cluster, if not provided, it will use default values
+ The database will be created with matched database, table , user and password on the above config
+ Start service: munge(auth), slurmctld(controller), slurmdbd(database controller) and slurmd(computing)
:::
```
#!/bin/bash
set -e

# Configure SLURM
cpus=${1:-2} 
memory=${2:-2048}

sed -e "s/<<CPUS>>/$cpus/g" -e "s/<<MEMORY>>/$memory/g" /etc/slurm-llnl/slurm.conf.template > /etc/slurm-llnl/slurm.conf
cat /etc/slurm-llnl/slurm.conf |grep NodeName
# Start required services
# slurmdbd
service mysql start
mysql -e "CREATE DATABASE slurm_acct_db;" && \
mysql -e "CREATE USER 'slurm'@'localhost' IDENTIFIED BY 'slurm';" && \
mysql -e "GRANT ALL PRIVILEGES ON slurm_acct_db.* TO 'slurm'@'localhost';" && \
mysql -e "FLUSH PRIVILEGES;"
service slurmdbd start
# slurmcltd
/etc/init.d/munge start
service slurmctld start
# ssh
/usr/sbin/sshd
# Wait for services to stabilize
sleep 10
# Configure SLURM accounts
sacctmgr -i add cluster localcluster
sacctmgr -i --quiet add account river Cluster=localcluster
sacctmgr -i --quiet add user river account=river DefaultAccount=root
service slurmdbd restart
service slurmctld restart
slurmd -D
```

## Submit job
This is for simple illustration on how to submit job basically. If you do not build successfully, it can be pulled from dockerhub
[**nttg8100/river-hpc:1.0.0**](https://hub.docker.com/r/nttg8100/river-hpc)

:::danger
It is a single node, so the user can run **WITHOUT** submitting job which violate the purpose of SLURM. 
There is no way to force user to submit job when they are in a single node
:::

1. Create a folder that contain those above files, build docker
```bash
docker build -t river-hpc:1.0.0
```

1. Run the docker, forward port 22 so you can login to localhost using 8080 port
```bash
docker run -p 8080:22 river-hpc:1.0.0
```

1. Open a new terminal, login with `password`
```bash
ssh river@localhost -p 8081
```

1. Submit a basic job
```bash
srun --pty bash
```

1. Squeue to get job
```bash
squeue -o "%i %P %u %T %M %l %D %C %m %R %Z %N" | column -t
```