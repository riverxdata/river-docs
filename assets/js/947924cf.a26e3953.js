"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[3949],{3386(e,n,s){s.d(n,{A:()=>i});const i=s.p+"assets/images/intro-7aaa912f8e48c1bb524eacaf56d8a3cd.png"},3838(e){e.exports=JSON.parse('{"permalink":"/river-docs/blog/machine-learning-bioinformatics-part1-knn","source":"@site/blog/2026-01/2026-02-02.md","title":"Machine Learning in Bioinformatics Part 1: Building KNN from Scratch","description":"Machine learning is transforming bioinformatics, enabling us to discover patterns in biological data. In this first part, we\'ll build a K-Nearest Neighbors (KNN) classifier from scratch using only Python, then apply it to simulated gene expression data. This post is designed for anyone who knows basic Python and biology\u2014no advanced ML experience required!","date":"2026-02-02T00:00:00.000Z","tags":[{"inline":true,"label":"machine-learning","permalink":"/river-docs/blog/tags/machine-learning"},{"inline":true,"label":"bioinformatics","permalink":"/river-docs/blog/tags/bioinformatics"},{"inline":true,"label":"python","permalink":"/river-docs/blog/tags/python"},{"inline":true,"label":"knn","permalink":"/river-docs/blog/tags/knn"},{"inline":true,"label":"gene-expression","permalink":"/river-docs/blog/tags/gene-expression"}],"readingTime":11.01,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"machine-learning-bioinformatics-part1-knn","title":"Machine Learning in Bioinformatics Part 1: Building KNN from Scratch","authors":["river"],"tags":["machine-learning","bioinformatics","python","knn","gene-expression"],"image":"./imgs/intro.png"},"unlisted":false,"prevItem":{"title":"Running GitHub Actions Locally with act: 5x Faster Development","permalink":"/river-docs/blog/cicd-bioinformatics-act-local-github-action"},"nextItem":{"title":"Introduction to AI/ML in Bioinformatics: Classification Models & Evaluation","permalink":"/river-docs/blog/intro-ai-ml-bioinformatics-applications"}}')},5751(e,n,s){s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>p,frontMatter:()=>r,metadata:()=>i,toc:()=>c});var i=s(3838),t=s(4848),a=s(8453);const r={slug:"machine-learning-bioinformatics-part1-knn",title:"Machine Learning in Bioinformatics Part 1: Building KNN from Scratch",authors:["river"],tags:["machine-learning","bioinformatics","python","knn","gene-expression"],image:"./imgs/intro.png"},l=void 0,o={image:s(3386).A,authorsImageUrls:[void 0]},c=[{value:"Why Machine Learning in Bioinformatics?",id:"why-machine-learning-in-bioinformatics",level:2},{value:"What is KNN?",id:"what-is-knn",level:2},{value:"Part 1: Building KNN from Scratch",id:"part-1-building-knn-from-scratch",level:2},{value:"Step 1: Import Libraries and Create Simulated Gene Expression Data",id:"step-1-import-libraries-and-create-simulated-gene-expression-data",level:3},{value:"Step 2: Calculate Distance Between Samples",id:"step-2-calculate-distance-between-samples",level:3},{value:"Step 3: Find K Nearest Neighbors",id:"step-3-find-k-nearest-neighbors",level:3},{value:"Step 4: Vote for the Class",id:"step-4-vote-for-the-class",level:3},{value:"Step 5: Build the Complete KNN Classifier",id:"step-5-build-the-complete-knn-classifier",level:3},{value:"Part 2: Tuning Parameters",id:"part-2-tuning-parameters",level:2},{value:"Experiment 1: Effect of K Value",id:"experiment-1-effect-of-k-value",level:3},{value:"Experiment 2: Distance Metric Comparison",id:"experiment-2-distance-metric-comparison",level:3},{value:"How KNN Learns the Decision Boundary",id:"how-knn-learns-the-decision-boundary",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"What&#39;s Next?",id:"whats-next",level:2},{value:"Summary Code: Full Example",id:"summary-code-full-example",level:2}];function d(e){const n={code:"code",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"Machine learning is transforming bioinformatics, enabling us to discover patterns in biological data. In this first part, we'll build a K-Nearest Neighbors (KNN) classifier from scratch using only Python, then apply it to simulated gene expression data. This post is designed for anyone who knows basic Python and biology\u2014no advanced ML experience required!"}),"\n",(0,t.jsx)(n.h2,{id:"why-machine-learning-in-bioinformatics",children:"Why Machine Learning in Bioinformatics?"}),"\n",(0,t.jsx)(n.p,{children:"Biologists today collect massive amounts of data:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Gene expression"})," from thousands of genes across different conditions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Patient samples"})," with different disease states or treatment responses"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Protein sequences"})," that need classification"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Imaging data"})," from microscopy or medical scans"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Manually analyzing this data is impossible. Machine learning helps us:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Find patterns"}),": Automatically discover which genes distinguish disease types"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Make predictions"}),": Classify new patients based on gene expression patterns"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Understand biology"}),": Identify important features in biological systems"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"what-is-knn",children:"What is KNN?"}),"\n",(0,t.jsx)(n.p,{children:"K-Nearest Neighbors (KNN) is one of the simplest yet powerful machine learning algorithms:"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"The Idea"}),": To classify a new sample, look at its K closest neighbors in your training data and vote on the class."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple Example"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Imagine you have cancer and normal patient samples plotted by gene expression"}),"\n",(0,t.jsx)(n.li,{children:"A new patient arrives"}),"\n",(0,t.jsx)(n.li,{children:"You find the 3 nearest patient samples (K=3)"}),"\n",(0,t.jsx)(n.li,{children:"If 2 are cancer and 1 is normal, you classify the new patient as cancer"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Why start with KNN?"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Easy to understand (no complex math)"}),"\n",(0,t.jsx)(n.li,{children:"Works well for bioinformatics data"}),"\n",(0,t.jsx)(n.li,{children:"Teaches core ML concepts: distance metrics, classification, and parameter tuning"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-1-building-knn-from-scratch",children:"Part 1: Building KNN from Scratch"}),"\n",(0,t.jsx)(n.p,{children:"Let's code KNN step-by-step in Python. You'll understand exactly what's happening."}),"\n",(0,t.jsx)(n.h3,{id:"step-1-import-libraries-and-create-simulated-gene-expression-data",children:"Step 1: Import Libraries and Create Simulated Gene Expression Data"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Simulate gene expression data\n# Let\'s say we have 100 samples with 2 genes (for easy visualization)\n# In reality, we\'d have thousands of genes\n\ndef create_gene_expression_data():\n    """\n    Simulate gene expression data for two disease types\n    \n    Gene expression = measured as normalized counts (e.g., from RNA-seq)\n    """\n    # Class 0: Normal samples\n    # These samples show low expression in both genes\n    normal = np.random.normal(loc=5, scale=1.5, size=(50, 2))\n    \n    # Class 1: Disease samples\n    # These samples show higher expression, especially in gene 1\n    disease = np.random.normal(loc=8, scale=1.5, size=(50, 2))\n    \n    # Combine data and create labels\n    X = np.vstack([normal, disease])  # Features (gene expression)\n    y = np.hstack([np.zeros(50), np.ones(50)])  # Labels (0=normal, 1=disease)\n    \n    return X, y\n\n# Load data\nX, y = create_gene_expression_data()\n\n# Split into training and test sets\ntrain_size = 80\nX_train, X_test = X[:train_size], X[train_size:]\ny_train, y_test = y[:train_size], y[train_size:]\n\nprint(f"Training set: {X_train.shape[0]} samples with {X_train.shape[1]} genes")\nprint(f"Test set: {X_test.shape[0]} samples with {X_test.shape[1]} genes")\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Output:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Training set: 80 samples with 2 genes\nTest set: 20 samples with 2 genes\nClasses: 50 Normal, 50 Disease\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"What we did:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Created 100 simulated patients (50 normal, 50 disease)"}),"\n",(0,t.jsx)(n.li,{children:"Each patient has expression levels for 2 genes"}),"\n",(0,t.jsx)(n.li,{children:"Normal patients: ~5 expression (biologically, lower expression)"}),"\n",(0,t.jsx)(n.li,{children:"Disease patients: ~8 expression (biologically, higher expression)"}),"\n",(0,t.jsx)(n.li,{children:"Split 80 samples for training, 20 for testing"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"step-2-calculate-distance-between-samples",children:"Step 2: Calculate Distance Between Samples"}),"\n",(0,t.jsxs)(n.p,{children:['KNN finds the "closest" neighbors. We need a distance metric. Let\'s implement ',(0,t.jsx)(n.strong,{children:"Euclidean distance"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def euclidean_distance(sample1, sample2):\n    """\n    Calculate Euclidean distance between two samples.\n    \n    Distance = sqrt((x1-x2)\xb2 + (y1-y2)\xb2 + ...)\n    \n    This measures "how different" two patients\' gene expression profiles are.\n    """\n    return np.sqrt(np.sum((sample1 - sample2) ** 2))\n\n# Test distance calculation\nsample_a = np.array([5.2, 4.8])  # Normal patient (low expression)\nsample_b = np.array([8.1, 8.3])  # Disease patient (high expression)\nsample_c = np.array([5.5, 4.9])  # Another normal patient (similar to A)\n\ndist_a_to_b = euclidean_distance(sample_a, sample_b)\ndist_a_to_c = euclidean_distance(sample_a, sample_c)\n\nprint(f"Distance from normal A to disease B: {dist_a_to_b:.2f}")\nprint(f"Distance from normal A to normal C: {dist_a_to_c:.2f}")\nprint(f"\u2192 A is closer to C (similar class) than B")\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Output:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Distance from normal A to disease B: 4.55\nDistance from normal A to normal C: 0.32\n\u2192 A is closer to C (similar class) than B \u2713\n"})}),"\n",(0,t.jsxs)(n.p,{children:["This is the key insight: ",(0,t.jsx)(n.strong,{children:"samples from the same class are closer together!"})]}),"\n",(0,t.jsx)(n.h3,{id:"step-3-find-k-nearest-neighbors",children:"Step 3: Find K Nearest Neighbors"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def find_nearest_neighbors(query_sample, training_data, training_labels, k=3):\n    """\n    Find the K nearest neighbors to a query sample.\n    \n    Returns:\n    - distances: distance to each training sample\n    - indices: index of each training sample\n    - neighbor_labels: class labels of the K nearest neighbors\n    """\n    # Calculate distance from query to all training samples\n    distances = []\n    for i, train_sample in enumerate(training_data):\n        dist = euclidean_distance(query_sample, train_sample)\n        distances.append(dist)\n    \n    distances = np.array(distances)\n    \n    # Find indices of K smallest distances\n    # argsort returns indices in ascending order\n    k_indices = np.argsort(distances)[:k]\n    \n    # Get labels of K nearest neighbors\n    neighbor_labels = training_labels[k_indices]\n    neighbor_distances = distances[k_indices]\n    \n    return neighbor_distances, k_indices, neighbor_labels\n\n# Test with a new patient (test sample)\nnew_patient = X_test[0]  # First test patient\nk = 3\n\ndistances, indices, labels = find_nearest_neighbors(new_patient, X_train, y_train, k=k)\n\nprint(f"New patient gene expression: {new_patient}")\nprint(f"\\nK={k} nearest neighbors:")\nfor i, (idx, dist, label) in enumerate(zip(indices, distances, labels)):\n    class_name = "Normal" if label == 0 else "Disease"\n    print(f"  Neighbor {i+1}: {class_name} patient (distance: {dist:.2f})")\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Output:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"New patient gene expression: [6.53797749 9.18062691]\n\nK=3 nearest neighbors:\n  Neighbor 1: Disease patient (distance: 0.38)\n  Neighbor 2: Disease patient (distance: 0.49)\n  Neighbor 3: Disease patient (distance: 0.94)\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"What's happening"}),': All 3 nearest neighbors are disease patients, so the KNN algorithm will predict "Disease" for this new patient.']}),"\n",(0,t.jsx)(n.h3,{id:"step-4-vote-for-the-class",children:"Step 4: Vote for the Class"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def predict_single_sample(query_sample, training_data, training_labels, k=3):\n    """\n    Predict the class of a query sample using KNN voting.\n    \n    Algorithm:\n    1. Find K nearest neighbors\n    2. Count votes for each class\n    3. Return the most common class (majority vote)\n    """\n    distances, indices, neighbor_labels = find_nearest_neighbors(\n        query_sample, training_data, training_labels, k\n    )\n    \n    # Count votes for each class\n    votes = Counter(neighbor_labels)\n    \n    # Return class with most votes\n    prediction = votes.most_common(1)[0][0]\n    \n    return prediction\n\n# Predict on a new patient\nprediction = predict_single_sample(new_patient, X_train, y_train, k=3)\ntrue_label = y_test[0]\n\nprint(f"Predicted class: {\'Normal\' if prediction == 0 else \'Disease\'}")\nprint(f"True class: {\'Normal\' if true_label == 0 else \'Disease\'}")\nprint(f"Correct: {prediction == true_label} \u2713" if prediction == true_label else f"Correct: {prediction == true_label} \u2717")\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Output:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Predicted class: Disease\nTrue class: Disease\nCorrect: True \u2713\n"})}),"\n",(0,t.jsx)(n.p,{children:"Perfect! Our KNN classifier correctly predicted this patient as having the disease. This shows the power of voting: even though one neighbor had distance 0.94, the majority (2 out of 3) voted for disease, leading to the correct prediction."}),"\n",(0,t.jsx)(n.h3,{id:"step-5-build-the-complete-knn-classifier",children:"Step 5: Build the Complete KNN Classifier"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class KNNClassifier:\n    """\n    A simple KNN classifier for bioinformatics data classification.\n    \n    Parameters:\n    - k: number of nearest neighbors to consider\n    - distance_metric: \'euclidean\' (default) or \'manhattan\'\n    """\n    \n    def __init__(self, k=3, distance_metric=\'euclidean\'):\n        self.k = k\n        self.distance_metric = distance_metric\n        self.X_train = None\n        self.y_train = None\n    \n    def distance(self, sample1, sample2):\n        """Calculate distance between two samples."""\n        if self.distance_metric == \'euclidean\':\n            # Euclidean: sqrt(sum of squared differences)\n            return np.sqrt(np.sum((sample1 - sample2) ** 2))\n        elif self.distance_metric == \'manhattan\':\n            # Manhattan: sum of absolute differences\n            # Like walking on a city grid instead of straight line\n            return np.sum(np.abs(sample1 - sample2))\n        else:\n            raise ValueError("Unknown distance metric")\n    \n    def fit(self, X_train, y_train):\n        """\n        Store training data.\n        KNN is a "lazy learner" - it doesn\'t learn patterns,\n        it just memorizes the training data.\n        """\n        self.X_train = X_train\n        self.y_train = y_train\n        print(f"\u2713 Stored {len(X_train)} training samples")\n    \n    def predict(self, X_test):\n        """Predict class for all test samples."""\n        predictions = []\n        for sample in X_test:\n            # Calculate distances to all training samples\n            distances = np.array([\n                self.distance(sample, train_sample) \n                for train_sample in self.X_train\n            ])\n            \n            # Find K nearest neighbors\n            k_indices = np.argsort(distances)[:self.k]\n            neighbor_labels = self.y_train[k_indices]\n            \n            # Vote for the most common class\n            votes = Counter(neighbor_labels)\n            prediction = votes.most_common(1)[0][0]\n            predictions.append(prediction)\n        \n        return np.array(predictions)\n    \n    def score(self, X_test, y_test):\n        """Calculate accuracy on test set."""\n        predictions = self.predict(X_test)\n        accuracy = np.mean(predictions == y_test)\n        return accuracy\n\n# Create and train the classifier\nknn = KNNClassifier(k=3)\nknn.fit(X_train, y_train)\n\n# Make predictions\ny_pred = knn.predict(X_test)\naccuracy = knn.score(X_test, y_test)\n\nprint(f"\\nKNN Classifier Performance:")\nprint(f"K = 3")\nprint(f"Accuracy on test set: {accuracy:.2%}")\nprint(f"Correct predictions: {(y_pred == y_test).sum()}/{len(y_test)}")\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Output:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u2713 Stored 80 training samples\n\nKNN Classifier Performance:\nK = 3\nAccuracy on test set: 90.00%\nCorrect predictions: 18/20\n"})}),"\n",(0,t.jsx)(n.p,{children:"Excellent! Our KNN classifier achieves 90% accuracy on the test set\u2014correctly predicting the disease status of 18 out of 20 patients based on gene expression patterns."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-2-tuning-parameters",children:"Part 2: Tuning Parameters"}),"\n",(0,t.jsx)(n.p,{children:"The power of KNN comes from parameter tuning. Let's experiment!"}),"\n",(0,t.jsx)(n.h3,{id:"experiment-1-effect-of-k-value",children:"Experiment 1: Effect of K Value"}),"\n",(0,t.jsx)(n.p,{children:"K controls how many neighbors to consider. What's the best value?"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Test different K values\nk_values = [1, 3, 5, 7, 9, 15]\ntrain_accuracies = []\ntest_accuracies = []\n\nfor k in k_values:\n    knn = KNNClassifier(k=k)\n    knn.fit(X_train, y_train)\n    \n    train_acc = knn.score(X_train, y_train)\n    test_acc = knn.score(X_test, y_test)\n    \n    train_accuracies.append(train_acc)\n    test_accuracies.append(test_acc)\n    \n    print(f"K={k:2d} | Train: {train_acc:.2%} | Test: {test_acc:.2%}")\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Output:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"K= 1 | Train: 100.00% | Test: 90.00%\nK= 3 | Train: 96.25% | Test: 90.00%\nK= 5 | Train: 97.50% | Test: 95.00%\nK= 7 | Train: 93.75% | Test: 95.00%\nK= 9 | Train: 96.25% | Test: 95.00%\nK=15 | Train: 95.00% | Test: 90.00%\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Observations:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"K=1"}),": Perfect on training (100%) but may overfit (memorizing individual patients)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"K=3-5"}),": Good balance on both training and test sets"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"K=7-9"}),": Smoother, robust predictions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"K=15"}),": All neighbors voted, might be too general (underfit)"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Biological interpretation:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Small K (1-3): Very sensitive to each patient's unique gene expression"}),"\n",(0,t.jsx)(n.li,{children:"Medium K (5-7): Balanced\u2014considers local gene expression patterns while being robust"}),"\n",(0,t.jsx)(n.li,{children:"Large K (15): Very general\u2014might miss subtle disease signatures but robust to noise"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["The sweet spot for this dataset appears to be ",(0,t.jsx)(n.strong,{children:"K=5-7"}),", where test accuracy plateaus at 95%."]}),"\n",(0,t.jsx)(n.h3,{id:"experiment-2-distance-metric-comparison",children:"Experiment 2: Distance Metric Comparison"}),"\n",(0,t.jsx)(n.p,{children:"Does it matter how we measure distance?"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Compare Euclidean vs Manhattan distance\ndistances = [\'euclidean\', \'manhattan\']\nresults = {}\n\nprint("\\nComparing Distance Metrics:")\nprint("-" * 50)\n\nfor dist_metric in distances:\n    accuracies = []\n    for k in k_values:\n        knn = KNNClassifier(k=k, distance_metric=dist_metric)\n        knn.fit(X_train, y_train)\n        test_acc = knn.score(X_test, y_test)\n        accuracies.append(test_acc)\n    results[dist_metric] = accuracies\n\nprint("\\nDistance Metric Comparison:")\nprint(f"{\'K\':<5} {\'Euclidean\':<15} {\'Manhattan\':<15}")\nprint("-" * 35)\nfor k, eucl_acc, manh_acc in zip(k_values, results[\'euclidean\'], results[\'manhattan\']):\n    print(f"{k:<5} {eucl_acc:<14.2%} {manh_acc:<14.2%}")\n\nprint("\\nBest parameters:")\nfor dist_metric in distances:\n    best_k = k_values[np.argmax(results[dist_metric])]\n    best_acc = max(results[dist_metric])\n    print(f"  {dist_metric}: K={best_k}, Accuracy={best_acc:.2%}")\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Output:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Comparing Distance Metrics:\n--------------------------------------------------\n\nDistance Metric Comparison:\nK     Euclidean       Manhattan       \n-------------------------------------\n1     90.00%          90.00%          \n3     90.00%          90.00%          \n5     95.00%          95.00%          \n7     95.00%          95.00%          \n9     95.00%          95.00%          \n15    90.00%          90.00%          \n\nBest parameters:\n  euclidean: K=5, Accuracy=95.00%\n  manhattan: K=5, Accuracy=95.00%\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Findings:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Both metrics perform identically"})," for this gene expression dataset"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Euclidean distance"})," is generally preferred for continuous data like gene expression"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Manhattan distance"})," (sum of absolute differences) can be useful for high-dimensional data or when features have different scales"]}),"\n",(0,t.jsxs)(n.li,{children:["For bioinformatics applications with normalized gene expression data, ",(0,t.jsx)(n.strong,{children:"Euclidean distance is the safer default"})]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"how-knn-learns-the-decision-boundary",children:"How KNN Learns the Decision Boundary"}),"\n",(0,t.jsx)(n.p,{children:'Even though KNN is a "lazy learner" that doesn\'t explicitly learn patterns, it implicitly learns decision boundaries through distance-based voting.'}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"What happened in our experiment:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Normal patients cluster around gene expression \u2248 [5, 5]"}),"\n",(0,t.jsx)(n.li,{children:"Disease patients cluster around gene expression \u2248 [8, 8]"}),"\n",(0,t.jsx)(n.li,{children:"When classifying a new patient, KNN votes based on proximity to these clusters"}),"\n",(0,t.jsx)(n.li,{children:"This creates a natural decision boundary between the two classes"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"For our 2-gene example with K=5:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Test accuracy: 95% (18 out of 20 patients correct)"}),"\n",(0,t.jsx)(n.li,{children:"The 2 misclassified patients were likely on the boundary between normal and disease"}),"\n",(0,t.jsx)(n.li,{children:"In real bioinformatics with thousands of genes, KNN works even better because true disease signatures are stronger across many genes"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Why KNN works for gene expression data:"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Genes cluster by function"}),": Genes involved in the same pathway have correlated expression"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Disease is distinctive"}),": Disease samples show consistent expression patterns different from normal"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"K controls smoothness"}),": Small K captures local patterns, large K finds global trends"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Distance captures similarity"}),": Euclidean distance naturally measures how different two gene expression profiles are"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"KNN is intuitive"}),': "You are similar to your neighbors" - easy to explain to biologists']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"K matters"}),": Small K is sensitive, large K is robust. K=3-5 often works well"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Distance metrics matter"}),": Euclidean works well for continuous data like gene expression"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"No learning phase"}),": KNN just memorizes training data and votes at prediction time"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Works for bioinformatics"}),": Genes naturally cluster by function and disease state"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"whats-next",children:"What's Next?"}),"\n",(0,t.jsx)(n.p,{children:"In Part 2, we'll:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Use ",(0,t.jsx)(n.strong,{children:"real gene expression data"})," (not simulated)"]}),"\n",(0,t.jsxs)(n.li,{children:["Scale to ",(0,t.jsx)(n.strong,{children:"thousands of genes"})]}),"\n",(0,t.jsxs)(n.li,{children:["Learn about ",(0,t.jsx)(n.strong,{children:"feature selection"})," (which genes matter?)"]}),"\n",(0,t.jsxs)(n.li,{children:["Evaluate using ",(0,t.jsx)(n.strong,{children:"cross-validation"})," for robust performance estimates"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"summary-code-full-example",children:"Summary Code: Full Example"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Quick reference - full KNN from scratch\nimport numpy as np\nfrom collections import Counter\n\nclass SimpleKNN:\n    def __init__(self, k=3):\n        self.k = k\n        self.X_train = None\n        self.y_train = None\n    \n    def fit(self, X, y):\n        self.X_train = X\n        self.y_train = y\n    \n    def predict(self, X_test):\n        predictions = []\n        for sample in X_test:\n            distances = np.sqrt(np.sum((self.X_train - sample) ** 2, axis=1))\n            k_indices = np.argsort(distances)[:self.k]\n            labels = self.y_train[k_indices]\n            prediction = Counter(labels).most_common(1)[0][0]\n            predictions.append(prediction)\n        return np.array(predictions)\n\n# Usage\nknn = SimpleKNN(k=5)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\naccuracy = np.mean(predictions == y_test)\nprint(f"Accuracy: {accuracy:.2%}")\n'})}),"\n",(0,t.jsx)(n.p,{children:"By coding KNN from scratch, you've learned machine learning fundamentals that apply to every algorithm. You now understand distance metrics, classification, and parameter tuning\u2014all essential concepts in bioinformatics!"})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,s){s.d(n,{R:()=>r,x:()=>l});var i=s(6540);const t={},a=i.createContext(t);function r(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);