"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[5648],{1383(e,n,s){s.r(n),s.d(n,{assets:()=>t,contentTitle:()=>o,default:()=>u,frontMatter:()=>l,metadata:()=>r,toc:()=>c});var r=s(8173),i=s(4848),a=s(8453);const l={slug:"how-to-build-slurm-hpc-part-3",title:"Building a Slurm HPC Cluster (Part 3) - Administration and Best Practices",authors:["river"],tags:["slurm","hpc","administration","security","best-practices"],image:"./imgs/hpc_3.svg"},o=void 0,t={image:s(2110).A,authorsImageUrls:[void 0]},c=[{value:"Series Overview",id:"series-overview",level:2},{value:"Administration Overview",id:"administration-overview",level:2},{value:"User and Resource Management",id:"user-and-resource-management",level:2},{value:"Adding Users and Groups",id:"adding-users-and-groups",level:3},{value:"Setting Resource Limits",id:"setting-resource-limits",level:3},{value:"Account-Level Limits",id:"account-level-limits",level:4},{value:"User-Level Limits",id:"user-level-limits",level:4},{value:"Quality of Service (QoS)",id:"quality-of-service-qos",level:3},{value:"Fair-Share Scheduling",id:"fair-share-scheduling",level:3},{value:"Node Management",id:"node-management",level:2},{value:"Checking Node Status",id:"checking-node-status",level:3},{value:"Node Maintenance",id:"node-maintenance",level:3},{value:"Draining a Node",id:"draining-a-node",level:4},{value:"Resuming a Node",id:"resuming-a-node",level:4},{value:"Forcing Node Down",id:"forcing-node-down",level:4},{value:"Adding New Compute Nodes",id:"adding-new-compute-nodes",level:3},{value:"Monitoring and Troubleshooting",id:"monitoring-and-troubleshooting",level:2},{value:"Checking Slurm Logs",id:"checking-slurm-logs",level:3},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Issue: Node Shows as DOWN",id:"issue-node-shows-as-down",level:4},{value:"Issue: Jobs Stuck in Pending",id:"issue-jobs-stuck-in-pending",level:4},{value:"Issue: Jobs Failing Immediately",id:"issue-jobs-failing-immediately",level:4},{value:"Issue: Accounting Database Not Working",id:"issue-accounting-database-not-working",level:4},{value:"System Logs with rsyslog",id:"system-logs-with-rsyslog",level:3},{value:"Security Best Practices",id:"security-best-practices",level:2},{value:"SSH Hardening",id:"ssh-hardening",level:3},{value:"Munge Authentication",id:"munge-authentication",level:3},{value:"Docker Security",id:"docker-security",level:3},{value:"Firewall Configuration",id:"firewall-configuration",level:3},{value:"Shared Storage Best Practices",id:"shared-storage-best-practices",level:2},{value:"NFS Performance Tuning",id:"nfs-performance-tuning",level:3},{value:"Storage Layout",id:"storage-layout",level:3},{value:"Quotas",id:"quotas",level:3},{value:"Integration with Data Processing Frameworks",id:"integration-with-data-processing-frameworks",level:2},{value:"Apache Spark",id:"apache-spark",level:3},{value:"Ray (Distributed ML)",id:"ray-distributed-ml",level:3},{value:"Dask",id:"dask",level:3},{value:"Nextflow (Bioinformatics)",id:"nextflow-bioinformatics",level:3},{value:"Maintenance Tasks",id:"maintenance-tasks",level:2},{value:"Regular Updates",id:"regular-updates",level:3},{value:"Backup Critical Data",id:"backup-critical-data",level:3},{value:"Monitoring Disk Space",id:"monitoring-disk-space",level:3},{value:"Performance Optimization Tips",id:"performance-optimization-tips",level:2},{value:"1. Tune Scheduler Parameters",id:"1-tune-scheduler-parameters",level:3},{value:"2. Optimize Job Packing",id:"2-optimize-job-packing",level:3},{value:"3. Create Multiple Partitions",id:"3-create-multiple-partitions",level:3},{value:"4. Enable Job Arrays for Batch Processing",id:"4-enable-job-arrays-for-batch-processing",level:3},{value:"Conclusion",id:"conclusion",level:2},{value:"Part 1: Foundations",id:"part-1-foundations",level:3},{value:"Part 2: Production Deployment",id:"part-2-production-deployment",level:3},{value:"Part 3: Administration (This Post)",id:"part-3-administration-this-post",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"What&#39;s Next?",id:"whats-next",level:2},{value:"Resources",id:"resources",level:2},{value:"Contact",id:"contact",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["In ",(0,i.jsx)(n.a,{href:"/blog/how-to-build-slurm-hpc-part-1",children:"Part 1"})," and ",(0,i.jsx)(n.a,{href:"/blog/how-to-build-slurm-hpc-part-2",children:"Part 2"}),", we built a complete Slurm HPC cluster from a single node to a production-ready multi-node system. Now let's learn how to manage, maintain, and secure it effectively."]}),"\n",(0,i.jsx)(n.p,{children:"This final post covers daily administration tasks, troubleshooting, security hardening, and integration with data processing frameworks."}),"\n",(0,i.jsx)(n.h2,{id:"series-overview",children:"Series Overview"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.a,{href:"/blog/how-to-build-slurm-hpc-part-1",children:"Part 1"})}),": Introduction, Architecture, and Single Node Setup"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.a,{href:"/blog/how-to-build-slurm-hpc-part-2",children:"Part 2"})}),": Scaling to Production with Ansible"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Part 3 (This Post)"}),": Administration and Best Practices"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"administration-overview",children:"Administration Overview"}),"\n",(0,i.jsx)(n.p,{children:"Managing a Slurm cluster involves several key areas:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cluster Management"}),": Build, maintain, and update the cluster via Ansible"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"User Management"}),": Synchronize users across nodes with proper permissions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Login Security"}),": Implement SSH hardening with 2FA or key pairs"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Resource Management"}),": Enforce limits and fair-share policies"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Monitoring"}),": Track performance and resource utilization"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Troubleshooting"}),": Diagnose and resolve issues"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"user-and-resource-management",children:"User and Resource Management"}),"\n",(0,i.jsx)(n.h3,{id:"adding-users-and-groups",children:"Adding Users and Groups"}),"\n",(0,i.jsx)(n.p,{children:"Slurm uses accounts (groups) to organize users and apply resource policies:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Add a new account/group\nsacctmgr add account research_team Description="Research Team"\n\n# Add a user to an account\nsacctmgr add user john account=research_team\n\n# Add user with multiple accounts\nsacctmgr add user alice account=research_team,dev_team DefaultAccount=research_team\n\n# View accounts\nsacctmgr show account\n\n# View users\nsacctmgr show user\n'})}),"\n",(0,i.jsx)(n.h3,{id:"setting-resource-limits",children:"Setting Resource Limits"}),"\n",(0,i.jsx)(n.h4,{id:"account-level-limits",children:"Account-Level Limits"}),"\n",(0,i.jsx)(n.p,{children:"Control resources for entire groups:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Limit CPU minutes (prevents monopolizing cluster)\nsacctmgr modify account research_team set GrpCPUMins=100000\n\n# Limit memory (in MB)\nsacctmgr modify account research_team set GrpMem=500000\n\n# Limit concurrent jobs\nsacctmgr modify account research_team set GrpJobs=50\n\n# Limit concurrent running jobs\nsacctmgr modify account research_team set GrpJobsRun=20\n\n# Limit number of nodes\nsacctmgr modify account research_team set GrpNodes=10\n"})}),"\n",(0,i.jsx)(n.h4,{id:"user-level-limits",children:"User-Level Limits"}),"\n",(0,i.jsx)(n.p,{children:"Control individual user behavior:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Limit jobs in queue\nsacctmgr modify user john set MaxJobs=10\n\n# Limit running jobs\nsacctmgr modify user john set MaxJobsRun=5\n\n# Limit wall time (in minutes)\nsacctmgr modify user john set MaxWall=1440  # 24 hours\n\n# Limit CPUs per job\nsacctmgr modify user john set MaxCPUs=32\n\n# View user limits\nsacctmgr show user john withassoc format=user,account,maxjobs,maxsubmit,maxwall\n"})}),"\n",(0,i.jsx)(n.h3,{id:"quality-of-service-qos",children:"Quality of Service (QoS)"}),"\n",(0,i.jsx)(n.p,{children:"QoS allows you to create service tiers with different priorities:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Create QoS levels\nsacctmgr add qos normal priority=100\nsacctmgr add qos high priority=500 MaxWall=2-00:00:00 MaxJobs=5\nsacctmgr add qos low priority=50\n\n# Assign QoS to account\nsacctmgr modify account research_team set qos=normal,high\n\n# Users can specify QoS when submitting\nsbatch --qos=high job_script.sh\n"})}),"\n",(0,i.jsx)(n.h3,{id:"fair-share-scheduling",children:"Fair-Share Scheduling"}),"\n",(0,i.jsx)(n.p,{children:"Ensure equitable resource distribution:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Set fair-share values (higher = more priority)\nsacctmgr modify account research_team set fairshare=100\nsacctmgr modify account dev_team set fairshare=50\n\n# View fair-share tree\nsshare -a\n\n# View detailed fair-share info\nsshare -A research_team --all\n"})}),"\n",(0,i.jsx)(n.h2,{id:"node-management",children:"Node Management"}),"\n",(0,i.jsx)(n.h3,{id:"checking-node-status",children:"Checking Node Status"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# View all nodes\nsinfo\n\n# Detailed node information\nsinfo -Nel\n\n# Show node states\nsinfo -N -o "%N %T %C %m %e %f"\n\n# View specific node details\nscontrol show node worker-01\n'})}),"\n",(0,i.jsx)("figure",{markdown:"span",children:(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Job sinfo",src:s(7322).A+"",width:"1310",height:"351"})})}),"\n",(0,i.jsx)(n.p,{children:"Node states you'll encounter:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"IDLE"}),": Available for jobs"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ALLOCATED"}),": Running jobs"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"MIXED"}),": Partially allocated"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"DRAIN"}),": Won't accept new jobs (draining)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"DRAINED"}),": Fully drained"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"DOWN"}),": Not responding"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"node-maintenance",children:"Node Maintenance"}),"\n",(0,i.jsx)(n.h4,{id:"draining-a-node",children:"Draining a Node"}),"\n",(0,i.jsx)(n.p,{children:"When you need to perform maintenance:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Drain node (won\'t accept new jobs, allows running jobs to finish)\nscontrol update NodeName=worker-01 State=drain Reason="Hardware upgrade"\n\n# Force drain (terminate running jobs)\nscontrol update NodeName=worker-01 State=drain Reason="Emergency maintenance"\n\n# Check drain reason\nsinfo -R\n'})}),"\n",(0,i.jsx)(n.h4,{id:"resuming-a-node",children:"Resuming a Node"}),"\n",(0,i.jsx)(n.p,{children:"After maintenance:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Resume node\nscontrol update NodeName=worker-01 State=resume\n\n# Verify it's back\nsinfo -n worker-01\n"})}),"\n",(0,i.jsx)(n.h4,{id:"forcing-node-down",children:"Forcing Node Down"}),"\n",(0,i.jsx)(n.p,{children:"If a node is misbehaving:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Mark node as down\nscontrol update NodeName=worker-01 State=down Reason="Hardware failure"\n\n# When fixed, resume\nscontrol update NodeName=worker-01 State=resume\n'})}),"\n",(0,i.jsx)(n.h3,{id:"adding-new-compute-nodes",children:"Adding New Compute Nodes"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Update Ansible inventory (",(0,i.jsx)(n.code,{children:"inventories/hosts"}),"):"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-ini",children:"[slurm_worker]\nworker-01 ansible_host=192.168.58.11\nworker-02 ansible_host=192.168.58.12\nworker-03 ansible_host=192.168.58.13  # NEW\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsx)(n.li,{children:"Run Ansible playbook:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ansible-playbook -i inventories/hosts river_cluster.yml --limit worker-03\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"3",children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Update ",(0,i.jsx)(n.code,{children:"slurm.conf"})," on controller and all nodes (Ansible handles this)"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Restart slurmctld:"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"sudo systemctl restart slurmctld\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"5",children:["\n",(0,i.jsx)(n.li,{children:"Verify the new node:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"sinfo\nscontrol show node worker-03\n"})}),"\n",(0,i.jsx)(n.h2,{id:"monitoring-and-troubleshooting",children:"Monitoring and Troubleshooting"}),"\n",(0,i.jsx)(n.h3,{id:"checking-slurm-logs",children:"Checking Slurm Logs"}),"\n",(0,i.jsx)(n.p,{children:"Logs are essential for diagnosing issues:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Controller logs\nsudo tail -f /var/log/slurm/slurmctld.log\n\n# Worker node logs (on compute nodes)\nsudo tail -f /var/log/slurm/slurmd.log\n\n# Database logs\nsudo tail -f /var/log/slurm/slurmdbd.log\n\n# Filter for errors\nsudo grep "error" /var/log/slurm/*.log\n\n# Filter for specific node\nsudo grep "worker-01" /var/log/slurm/slurmctld.log\n\n# Last 100 lines with context\nsudo tail -100 /var/log/slurm/slurmctld.log\n'})}),"\n",(0,i.jsx)(n.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,i.jsx)(n.h4,{id:"issue-node-shows-as-down",children:"Issue: Node Shows as DOWN"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Diagnosis:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'sinfo\n# OUTPUT: worker-01    down   ...\n\nscontrol show node worker-01\n# Check "Reason" field\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Solutions:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# 1. Check if slurmd is running\nssh worker-01 "sudo systemctl status slurmd"\n\n# 2. Restart slurmd\nssh worker-01 "sudo systemctl restart slurmd"\n\n# 3. Check network connectivity\nping worker-01\n\n# 4. Check logs\nssh worker-01 "sudo tail -50 /var/log/slurm/slurmd.log"\n\n# 5. Resume the node\nscontrol update NodeName=worker-01 State=resume\n'})}),"\n",(0,i.jsx)(n.h4,{id:"issue-jobs-stuck-in-pending",children:"Issue: Jobs Stuck in Pending"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Diagnosis:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"squeue\n# See jobs in PD (pending) state\n\n# Check why job is pending\nsqueue --start -j JOB_ID\n\n# View detailed job info\nscontrol show job JOB_ID\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Common reasons:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"Resources"}),": Not enough resources available"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"Priority"}),": Lower priority than other jobs"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"Dependency"}),": Waiting for another job to complete"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"QOSMaxJobsPerUser"}),": User has too many jobs running"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Solutions:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# 1. Check available resources\nsinfo -o "%P %a %l %D %N %C"\n\n# 2. View job requirements\nscontrol show job JOB_ID | grep -E "Partition|NumNodes|MinMemory"\n\n# 3. Cancel job if needed\nscancel JOB_ID\n\n# 4. Modify pending job\nscontrol update JobId=JOB_ID NumNodes=1\n'})}),"\n",(0,i.jsx)(n.h4,{id:"issue-jobs-failing-immediately",children:"Issue: Jobs Failing Immediately"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Diagnosis:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Check job status\nsacct -j JOB_ID\n\n# View job output files\ncat slurm-JOB_ID.out\ncat slurm-JOB_ID.err\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Common causes:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Script errors (check shebang line)"}),"\n",(0,i.jsx)(n.li,{children:"Missing executables"}),"\n",(0,i.jsx)(n.li,{children:"Resource limits exceeded"}),"\n",(0,i.jsx)(n.li,{children:"Permission issues"}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"issue-accounting-database-not-working",children:"Issue: Accounting Database Not Working"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Diagnosis:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Check slurmdbd status\nsudo systemctl status slurmdbd\n\n# Test database connection\nsudo mysql -u slurm -p slurm_acct_db -e "SHOW TABLES;"\n\n# Check slurmdbd logs\nsudo tail -50 /var/log/slurm/slurmdbd.log\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Solutions:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# 1. Restart slurmdbd\nsudo systemctl restart slurmdbd\n\n# 2. Verify database credentials in slurmdbd.conf\nsudo cat /etc/slurm-llnl/slurmdbd.conf\n\n# 3. Check database permissions\nsudo mysql -e \"SHOW GRANTS FOR 'slurm'@'localhost';\"\n\n# 4. Restart slurmctld to reconnect\nsudo systemctl restart slurmctld\n"})}),"\n",(0,i.jsx)(n.h3,{id:"system-logs-with-rsyslog",children:"System Logs with rsyslog"}),"\n",(0,i.jsx)(n.p,{children:"Our Ansible setup configures centralized logging:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# On controller (rsyslog server)\nsudo tail -f /var/log/syslog\n\n# Filter by hostname\nsudo grep "worker-01" /var/log/syslog\n\n# Filter by service\nsudo grep "slurmd" /var/log/syslog\n\n# Check authentication logs\nsudo tail -f /var/log/auth.log\n'})}),"\n",(0,i.jsx)(n.h2,{id:"security-best-practices",children:"Security Best Practices"}),"\n",(0,i.jsx)(n.h3,{id:"ssh-hardening",children:"SSH Hardening"}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsx)(n.p,{children:"Secure your login nodes! HPC clusters are attractive targets for attackers."})}),"\n",(0,i.jsxs)(n.p,{children:["For detailed SSH security setup, see our ",(0,i.jsx)(n.a,{href:"/docs/resources/administration/ssh-remote-server",children:"SSH Remote Server documentation"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"Key recommendations:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Disable Password Authentication"}),":"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# /etc/ssh/sshd_config\nPasswordAuthentication no\nPubkeyAuthentication yes\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Implement 2FA"})," with Google Authenticator or similar"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Use SSH Key Pairs"}),":"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Generate key on your machine\nssh-keygen -t ed25519 -C "your_email@example.com"\n\n# Copy to cluster\nssh-copy-id user@controller-node\n'})}),"\n",(0,i.jsxs)(n.ol,{start:"4",children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Limit SSH Access"}),":"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# /etc/ssh/sshd_config\nAllowUsers alice bob charlie\nAllowGroups cluster_users\n\n# Or deny specific users\nDenyUsers baduser\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"5",children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Change Default Port"})," (security through obscurity):"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# /etc/ssh/sshd_config\nPort 2222\n"})}),"\n",(0,i.jsx)(n.h3,{id:"munge-authentication",children:"Munge Authentication"}),"\n",(0,i.jsx)(n.p,{children:"Munge provides authentication between Slurm components:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Verify munge is running\nsudo systemctl status munge\n\n# Test munge\nmunge -n | unmunge\n\n# Generate new key (do this on controller, then distribute)\nsudo /usr/sbin/create-munge-key\n\n# Copy key to all nodes (Ansible does this automatically)\nsudo scp /etc/munge/munge.key worker-01:/etc/munge/\n\n# Restart munge on all nodes\nsudo systemctl restart munge\n"})}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Critical"}),": The munge key must be identical on all nodes and have proper permissions (0400, owned by munge",":munge",")."]})}),"\n",(0,i.jsx)(n.h3,{id:"docker-security",children:"Docker Security"}),"\n",(0,i.jsxs)(n.admonition,{type:"danger",children:[(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Critical Security Issue"}),": Users in the ",(0,i.jsx)(n.code,{children:"docker"})," group can gain root privileges!"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# DON'T DO THIS (unless they're admins)\nsudo usermod -aG docker regular_user\n"})}),(0,i.jsx)(n.p,{children:"Why? Because they can run:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"docker run -v /:/hostfs --privileged -it ubuntu bash\n# Now they have root access to the host filesystem!\n"})})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use Docker Rootless Mode"}),":"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Install rootless docker\ncurl -fsSL https://get.docker.com/rootless | sh\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use Apptainer/Singularity"})," (designed for HPC):"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Install Apptainer\nsudo apt-get install apptainer\n\n# Run containers without root\napptainer run docker://ubuntu:latest\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"3",children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Restrict Docker Group"}),": Only add administrators to docker group"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"firewall-configuration",children:"Firewall Configuration"}),"\n",(0,i.jsx)(n.p,{children:"Restrict access to Slurm ports:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Allow Slurm ports only from cluster network\nsudo ufw allow from 192.168.58.0/24 to any port 6817  # slurmctld\nsudo ufw allow from 192.168.58.0/24 to any port 6818  # slurmd\nsudo ufw allow from 192.168.58.0/24 to any port 6819  # slurmdbd\n\n# Allow SSH from anywhere\nsudo ufw allow 22/tcp\n\n# Enable firewall\nsudo ufw enable\n"})}),"\n",(0,i.jsx)(n.h2,{id:"shared-storage-best-practices",children:"Shared Storage Best Practices"}),"\n",(0,i.jsx)(n.h3,{id:"nfs-performance-tuning",children:"NFS Performance Tuning"}),"\n",(0,i.jsx)(n.p,{children:"Optimize NFS for your workload:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# /etc/fstab on compute nodes\ncontroller-01:/home /home nfs4 rw,soft,rsize=262144,wsize=262144,timeo=14,intr 0 0\n"})}),"\n",(0,i.jsx)(n.p,{children:"Parameters explained:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"soft"}),": Timeout after retry (vs ",(0,i.jsx)(n.code,{children:"hard"})," which waits forever)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"rsize/wsize"}),": Read/write buffer size (larger = better performance)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"timeo"}),": Timeout value"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"intr"}),": Allow interrupts"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"storage-layout",children:"Storage Layout"}),"\n",(0,i.jsx)(n.p,{children:"Recommended directory structure:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"/home/          # User home directories (SSD/NVMe)\n  \u251c\u2500 alice/\n  \u251c\u2500 bob/\n  \u2514\u2500 charlie/\n\n/mnt/data/      # Large datasets (HDD or object storage)\n  \u251c\u2500 shared/    # Common datasets\n  \u251c\u2500 projects/  # Project-specific data\n  \u2514\u2500 scratch/   # Temporary data (auto-cleanup)\n\n/opt/           # Shared software/modules\n  \u251c\u2500 anaconda/\n  \u251c\u2500 modules/\n  \u2514\u2500 apps/\n"})}),"\n",(0,i.jsx)(n.h3,{id:"quotas",children:"Quotas"}),"\n",(0,i.jsx)(n.p,{children:"Prevent users from filling up shared storage:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Set user quotas\nsudo setquota -u alice 50G 60G 0 0 /home\nsudo setquota -u alice 500G 550G 0 0 /mnt/data\n\n# Check quotas\nquota -u alice\n\n# View all quotas\nsudo repquota -a\n"})}),"\n",(0,i.jsx)(n.h2,{id:"integration-with-data-processing-frameworks",children:"Integration with Data Processing Frameworks"}),"\n",(0,i.jsx)(n.p,{children:"One of Slurm's greatest strengths is integration with modern computing frameworks:"}),"\n",(0,i.jsx)(n.h3,{id:"apache-spark",children:"Apache Spark"}),"\n",(0,i.jsx)(n.p,{children:"Submit Spark jobs to Slurm:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"#!/bin/bash\n#SBATCH --job-name=spark-job\n#SBATCH --nodes=4\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32G\n\n# Load Spark module\nmodule load spark/3.5.0\n\n# Run Spark application\nspark-submit \\\n  --master yarn \\\n  --num-executors 4 \\\n  --executor-cores 8 \\\n  --executor-memory 28G \\\n  my_spark_app.py\n"})}),"\n",(0,i.jsx)(n.h3,{id:"ray-distributed-ml",children:"Ray (Distributed ML)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"#!/bin/bash\n#SBATCH --job-name=ray-job\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=16\n#SBATCH --gpus-per-node=2\n\n# Start Ray cluster\nray start --head --port=6379\nsrun --nodes=1 --ntasks=1 ray start --address=$HEAD_NODE:6379\n\n# Run Ray application\npython ray_train.py\n"})}),"\n",(0,i.jsx)(n.h3,{id:"dask",children:"Dask"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from dask_jobqueue import SLURMCluster\nfrom dask.distributed import Client\n\ncluster = SLURMCluster(\n    cores=8,\n    memory="16GB",\n    processes=2,\n    walltime="02:00:00",\n    queue="compute"\n)\n\ncluster.scale(jobs=10)  # Request 10 jobs\nclient = Client(cluster)\n\n# Your Dask code here\n'})}),"\n",(0,i.jsx)(n.h3,{id:"nextflow-bioinformatics",children:"Nextflow (Bioinformatics)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-groovy",children:"// nextflow.config\nprocess {\n    executor = 'slurm'\n    queue = 'compute'\n    memory = '8 GB'\n    time = '2h'\n}\n"})}),"\n",(0,i.jsx)(n.p,{children:"Run with:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"nextflow run nf-core/rnaseq -profile slurm\n"})}),"\n",(0,i.jsx)(n.h2,{id:"maintenance-tasks",children:"Maintenance Tasks"}),"\n",(0,i.jsx)(n.h3,{id:"regular-updates",children:"Regular Updates"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Update cluster via Ansible\nansible-playbook -i inventories/hosts river_cluster.yml --tags update\n\n# Update specific nodes\nansible-playbook -i inventories/hosts river_cluster.yml --limit worker-01,worker-02\n"})}),"\n",(0,i.jsx)(n.h3,{id:"backup-critical-data",children:"Backup Critical Data"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Backup Slurm configuration\nsudo cp /etc/slurm-llnl/slurm.conf /backup/slurm.conf.$(date +%Y%m%d)\n\n# Backup accounting database\nsudo mysqldump -u slurm -p slurm_acct_db > slurm_acct_backup_$(date +%Y%m%d).sql\n\n# Backup user data (use rsync for efficiency)\nsudo rsync -av /home/ /backup/home/\n"})}),"\n",(0,i.jsx)(n.h3,{id:"monitoring-disk-space",children:"Monitoring Disk Space"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Check disk usage on all nodes\nansible all -i inventories/hosts -m shell -a "df -h"\n\n# Check specific directory\nansible all -i inventories/hosts -m shell -a "du -sh /var/log/slurm"\n\n# Find large files\nfind /home -type f -size +1G -exec ls -lh {} \\;\n'})}),"\n",(0,i.jsx)(n.h2,{id:"performance-optimization-tips",children:"Performance Optimization Tips"}),"\n",(0,i.jsx)(n.h3,{id:"1-tune-scheduler-parameters",children:"1. Tune Scheduler Parameters"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# /etc/slurm-llnl/slurm.conf\n\n# Increase scheduling frequency\nSchedulerTimeSlice=30\n\n# Prioritize recent submitters less\nPriorityWeightAge=1000\nPriorityWeightFairshare=10000\n\n# Enable backfill scheduling with larger window\nSchedulerType=sched/backfill\nbf_window=1440  # 24 hours\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-optimize-job-packing",children:"2. Optimize Job Packing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Use CR_CPU for CPU-bound jobs\nSelectType=select/cons_tres\nSelectTypeParameters=CR_CPU\n\n# Or CR_Memory for memory-bound jobs\nSelectTypeParameters=CR_Memory\n\n# Or CR_Core for mixed workloads\nSelectTypeParameters=CR_Core\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-create-multiple-partitions",children:"3. Create Multiple Partitions"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# /etc/slurm-llnl/slurm.conf\n\n# Fast partition for short jobs\nPartitionName=quick Nodes=worker-[01-02] Default=NO MaxTime=01:00:00 State=UP Priority=100\n\n# Standard partition\nPartitionName=standard Nodes=worker-[01-04] Default=YES MaxTime=2-00:00:00 State=UP Priority=50\n\n# Long partition for extended jobs\nPartitionName=long Nodes=worker-[03-04] Default=NO MaxTime=7-00:00:00 State=UP Priority=25\n\n# GPU partition\nPartitionName=gpu Nodes=gpu-[01-02] Default=NO MaxTime=1-00:00:00 State=UP Priority=75\n"})}),"\n",(0,i.jsx)(n.h3,{id:"4-enable-job-arrays-for-batch-processing",children:"4. Enable Job Arrays for Batch Processing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"#!/bin/bash\n#SBATCH --array=1-100%10  # 100 tasks, max 10 concurrent\n\n# Process task based on array index\npython process.py --input data_${SLURM_ARRAY_TASK_ID}.txt\n"})}),"\n",(0,i.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(n.p,{children:"Congratulations! You now have the knowledge to build, deploy, and manage a production Slurm HPC cluster. Let's recap the journey:"}),"\n",(0,i.jsx)(n.h3,{id:"part-1-foundations",children:"Part 1: Foundations"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Understanding Slurm architecture"}),"\n",(0,i.jsx)(n.li,{children:"Single-node setup for learning"}),"\n",(0,i.jsx)(n.li,{children:"Critical cgroup configuration"}),"\n",(0,i.jsx)(n.li,{children:"Job accounting basics"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"part-2-production-deployment",children:"Part 2: Production Deployment"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Ansible automation"}),"\n",(0,i.jsx)(n.li,{children:"Multi-node cluster setup"}),"\n",(0,i.jsx)(n.li,{children:"Monitoring with Grafana"}),"\n",(0,i.jsx)(n.li,{children:"Slack alerting"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"part-3-administration-this-post",children:"Part 3: Administration (This Post)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"User and resource management"}),"\n",(0,i.jsx)(n.li,{children:"Node maintenance and troubleshooting"}),"\n",(0,i.jsx)(n.li,{children:"Security hardening"}),"\n",(0,i.jsx)(n.li,{children:"Performance optimization"}),"\n",(0,i.jsx)(n.li,{children:"Framework integration"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Start Simple, Scale Smart"}),": Master single-node before going multi-node"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Automate Everything"}),": Use Ansible for reproducible deployments"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Monitor Proactively"}),": Set up alerting before problems occur"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Security First"}),": SSH hardening, proper permissions, Docker caution"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Regular Maintenance"}),": Backups, updates, and log monitoring"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Documentation"}),": Document your cluster configuration and procedures"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"whats-next",children:"What's Next?"}),"\n",(0,i.jsx)(n.p,{children:"Consider these advanced topics:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"High Availability"}),": Redundant controllers with failover"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LDAP Integration"}),": Centralized authentication for large organizations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"GPU Scheduling"}),": Optimize for machine learning workloads"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cloud Bursting"}),": Expand to cloud resources during peak demand"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Custom Plugins"}),": Extend Slurm with custom scheduling policies"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Part 1"}),": ",(0,i.jsx)(n.a,{href:"/blog/how-to-build-slurm-hpc-part-1",children:"Single Node Setup"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Part 2"}),": ",(0,i.jsx)(n.a,{href:"/blog/how-to-build-slurm-hpc-part-2",children:"Production Deployment"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Documentation"}),": ",(0,i.jsx)(n.a,{href:"/docs/resources/high-performance-computing/how-to-build-slurm-scalable-using-ansible/administration",children:"Administration Guide"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"GitHub"}),": ",(0,i.jsx)(n.a,{href:"https://github.com/riverxdata/river-slurm",children:"RiverXData Slurm Ansible"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Official Docs"}),": ",(0,i.jsx)(n.a,{href:"https://slurm.schedmd.com/",children:"SchedMD Slurm Documentation"})]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"contact",children:"Contact"}),"\n",(0,i.jsxs)(n.p,{children:["Have questions or need help with your cluster? Reach out at: ",(0,i.jsx)(n.a,{href:"mailto:nttg8100@gmail.com",children:"nttg8100@gmail.com"})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"This concludes the RiverXData series on building Slurm HPC clusters. Thank you for following along! We hope this guide helps you build and manage effective HPC infrastructure."})})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},2110(e,n,s){s.d(n,{A:()=>r});const r=s.p+"assets/images/hpc_3-97a3980c92b005b9ee728085f5993f66.svg"},7322(e,n,s){s.d(n,{A:()=>r});const r=s.p+"assets/images/job_sinfo-36f054ef9651148a5f22d7075a11829c.png"},8173(e){e.exports=JSON.parse('{"permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-3","source":"@site/blog/2026-01/2026-01-14.md","title":"Building a Slurm HPC Cluster (Part 3) - Administration and Best Practices","description":"In Part 1 and Part 2, we built a complete Slurm HPC cluster from a single node to a production-ready multi-node system. Now let\'s learn how to manage, maintain, and secure it effectively.","date":"2026-01-14T00:00:00.000Z","tags":[{"inline":true,"label":"slurm","permalink":"/river-docs/blog/tags/slurm"},{"inline":true,"label":"hpc","permalink":"/river-docs/blog/tags/hpc"},{"inline":true,"label":"administration","permalink":"/river-docs/blog/tags/administration"},{"inline":true,"label":"security","permalink":"/river-docs/blog/tags/security"},{"inline":true,"label":"best-practices","permalink":"/river-docs/blog/tags/best-practices"}],"readingTime":12.13,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"how-to-build-slurm-hpc-part-3","title":"Building a Slurm HPC Cluster (Part 3) - Administration and Best Practices","authors":["river"],"tags":["slurm","hpc","administration","security","best-practices"],"image":"./imgs/hpc_3.svg"},"unlisted":false,"nextItem":{"title":"Building a Slurm HPC Cluster (Part 2) - Scaling to Production with Ansible","permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-2"}}')},8453(e,n,s){s.d(n,{R:()=>l,x:()=>o});var r=s(6540);const i={},a=r.createContext(i);function l(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);