"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[8903],{882(e,n,s){s.d(n,{A:()=>r});const r=s.p+"assets/images/account_usage-97842aa935556fdd317294c67a737847.png"},1020(e,n,s){s.d(n,{A:()=>r});const r=s.p+"assets/images/hpc_1-a68aaed9f6176d7a379307a40d10c1c3.svg"},1560(e,n,s){s.d(n,{A:()=>r});const r=s.p+"assets/images/HPC_architecture-41812b9d27b4da09fa4b92ab9cabd700.png"},1571(e,n,s){s.d(n,{A:()=>r});const r=s.p+"assets/images/memory_before_stress-3a4b03c93c2340876dc5a870fe15d2e7.png"},1717(e,n,s){s.d(n,{A:()=>r});const r=s.p+"assets/images/add_db-12d303160989d2beda2434df173597ee.png"},3829(e,n,s){s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>u,frontMatter:()=>l,metadata:()=>r,toc:()=>o});var r=s(9936),t=s(4848),i=s(8453);const l={slug:"how-to-build-slurm-hpc-part-1",title:"Building a Slurm HPC Cluster (Part 1) - Single Node Setup and Fundamentals",authors:["river"],tags:["slurm","hpc","tutorial","getting-started"],image:"./imgs/hpc_1.svg"},c=void 0,a={image:s(1020).A,authorsImageUrls:[void 0]},o=[{value:"Series Overview",id:"series-overview",level:2},{value:"Why Slurm?",id:"why-slurm",level:2},{value:"Understanding Slurm Architecture",id:"understanding-slurm-architecture",level:2},{value:"Core Components",id:"core-components",level:3},{value:"Node Types",id:"node-types",level:3},{value:"Single Node Setup - Understanding the Fundamentals",id:"single-node-setup---understanding-the-fundamentals",level:2},{value:"Basic Installation",id:"basic-installation",level:3},{value:"Configuring slurm.conf",id:"configuring-slurmconf",level:3},{value:"Starting Services",id:"starting-services",level:3},{value:"Critical: Resource Limiting with cgroups",id:"critical-resource-limiting-with-cgroups",level:3},{value:"Enabling Accounting",id:"enabling-accounting",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Resources",id:"resources",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(n.p,{children:["Building a High-Performance Computing (HPC) cluster can seem daunting, but with the right approach, you can create a robust system for managing computational workloads. This is ",(0,t.jsx)(n.strong,{children:"Part 1"})," of a 3-part series where we'll build a complete Slurm cluster from scratch."]}),"\n",(0,t.jsx)(n.p,{children:"In this first post, we'll cover the fundamentals by setting up a single-node Slurm cluster and understanding the core concepts."}),"\n",(0,t.jsx)(n.h2,{id:"series-overview",children:"Series Overview"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Part 1 (This Post)"}),": Introduction, Architecture, and Single Node Setup"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/blog/how-to-build-slurm-hpc-part-2",children:"Part 2"})}),": Scaling to Production with Ansible"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/blog/how-to-build-slurm-hpc-part-3",children:"Part 3"})}),": Administration and Best Practices"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"why-slurm",children:"Why Slurm?"}),"\n",(0,t.jsx)(n.p,{children:"When it comes to job scheduling in HPC environments, several options exist including PBS, Grid Engine, and IBM's LSF. However, Slurm (Simple Linux Utility for Resource Management) stands out for several compelling reasons:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Open Source"}),": Free to use with a large, active community"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scalability"}),": Designed to scale from small clusters to the world's largest supercomputers"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Flexibility"}),": Fine-grained control over job scheduling, resource allocation, and priority settings"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Integration"}),": Works seamlessly with MPI, distributed computing frameworks (Spark, Ray, Dask), and monitoring tools"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Performance"}),": Optimized for high throughput with minimal overhead"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"understanding-slurm-architecture",children:"Understanding Slurm Architecture"}),"\n",(0,t.jsx)(n.p,{children:"Before diving into the implementation, it's crucial to understand the key components of a Slurm cluster:"}),"\n",(0,t.jsx)("figure",{markdown:"span",children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"HPC Architecture",src:s(1560).A+"",width:"1440",height:"960"})})}),"\n",(0,t.jsxs)(n.p,{children:["source: ",(0,t.jsx)(n.a,{href:"https://www.marquette.edu/high-performance-computing/architecture.php",children:"https://www.marquette.edu/high-performance-computing/architecture.php"})]}),"\n",(0,t.jsx)(n.h3,{id:"core-components",children:"Core Components"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"slurmctld (Controller Daemon)"}),": The brain of the cluster, running on the controller node. It handles job scheduling, resource tracking, and communicates with compute nodes."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"slurmd (Node Daemon)"}),": Runs on compute nodes to execute jobs and report status back to the controller."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"slurmdbd (Database Daemon)"}),": Optional but recommended for storing job accounting data, resource usage tracking, and fair-share scheduling."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"node-types",children:"Node Types"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Node Type"}),(0,t.jsx)(n.th,{children:"Services"}),(0,t.jsx)(n.th,{children:"Purpose"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Controller"}),(0,t.jsx)(n.td,{children:"slurmctld"}),(0,t.jsx)(n.td,{children:"Manages job scheduling and resources"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Compute"}),(0,t.jsx)(n.td,{children:"slurmd"}),(0,t.jsx)(n.td,{children:"Executes submitted jobs"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Login"}),(0,t.jsx)(n.td,{children:"Slurm clients"}),(0,t.jsx)(n.td,{children:"User access point for job submission"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Database"}),(0,t.jsx)(n.td,{children:"slurmdbd, MySQL/MariaDB"}),(0,t.jsx)(n.td,{children:"Stores accounting data"})]})]})]}),"\n",(0,t.jsx)("figure",{markdown:"span",children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Slurm Architecture",src:s(8584).A+"",width:"811",height:"681"})})}),"\n",(0,t.jsxs)(n.p,{children:["source: ",(0,t.jsx)(n.a,{href:"https://www.schedmd.com/",children:"https://www.schedmd.com/"})]}),"\n",(0,t.jsxs)(n.p,{children:["For a deeper understanding of Slurm architecture, check our ",(0,t.jsx)(n.a,{href:"/docs/resources/high-performance-computing/slurm-architecture",children:"Slurm Architecture documentation"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"single-node-setup---understanding-the-fundamentals",children:"Single Node Setup - Understanding the Fundamentals"}),"\n",(0,t.jsx)(n.p,{children:"Starting with a single-node setup helps you understand how Slurm works before scaling up. This approach is perfect for learning and local development."}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{markdown:"span",children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Slurm Logo",src:s(5177).A+"",width:"590",height:"540"})})})}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsx)(n.p,{children:"This setup runs on Ubuntu 20.04 and includes all standard Slurm features. Note that this configuration is for learning purposes - for production environments, you'll want the multi-node setup covered in Part 2."})}),"\n",(0,t.jsx)(n.h3,{id:"basic-installation",children:"Basic Installation"}),"\n",(0,t.jsx)(n.p,{children:"First, install the required Slurm components:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sudo apt-get update -y && sudo apt-get install -y slurmd slurmctld\n"})}),"\n",(0,t.jsx)(n.p,{children:"Verify the installation:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Locate slurmd and slurmctld\nwhich slurmd\n# Output: /usr/sbin/slurmd\n\nwhich slurmctld\n# Output: /usr/sbin/slurmctld\n"})}),"\n",(0,t.jsx)(n.h3,{id:"configuring-slurmconf",children:"Configuring slurm.conf"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"slurm.conf"})," file is the heart of your Slurm configuration. This file must be identical across all nodes in a cluster (but for now, we just have one node)."]}),"\n",(0,t.jsxs)(n.p,{children:["Create your ",(0,t.jsx)(n.code,{children:"slurm.conf"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cat <<EOF > slurm.conf\n# slurm.conf for a single-node Slurm cluster\nClusterName=localcluster\nSlurmctldHost=localhost\nMpiDefault=none\nProctrackType=proctrack/linuxproc\nReturnToService=2\nSlurmctldPidFile=/run/slurmctld.pid\nSlurmctldPort=6817\nSlurmdPidFile=/run/slurmd.pid\nSlurmdPort=6818\nSlurmdSpoolDir=/var/lib/slurm-llnl/slurmd\nSlurmUser=slurm\nStateSaveLocation=/var/lib/slurm-llnl/slurmctld\nSwitchType=switch/none\nTaskPlugin=task/none\n\n# TIMERS\nInactiveLimit=0\nKillWait=30\nMinJobAge=300\nSlurmctldTimeout=120\nSlurmdTimeout=300\nWaittime=0\n\n# SCHEDULING\nSchedulerType=sched/backfill\nSelectType=select/cons_tres\nSelectTypeParameters=CR_Core\n\n# ACCOUNTING (not enabled yet)\nAccountingStorageType=accounting_storage/none\nJobAcctGatherType=jobacct_gather/none\nJobAcctGatherFrequency=30\n\n# LOGGING\nSlurmctldDebug=info\nSlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log\nSlurmdDebug=info\nSlurmdLogFile=/var/log/slurm-llnl/slurmd.log\n\n# COMPUTE NODES (adjust CPUs and RealMemory to match your system)\nNodeName=localhost CPUs=2 Sockets=1 CoresPerSocket=2 ThreadsPerCore=1 RealMemory=1024 State=UNKNOWN\n\n# PARTITION CONFIGURATION\nPartitionName=LocalQ Nodes=ALL Default=YES MaxTime=INFINITE State=UP\nEOF\n\nsudo mv slurm.conf /etc/slurm-llnl/slurm.conf\n"})}),"\n",(0,t.jsx)(n.h3,{id:"starting-services",children:"Starting Services"}),"\n",(0,t.jsx)(n.p,{children:"Start the Slurm daemons:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Start slurmd (compute daemon)\nsudo service slurmd start\nsudo service slurmd status\n\n# Start slurmctld (controller daemon)\nsudo service slurmctld start\nsudo service slurmctld status\n"})}),"\n",(0,t.jsx)("figure",{markdown:"span",children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"slurmd status",src:s(9903).A+"",width:"1310",height:"351"})})}),"\n",(0,t.jsx)("figure",{markdown:"span",children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"slurmctld status",src:s(7310).A+"",width:"1310",height:"351"})})}),"\n",(0,t.jsx)(n.p,{children:"Test your setup by submitting a simple interactive job:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'srun --mem 500MB -c 1 --pty bash\n\n# Check job details\nsqueue -o "%i %P %u %T %M %l %D %C %m %R %Z %N" | column -t\n'})}),"\n",(0,t.jsx)(n.h3,{id:"critical-resource-limiting-with-cgroups",children:"Critical: Resource Limiting with cgroups"}),"\n",(0,t.jsxs)(n.admonition,{type:"warning",children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"This is a critical step that's often overlooked!"})}),(0,t.jsx)(n.p,{children:"Without proper cgroup configuration, jobs can exceed their allocated resources, potentially causing system instability or crashes. The job scheduler will accept resource limits, but won't actually enforce them."})]}),"\n",(0,t.jsx)(n.p,{children:"Let's test this problem first. Submit a job requesting 500MB and try to allocate much more:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"srun --mem 500MB -c 1 --pty bash\n\n# Try to allocate 1GB of memory (exceeding the 500MB limit)\ndeclare -a mem\ni=0\nwhile :; do\n    mem[$i]=$(head -c 100M </dev/zero | tr '\\000' 'x') \n    ((i++))\n    echo \"Allocated: $((i * 100)) MB\"\ndone\n"})}),"\n",(0,t.jsx)(n.p,{children:"Before submitting the job, memory usage is less than 200MB:"}),"\n",(0,t.jsx)("figure",{markdown:"span",children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Memory before stress",src:s(1571).A+"",width:"2432",height:"434"})})}),"\n",(0,t.jsx)(n.p,{children:"After allocating 1GB, the job is not killed due to missing control group (cgroup) configuration:"}),"\n",(0,t.jsx)("figure",{markdown:"span",children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Over resource limit",src:s(7937).A+"",width:"2432",height:"434"})})}),"\n",(0,t.jsx)(n.p,{children:"You'll notice the job continues running even after exceeding 500MB - that's the problem!"}),"\n",(0,t.jsx)(n.p,{children:"Now let's fix it with cgroups:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cat <<EOF > cgroup.conf\nCgroupAutomount=yes\nCgroupMountpoint=/sys/fs/cgroup\nConstrainCores=yes\nConstrainRAMSpace=yes\nConstrainDevices=yes\nConstrainSwapSpace=yes\nMaxSwapPercent=5\nMemorySwappiness=0\nEOF\n\nsudo mv cgroup.conf /etc/slurm-llnl/cgroup.conf\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Update ",(0,t.jsx)(n.code,{children:"slurm.conf"})," to use cgroup plugins:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'sudo sed -i -e "s|ProctrackType=proctrack/linuxproc|ProctrackType=proctrack/cgroup|" \\\n            -e "s|TaskPlugin=task/none|TaskPlugin=task/cgroup|" /etc/slurm-llnl/slurm.conf\n'})}),"\n",(0,t.jsx)(n.p,{children:"Enable cgroup in GRUB and reboot:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sudo sed -i 's/^GRUB_CMDLINE_LINUX=\"/GRUB_CMDLINE_LINUX=\"cgroup_enable=memory swapaccount=1 /' /etc/default/grub\nsudo update-grub\nsudo reboot\n"})}),"\n",(0,t.jsx)(n.p,{children:"After reboot, restart Slurm services:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sudo service slurmctld restart\nsudo service slurmd restart\n"})}),"\n",(0,t.jsx)(n.p,{children:"Now test again with the same memory allocation script - this time, the job will be killed when it exceeds the limit!"}),"\n",(0,t.jsx)("figure",{markdown:"span",children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Out of Memory",src:s(9860).A+"",width:"1305",height:"434"})})}),"\n",(0,t.jsx)(n.h3,{id:"enabling-accounting",children:"Enabling Accounting"}),"\n",(0,t.jsx)(n.p,{children:"Job accounting is essential for:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Tracking who is using resources"}),"\n",(0,t.jsx)(n.li,{children:"Monitoring job completion and failures"}),"\n",(0,t.jsx)(n.li,{children:"Enforcing resource limits per user/group"}),"\n",(0,t.jsx)(n.li,{children:"Fair-share scheduling"}),"\n"]}),"\n",(0,t.jsx)("figure",{markdown:"span",children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Accounting disabled",src:s(7248).A+"",width:"1310",height:"351"})})}),"\n",(0,t.jsx)(n.p,{children:"Install the required packages:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sudo apt-get install slurmdbd mariadb-server -y\n"})}),"\n",(0,t.jsx)(n.p,{children:"Create the database and user:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sudo service mysql start\n\nsudo mysql -e \"CREATE DATABASE slurm_acct_db;\"\nsudo mysql -e \"CREATE USER 'slurm'@'localhost' IDENTIFIED BY 'slurm';\"\nsudo mysql -e \"GRANT ALL PRIVILEGES ON slurm_acct_db.* TO 'slurm'@'localhost';\"\nsudo mysql -e \"FLUSH PRIVILEGES;\"\n"})}),"\n",(0,t.jsx)(n.p,{children:"Verify the database was created:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'sudo mysql -e "SHOW DATABASES;" \nsudo mysql -e "SELECT User, Host FROM mysql.user;"\n'})}),"\n",(0,t.jsx)("figure",{markdown:"span",children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Add database",src:s(1717).A+"",width:"1314",height:"539"})})}),"\n",(0,t.jsxs)(n.p,{children:["Configure ",(0,t.jsx)(n.code,{children:"slurmdbd"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cat <<EOF > slurmdbd.conf\nPidFile=/run/slurmdbd.pid\nLogFile=/var/log/slurm/slurmdbd.log\nDebugLevel=error\nDbdHost=localhost\nDbdPort=6819\n\n# DB connection data\nStorageType=accounting_storage/mysql\nStorageHost=localhost\nStoragePort=3306\nStorageUser=slurm\nStoragePass=slurm\nStorageLoc=slurm_acct_db\nSlurmUser=slurm\nEOF\n\nsudo mv slurmdbd.conf /etc/slurm-llnl/slurmdbd.conf\nsudo service slurmdbd start\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Update ",(0,t.jsx)(n.code,{children:"slurm.conf"})," to enable accounting:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'sudo sed -i -e "s|AccountingStorageType=accounting_storage/none|AccountingStorageType=accounting_storage/slurmdbd\\nAccountingStorageEnforce=associations,limits,qos\\nAccountingStorageHost=localhost\\nAccountingStoragePort=6819|" /etc/slurm-llnl/slurm.conf \n\nsudo sed -i -e "s|JobAcctGatherType=jobacct_gather/none|JobAcctGatherType=jobacct_gather/cgroup|" /etc/slurm-llnl/slurm.conf\n\nsudo systemctl restart slurmctld slurmd\n'})}),"\n",(0,t.jsx)(n.p,{children:"Add your cluster and user to accounting:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Add cluster\nsudo sacctmgr -i add cluster localcluster\n\n# Add account for your user\nsudo sacctmgr -i add account $USER Cluster=localcluster\n\n# Add your user to the account\nsudo sacctmgr -i add user $USER account=$USER DefaultAccount=$USER\n\nsudo systemctl restart slurmctld slurmd\n"})}),"\n",(0,t.jsx)("figure",{markdown:"span",children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Add account",src:s(5814).A+"",width:"1327",height:"511"})})}),"\n",(0,t.jsx)(n.p,{children:"Now test accounting by submitting a job and viewing its details:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Submit a test job\nsrun --mem 500MB -c 1 hostname\n\n# View accounting information\nsacct\n"})}),"\n",(0,t.jsx)("figure",{markdown:"span",children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Account usage",src:s(882).A+"",width:"1327",height:"171"})})}),"\n",(0,t.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,t.jsx)(n.p,{children:"In this first part of our series, we've covered:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Why Slurm"}),": Understanding the advantages of Slurm over alternatives"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Architecture"}),": Core components (slurmctld, slurmd, slurmdbd) and their roles"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Basic Setup"}),": Installing and configuring a single-node cluster"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Critical cgroups"}),": Why resource limiting is essential (and how to enable it)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Accounting"}),": Setting up job tracking and resource monitoring"]}),"\n"]}),"\n",(0,t.jsxs)(n.admonition,{type:"info",children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"What's Next?"})}),(0,t.jsxs)(n.p,{children:["In ",(0,t.jsx)(n.a,{href:"/blog/how-to-build-slurm-hpc-part-2",children:"Part 2"}),", we'll take this knowledge and scale to a multi-node production cluster using Ansible automation. We'll add monitoring with Grafana, alerting via Slack, and shared storage with NFS."]})]}),"\n",(0,t.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Full Documentation"}),": ",(0,t.jsx)(n.a,{href:"/docs/resources/high-performance-computing/how-to-build-slurm-single-node-with-full-functions",children:"Single Node Slurm Setup"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Architecture Details"}),": ",(0,t.jsx)(n.a,{href:"/docs/resources/high-performance-computing/slurm-architecture",children:"Slurm Architecture"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"HPC Overview"}),": ",(0,t.jsx)(n.a,{href:"/docs/resources/high-performance-computing/high-performance-computing-overview",children:"High-Performance Computing Overview"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Official Docs"}),": ",(0,t.jsx)(n.a,{href:"https://slurm.schedmd.com/",children:"SchedMD Slurm Documentation"})]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.em,{children:["This is Part 1 of the RiverXData series on building Slurm HPC clusters. Continue to ",(0,t.jsx)(n.a,{href:"/blog/how-to-build-slurm-hpc-part-2",children:"Part 2"})," to learn about production deployment with Ansible."]})})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},5177(e,n,s){s.d(n,{A:()=>r});const r=s.p+"assets/images/Slurm_logo-aa681ab3230b0e3101ed236f97071acb.svg"},5814(e,n,s){s.d(n,{A:()=>r});const r=s.p+"assets/images/add_account-29ee0d7948684467ad5ebbaf8ac1b09c.png"},7248(e,n,s){s.d(n,{A:()=>r});const r=s.p+"assets/images/sacct_disable-3536da7262517e12af0bb6c732d8d3ea.png"},7310(e,n,s){s.d(n,{A:()=>r});const r=s.p+"assets/images/slurmctld_status-4e7ed6630174c2f0bc017d60c84c72f1.png"},7937(e,n,s){s.d(n,{A:()=>r});const r=s.p+"assets/images/overresource_limit-4d711b28cf5adccae532ca1c9aa28286.png"},8453(e,n,s){s.d(n,{R:()=>l,x:()=>c});var r=s(6540);const t={},i=r.createContext(t);function l(e){const n=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),r.createElement(i.Provider,{value:n},e.children)}},8584(e,n,s){s.d(n,{A:()=>r});const r=s.p+"assets/images/slurm_arch-83c1c084e3e6e81e2babfd383b976d6d.gif"},9860(e,n,s){s.d(n,{A:()=>r});const r=s.p+"assets/images/oom-06f78a37e67a2fb7a3547efffd709e59.png"},9903(e,n,s){s.d(n,{A:()=>r});const r=s.p+"assets/images/slurmd_status-27a3e648f8da8db229ed803a5229e185.png"},9936(e){e.exports=JSON.parse('{"permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-1","source":"@site/blog/2026-01/2026-01-09.md","title":"Building a Slurm HPC Cluster (Part 1) - Single Node Setup and Fundamentals","description":"Building a High-Performance Computing (HPC) cluster can seem daunting, but with the right approach, you can create a robust system for managing computational workloads. This is Part 1 of a 3-part series where we\'ll build a complete Slurm cluster from scratch.","date":"2026-01-09T00:00:00.000Z","tags":[{"inline":true,"label":"slurm","permalink":"/river-docs/blog/tags/slurm"},{"inline":true,"label":"hpc","permalink":"/river-docs/blog/tags/hpc"},{"inline":true,"label":"tutorial","permalink":"/river-docs/blog/tags/tutorial"},{"inline":true,"label":"getting-started","permalink":"/river-docs/blog/tags/getting-started"}],"readingTime":7.65,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"how-to-build-slurm-hpc-part-1","title":"Building a Slurm HPC Cluster (Part 1) - Single Node Setup and Fundamentals","authors":["river"],"tags":["slurm","hpc","tutorial","getting-started"],"image":"./imgs/hpc_1.svg"},"unlisted":false,"prevItem":{"title":"Building a Slurm HPC Cluster (Part 2) - Scaling to Production with Ansible","permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-2"},"nextItem":{"title":"RIVER- A Web Application to Run Nf-Core","permalink":"/river-docs/blog/river-platform-and-nextflow"}}')}}]);