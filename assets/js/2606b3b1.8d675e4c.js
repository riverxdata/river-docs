"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[1595],{4756(e,n,t){t.d(n,{A:()=>s});const s=t.p+"assets/images/run_simple_workflow-42cfbfc6380f739a1e0887d87226f1e6.png"},7234(e,n,t){t.d(n,{A:()=>s});const s=t.p+"assets/images/fastp-8f6715db45ea345fdcd8e0fbbc5d58f5.png"},7941(e,n,t){t.d(n,{A:()=>s});const s=t.p+"assets/images/nextflow_optimization-fe3c2c1da10d871e8be47657ef3626c9.svg"},7976(e,n,t){t.d(n,{A:()=>s});const s=t.p+"assets/images/fastqc-b1524f902c60d6d461195238764d229c.png"},8379(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>c});var s=t(9516),a=t(4848),i=t(8453);const r={slug:"bioinformatics-computing-resource-optimization-part1",title:"Bioinformatics Cost Optimization for Computing Resources Using Nextflow (Part 1)",authors:["river"],tags:["nextflow","hpc","workflow-optimization"],image:"./imgs_17_01/nextflow_optimization.svg"},o=void 0,l={image:t(7941).A,authorsImageUrls:[void 0]},c=[{value:"Getting Started with a Simple Workflow",id:"getting-started-with-a-simple-workflow",level:2},{value:"Workflow Structure",id:"workflow-structure",level:3},{value:"Workflow Overview",id:"workflow-overview",level:2},{value:"Modules",id:"modules",level:2},{value:"FastQC",id:"fastqc",level:3},{value:"FASTP",id:"fastp",level:3},{value:"Configuration",id:"configuration",level:2},{value:"Input Data",id:"input-data",level:2},{value:"Running the Benchmark",id:"running-the-benchmark",level:2},{value:"How to Evaluate Performance",id:"how-to-evaluate-performance",level:2},{value:"FastQC",id:"fastqc-1",level:3},{value:"FASTP",id:"fastp-1",level:3},{value:"External benchmark",id:"external-benchmark",level:2},{value:"Trimgalore",id:"trimgalore",level:3},{value:"STAR",id:"star",level:3},{value:"Nf-core application",id:"nf-core-application",level:2},{value:"Lessons Learned",id:"lessons-learned",level:2}];function d(e){const n={a:"a",admonition:"admonition",br:"br",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:"Many bioinformatics tools provide options to adjust the number of threads or CPU cores, which can reduce execution time with a modest increase in resource cost. But does doubling computational resources always result in processes running twice as fast? In practice, the speed-up is often less than linear, and each tool behaves differently."}),"\n",(0,a.jsx)(n.p,{children:"In this post, I'll demonstrate how to optimize resource usage for individual tools, using a simple benchmarking workflow as a case study. The goal is to help you tune your workflow for efficient, production-ready execution."}),"\n",(0,a.jsx)(n.p,{children:"With Nextflow\u2014now one of the most widely used workflow managers\u2014you gain powerful built-in features for tracing and reporting resource usage. By leveraging these capabilities, you can accurately measure CPU, memory, and run time for each step, helping you make informed optimization decisions to save both time and cost."}),"\n",(0,a.jsx)(n.h2,{id:"getting-started-with-a-simple-workflow",children:"Getting Started with a Simple Workflow"}),"\n",(0,a.jsx)(n.p,{children:"nf-core and Nextflow have become increasingly complex. Therefore, instead of jumping directly to nf-core/rnaseq, I'll use a simple workflow to demonstrate the concepts and what we need to know first. Later, we can apply the lessons learned from this workflow to improve more complex pipelines."}),"\n",(0,a.jsx)(n.p,{children:"I created a simple workflow using two common tools that support thread/core options."}),"\n",(0,a.jsx)(n.admonition,{type:"info",children:(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Tools"}),": FastQC and fastp"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Benchmarking approach"}),": Run tools with increasing CPU and memory allocations (2x and 3x) to observe resource consumption and execution time"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Repository"}),": ",(0,a.jsx)(n.a,{href:"https://github.com/nttg8100/nextflow-cost-optimization",children:"https://github.com/nttg8100/nextflow-cost-optimization"})]}),"\n"]})}),"\n",(0,a.jsx)(n.h3,{id:"workflow-structure",children:"Workflow Structure"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 benchmark.nf    # Main workflow file\n\u251c\u2500\u2500 inputs\n\u2502   \u251c\u2500\u2500 1_samplesheet.csv\n\u2502   \u2514\u2500\u2500 full_samplesheet.csv\n\u251c\u2500\u2500 modules\n\u2502   \u251c\u2500\u2500 fastp.nf\n\u2502   \u2514\u2500\u2500 fastqc.nf\n\u251c\u2500\u2500 nextflow.config\n\u251c\u2500\u2500 pixi.lock\n\u251c\u2500\u2500 pixi.toml\n"})}),"\n",(0,a.jsx)(n.h2,{id:"workflow-overview",children:"Workflow Overview"}),"\n",(0,a.jsx)(n.p,{children:"The following is a simple workflow with FastQC and fastp. It accepts a CSV file containing metadata and file paths for each sample, then performs FastQC and fastp independently."}),"\n",(0,a.jsx)(n.admonition,{type:"tip",children:(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Aliasing strategy"}),": Use aliases with prefix ",(0,a.jsx)(n.code,{children:"_\\<number>"})," (e.g., FASTQC_1, FASTQC_2) to configure increasing computing resources for benchmarking"]}),"\n"]})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"include { FASTQC as FASTQC_1 } from './modules/fastqc.nf'\ninclude { FASTQC as FASTQC_2 } from './modules/fastqc.nf'\ninclude { FASTQC as FASTQC_3 } from './modules/fastqc.nf'\ninclude { FASTP as FASTP_1 } from './modules/fastp.nf'\ninclude { FASTP as FASTP_2 } from './modules/fastp.nf'\ninclude { FASTP as FASTP_3 } from './modules/fastp.nf'\n\n// Parse CSV; skip header, split by comma, expand to tuple for process\nChannel\n    .fromPath(params.input)\n    .splitCsv(header:true)\n    .map { row ->\n        // Compose the meta map and reads list\n        def meta = [ id: row.sample, strandedness: row.strandedness ]\n        def reads = [ row.fastq_1, row.fastq_2 ]\n        tuple(meta, reads)\n    }\n    .set { sample_ch }\n\nworkflow {\n    if (params.run_fastqc) {\n        // fastqc benchmarks\n        FASTQC_1(sample_ch)\n        FASTQC_2(sample_ch)\n        FASTQC_3(sample_ch)\n    }\n    \n    if (params.run_fastp) {\n        // fastp benchmarks\n        FASTP_1(sample_ch,[], false, false, false)\n        FASTP_2(sample_ch,[], false, false, false)\n        FASTP_3(sample_ch,[], false, false, false)\n    }\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"modules",children:"Modules"}),"\n",(0,a.jsx)(n.h3,{id:"fastqc",children:"FastQC"}),"\n",(0,a.jsx)(n.p,{children:"This module uses containers to run FastQC with configurable resources."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'process FASTQC {\n    tag "${meta.id}"\n    label \'process_medium\'\n\n    conda "${moduleDir}/environment.yml"\n    container "${ workflow.containerEngine == \'singularity\' && !task.ext.singularity_pull_docker_container ?\n        \'https://depot.galaxyproject.org/singularity/fastqc:0.12.1--hdfd78af_0\' :\n        \'biocontainers/fastqc:0.12.1--hdfd78af_0\' }"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path("*.html"), emit: html\n    tuple val(meta), path("*.zip") , emit: zip\n    path  "versions.yml"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args          = task.ext.args ?: \'\'\n    def prefix        = task.ext.prefix ?: "${meta.id}"\n    // Make list of old name and new name pairs to use for renaming in the bash while loop\n    def old_new_pairs = reads instanceof Path || reads.size() == 1 ? [[ reads, "${prefix}.${reads.extension}" ]] : reads.withIndex().collect { entry, index -> [ entry, "${prefix}_${index + 1}.${entry.extension}" ] }\n    def rename_to     = old_new_pairs*.join(\' \').join(\' \')\n    def renamed_files = old_new_pairs.collect{ _old_name, new_name -> new_name }.join(\' \')\n\n    // The total amount of allocated RAM by FastQC equals the number of threads defined (--threads) times the amount of RAM defined (--memory)\n    // https://github.com/s-andrews/FastQC/blob/1faeea0412093224d7f6a07f777fad60a5650795/fastqc#L211-L222\n    // Dividing task.memory by task.cpus allows us to stick to the requested amount of RAM in the label\n    def memory_in_mb = task.memory ? task.memory.toUnit(\'MB\') / task.cpus : null\n    // FastQC memory value allowed range (100 - 10000)\n    def fastqc_memory = memory_in_mb > 10000 ? 10000 : (memory_in_mb < 100 ? 100 : memory_in_mb)\n\n    """\n    printf "%s %s\\\\n" ${rename_to} | while read old_name new_name; do\n        [ -f "\\${new_name}" ] || ln -s \\$old_name \\$new_name\n    done\n\n    fastqc \\\\\n        ${args} \\\\\n        --threads ${task.cpus} \\\\\n        --memory ${fastqc_memory} \\\\\n        ${renamed_files}\n\n    cat <<-END_VERSIONS > versions.yml\n    "${task.process}":\n        fastqc: \\$( fastqc --version | sed \'/FastQC v/!d; s/.*v//\' )\n    END_VERSIONS\n    """\n\n    stub:\n    def prefix = task.ext.prefix ?: "${meta.id}"\n    """\n    touch ${prefix}.html\n    touch ${prefix}.zip\n\n    cat <<-END_VERSIONS > versions.yml\n    "${task.process}":\n        fastqc: \\$( fastqc --version | sed \'/FastQC v/!d; s/.*v//\' )\n    END_VERSIONS\n    """\n}\n'})}),"\n",(0,a.jsx)(n.p,{children:"Overall, this script creates a command to run with threads and memory options. Here, memory is allocated per thread instead of total memory.\nIf I allocate 8 threads and 16 GB RAM, it will run the command as follows:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash -ue\nprintf "%s %s\\n" SRX1603630_T1_1.fastq.gz GM12878_REP2_1.gz SRX1603630_T1_2.fastq.gz GM12878_REP2_2.gz | while read old_name new_name; do\n    [ -f "${new_name}" ] || ln -s $old_name $new_name\ndone\n\nfastqc \\\n     \\\n    --threads 8 \\\n    --memory 2048 \\\n    GM12878_REP2_1.gz GM12878_REP2_2.gz\n\ncat <<-END_VERSIONS > versions.yml\n"FASTQC_3":\n    fastqc: $( fastqc --version | sed \'/FastQC v/!d; s/.*v//\' )\nEND_VERSIONS\n'})}),"\n",(0,a.jsx)(n.h3,{id:"fastp",children:"FASTP"}),"\n",(0,a.jsx)(n.p,{children:"fastp is simpler regarding CPUs and memory. It allows direct specification without needing to recalculate memory per thread."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'process FASTP {\n    tag "$meta.id"\n    label \'process_medium\'\n\n    conda "${moduleDir}/environment.yml"\n    container "${ workflow.containerEngine == \'singularity\' && !task.ext.singularity_pull_docker_container ?\n        \'https://community-cr-prod.seqera.io/docker/registry/v2/blobs/sha256/88/889a182b8066804f4799f3808a5813ad601381a8a0e3baa4ab8d73e739b97001/data\' :\n        \'community.wave.seqera.io/library/fastp:0.24.0--62c97b06e8447690\' }"\n\n    input:\n    tuple val(meta), path(reads)\n    path  adapter_fasta\n    val   discard_trimmed_pass\n    val   save_trimmed_fail\n    val   save_merged\n\n    output:\n    tuple val(meta), path(\'*.fastp.fastq.gz\') , optional:true, emit: reads\n    tuple val(meta), path(\'*.json\')           , emit: json\n    tuple val(meta), path(\'*.html\')           , emit: html\n    tuple val(meta), path(\'*.log\')            , emit: log\n    tuple val(meta), path(\'*.fail.fastq.gz\')  , optional:true, emit: reads_fail\n    tuple val(meta), path(\'*.merged.fastq.gz\'), optional:true, emit: reads_merged\n    path "versions.yml"                       , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: \'\'\n    def prefix = task.ext.prefix ?: "${meta.id}"\n    def adapter_list = adapter_fasta ? "--adapter_fasta ${adapter_fasta}" : ""\n    def fail_fastq = save_trimmed_fail && meta.single_end ? "--failed_out ${prefix}.fail.fastq.gz" : save_trimmed_fail && !meta.single_end ? "--failed_out ${prefix}.paired.fail.fastq.gz --unpaired1 ${prefix}_1.fail.fastq.gz --unpaired2 ${prefix}_2.fail.fastq.gz" : \'\'\n    def out_fq1 = discard_trimmed_pass ?: ( meta.single_end ? "--out1 ${prefix}.fastp.fastq.gz" : "--out1 ${prefix}_1.fastp.fastq.gz" )\n    def out_fq2 = discard_trimmed_pass ?: "--out2 ${prefix}_2.fastp.fastq.gz"\n    // Added soft-links to original fastqs for consistent naming in MultiQC\n    // Use single ended for interleaved. Add --interleaved_in in config.\n    if ( task.ext.args?.contains(\'--interleaved_in\') ) {\n        """\n        [ ! -f  ${prefix}.fastq.gz ] && ln -sf $reads ${prefix}.fastq.gz\n\n        fastp \\\\\n            --stdout \\\\\n            --in1 ${prefix}.fastq.gz \\\\\n            --thread $task.cpus \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $adapter_list \\\\\n            $fail_fastq \\\\\n            $args \\\\\n            2>| >(tee ${prefix}.fastp.log >&2) \\\\\n        | gzip -c > ${prefix}.fastp.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        "${task.process}":\n            fastp: \\$(fastp --version 2>&1 | sed -e "s/fastp //g")\n        END_VERSIONS\n        """\n    } else if (meta.single_end) {\n        """\n        [ ! -f  ${prefix}.fastq.gz ] && ln -sf $reads ${prefix}.fastq.gz\n\n        fastp \\\\\n            --in1 ${prefix}.fastq.gz \\\\\n            $out_fq1 \\\\\n            --thread $task.cpus \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $adapter_list \\\\\n            $fail_fastq \\\\\n            $args \\\\\n            2>| >(tee ${prefix}.fastp.log >&2)\n\n        cat <<-END_VERSIONS > versions.yml\n        "${task.process}":\n            fastp: \\$(fastp --version 2>&1 | sed -e "s/fastp //g")\n        END_VERSIONS\n        """\n    } else {\n        def merge_fastq = save_merged ? "-m --merged_out ${prefix}.merged.fastq.gz" : \'\'\n        """\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -sf ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -sf ${reads[1]} ${prefix}_2.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}_1.fastq.gz \\\\\n            --in2 ${prefix}_2.fastq.gz \\\\\n            $out_fq1 \\\\\n            $out_fq2 \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $adapter_list \\\\\n            $fail_fastq \\\\\n            $merge_fastq \\\\\n            --thread $task.cpus \\\\\n            --detect_adapter_for_pe \\\\\n            $args \\\\\n            2>| >(tee ${prefix}.fastp.log >&2)\n\n        cat <<-END_VERSIONS > versions.yml\n        "${task.process}":\n            fastp: \\$(fastp --version 2>&1 | sed -e "s/fastp //g")\n        END_VERSIONS\n        """\n    }\n\n    stub:\n    def prefix              = task.ext.prefix ?: "${meta.id}"\n    def is_single_output    = task.ext.args?.contains(\'--interleaved_in\') || meta.single_end\n    def touch_reads         = (discard_trimmed_pass) ? "" : (is_single_output) ? "echo \'\' | gzip > ${prefix}.fastp.fastq.gz" : "echo \'\' | gzip > ${prefix}_1.fastp.fastq.gz ; echo \'\' | gzip > ${prefix}_2.fastp.fastq.gz"\n    def touch_merged        = (!is_single_output && save_merged) ? "echo \'\' | gzip >  ${prefix}.merged.fastq.gz" : ""\n    def touch_fail_fastq    = (!save_trimmed_fail) ? "" : meta.single_end ? "echo \'\' | gzip > ${prefix}.fail.fastq.gz" : "echo \'\' | gzip > ${prefix}.paired.fail.fastq.gz ; echo \'\' | gzip > ${prefix}_1.fail.fastq.gz ; echo \'\' | gzip > ${prefix}_2.fail.fastq.gz"\n    """\n    $touch_reads\n    $touch_fail_fastq\n    $touch_merged\n    touch "${prefix}.fastp.json"\n    touch "${prefix}.fastp.html"\n    touch "${prefix}.fastp.log"\n\n    cat <<-END_VERSIONS > versions.yml\n    "${task.process}":\n        fastp: \\$(fastp --version 2>&1 | sed -e "s/fastp //g")\n    END_VERSIONS\n    """\n}\n'})}),"\n",(0,a.jsx)(n.p,{children:"This generates the command to run fastp with 2 reads, where memory is allocated automatically."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash -ue\n[ ! -f  GM12878_REP2_1.fastq.gz ] && ln -sf SRX1603630_T1_1.fastq.gz GM12878_REP2_1.fastq.gz\n[ ! -f  GM12878_REP2_2.fastq.gz ] && ln -sf SRX1603630_T1_2.fastq.gz GM12878_REP2_2.fastq.gz\nfastp \\\n    --in1 GM12878_REP2_1.fastq.gz \\\n    --in2 GM12878_REP2_2.fastq.gz \\\n    --out1 GM12878_REP2_1.fastp.fastq.gz \\\n    --out2 GM12878_REP2_2.fastp.fastq.gz \\\n    --json GM12878_REP2.fastp.json \\\n    --html GM12878_REP2.fastp.html \\\n     \\\n     \\\n     \\\n    --thread 2 \\\n    --detect_adapter_for_pe \\\n     \\\n    2>| >(tee GM12878_REP2.fastp.log >&2)\n\ncat <<-END_VERSIONS > versions.yml\n"FASTP_1":\n    fastp: $(fastp --version 2>&1 | sed -e "s/fastp //g")\nEND_VERSIONS\n'})}),"\n",(0,a.jsx)(n.h2,{id:"configuration",children:"Configuration"}),"\n",(0,a.jsx)(n.p,{children:"This configuration allows the workflow to run with:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Docker/Singularity for containerization"}),"\n",(0,a.jsx)(n.li,{children:"Tracing and reporting capabilities"}),"\n",(0,a.jsx)(n.li,{children:"Named processes for memory and CPU allocation"}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"params{\n    outdir = \"result\"\n    trace_report_suffix = new java.util.Date().format( 'yyyy-MM-dd_HH-mm-ss')\n    input = \"inputs/full_samplsheet.csv\"\n}\n\nprofiles {\n    singularity {\n        singularity.enabled     = true\n        singularity.autoMounts  = true\n        conda.enabled           = false\n        docker.enabled          = false\n        podman.enabled          = false\n        shifter.enabled         = false\n        charliecloud.enabled    = false\n        apptainer.enabled       = false\n    }\n    docker {\n        docker.enabled          = true\n        conda.enabled           = false\n        singularity.enabled     = false\n        podman.enabled          = false\n        shifter.enabled         = false\n        charliecloud.enabled    = false\n        apptainer.enabled       = false\n        docker.runOptions       = '-u $(id -u):$(id -g)'\n    }\n    emulate_amd64 {\n        // Run AMD64 containers on ARM hardware using emulation (slower but more compatible)\n        docker.runOptions       = '-u $(id -u):$(id -g) --platform=linux/amd64'\n    }\n\n}\n\nsingularity.registry = 'quay.io'\ndocker.registry = 'quay.io'\n\nprocess{\n    withName:FASTQC_1{\n        cpus = 2\n        memory = 4.GB\n    }\n    withName:FASTQC_2{\n        cpus = 4\n        memory = 8.GB\n    }\n    withName:FASTQC_3{\n        cpus = 8\n        memory = 16 .GB\n    }\n    withName:FASTP_1{\n        cpus = 2\n        memory = 4.GB\n    }\n    withName:FASTP_2{\n        cpus = 4\n        memory = 8.GB\n    }\n    withName:FASTP_3{\n        cpus = 8\n        memory = 16 .GB\n    }\n}\n\ntimeline {\n    enabled = true\n    file    = \"${params.outdir}/pipeline_info/execution_timeline_${params.trace_report_suffix}.html\"\n}\nreport {\n    enabled = true\n    file    = \"${params.outdir}/pipeline_info/execution_report_${params.trace_report_suffix}.html\"\n}\ntrace {\n    enabled = true\n    file    = \"${params.outdir}/pipeline_info/execution_trace_${params.trace_report_suffix}.txt\"\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"input-data",children:"Input Data"}),"\n",(0,a.jsx)(n.p,{children:"The sample sheet below includes two RNA-seq samples, each with input files ranging from 6 to 7 GB, representing real-world data sizes."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"sample,fastq_1,fastq_2,strandedness\nGM12878_REP1,s3://ngi-igenomes/test-data/rnaseq/SRX1603629_T1_1.fastq.gz,s3://ngi-igenomes/test-data/rnaseq/SRX1603629_T1_2.fastq.gz,reverse\nGM12878_REP2,s3://ngi-igenomes/test-data/rnaseq/SRX1603630_T1_1.fastq.gz,s3://ngi-igenomes/test-data/rnaseq/SRX1603630_T1_2.fastq.gz,reverse\n"})}),"\n",(0,a.jsx)(n.h2,{id:"running-the-benchmark",children:"Running the Benchmark"}),"\n",(0,a.jsxs)(n.p,{children:["The above scripts are stored in the GitHub repository: ",(0,a.jsx)(n.a,{href:"https://github.com/nttg8100/nextflow-cost-optimization",children:"https://github.com/nttg8100/nextflow-cost-optimization"})]}),"\n",(0,a.jsx)(n.p,{children:"Clone the repository:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/nttg8100/nextflow-cost-optimization.git\ncd nextflow-cost-optimization\n"})}),"\n",(0,a.jsx)(n.admonition,{type:"tip",children:(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Requirements"}),": At least 8 CPUs and 16 GB RAM (adjust based on your computing resources in ",(0,a.jsx)(n.code,{children:"nextflow.config"}),")"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Alternative"}),": If you don't have Docker, run with Singularity"]}),"\n"]})}),"\n",(0,a.jsx)(n.p,{children:"Run the workflow, which will execute commands using Nextflow (installed via pixi). Docker is required on your machine."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# docker\nmake test-fastqc-docker-amd64\nmake test-fastp-docker-amd64\n\n# if you use MacBook with ARM\nmake test-fastqc-docker-arm64\nmake test-fastp-docker-arm64\n\n# if you do not have Docker installed, use Singularity\nmake test-fastqc-singularity\nmake test-fastp-singularity\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"run workflow",src:t(4756).A+"",width:"1123",height:"580"})}),"\n",(0,a.jsx)(n.h2,{id:"how-to-evaluate-performance",children:"How to Evaluate Performance"}),"\n",(0,a.jsx)(n.p,{children:"What we need to optimize is finding the minimal CPUs and memory that can run tasks efficiently without being too slow.\nOpen the HTML reports generated when you run the workflow to analyze performance. Below are the metrics we should use for evaluation:"}),"\n",(0,a.jsxs)(n.admonition,{type:"info",children:[(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"%cpu"})}),":",(0,a.jsx)(n.br,{}),"\n","The average percentage of CPU usage by the process during its execution. For multi-threaded jobs, this value can exceed 100% (e.g., 400% for a process using all 4 allocated CPUs fully)."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"rss"})}),":",(0,a.jsx)(n.br,{}),"\n",'"Resident Set Size" \u2014 the maximum amount of physical memory (RAM) used by the process, measured in bytes. This shows how much memory your process actually consumed at peak.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"realtime"})}),":",(0,a.jsx)(n.br,{}),"\n","The total elapsed (wall-clock) time taken by the process, from start to finish, including waiting and processing time."]}),"\n"]}),"\n"]}),(0,a.jsx)(n.p,{children:"These metrics help you tune and optimize your pipeline by revealing the true resource usage for each step."})]}),"\n",(0,a.jsx)(n.h3,{id:"fastqc-1",children:"FastQC"}),"\n",(0,a.jsx)(n.p,{children:"Interestingly, the execution time barely changes even when memory and CPUs are increased. From this experiment, we should consider using the minimal setup with 2 CPUs and 4 GB RAM while achieving nearly the same performance."}),"\n",(0,a.jsx)(n.admonition,{type:"warning",children:(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Note"}),": The module is taken directly from nf-core. There may be a bug related to FastQC that hasn't been identified yet."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Finding"}),": With real datasets, we can use minimal resources (much cheaper) without significant impact on running time!"]}),"\n"]})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"report_fastqc",src:t(7976).A+"",width:"1121",height:"706"})}),"\n",(0,a.jsx)(n.h3,{id:"fastp-1",children:"FASTP"}),"\n",(0,a.jsxs)(n.p,{children:["With fastp, increasing memory and CPUs shows a slight reduction in running time.\n",(0,a.jsx)(n.img,{alt:"report_fastp",src:t(7234).A+"",width:"1121",height:"706"})]}),"\n",(0,a.jsx)(n.h2,{id:"external-benchmark",children:"External benchmark"}),"\n",(0,a.jsx)(n.p,{children:"The simple example above can be modified appropriately for more tools. Here are external benchmarks showing that adjusting computing resources helps reduce running time while keeping costs nearly the same:"}),"\n",(0,a.jsx)(n.h3,{id:"trimgalore",children:"Trimgalore"}),"\n",(0,a.jsxs)(n.p,{children:["Reference: ",(0,a.jsx)(n.a,{href:"https://github.com/FelixKrueger/TrimGalore/blob/master/CHANGELOG.md#version-060-release-on-1-mar-2019",children:"https://github.com/FelixKrueger/TrimGalore/blob/master/CHANGELOG.md#version-060-release-on-1-mar-2019"}),",\nUsing 4 CPUs is a good choice as it can reduce time by nearly 4x when compared with 1 CPU only.\n",(0,a.jsx)(n.img,{alt:"report_trimgalore",src:t(8506).A+"",width:"1810",height:"541"})]}),"\n",(0,a.jsx)(n.h3,{id:"star",children:"STAR"}),"\n",(0,a.jsxs)(n.p,{children:["Reference: ",(0,a.jsx)(n.a,{href:"https://academic.oup.com/bioinformatics/article/29/1/15/272537",children:"https://academic.oup.com/bioinformatics/article/29/1/15/272537"}),"\nUsing 12 threads can align nearly 2x the reads when compared with 6 threads, while memory usage remains unchanged."]}),"\n",(0,a.jsx)(n.h2,{id:"nf-core-application",children:"Nf-core application"}),"\n",(0,a.jsx)(n.admonition,{type:"tip",children:(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"If you run workflow with HPC, consider the ratio between CPUs and memory. If you spend all of memory, there is no cpus left for another jobs"}),"\n"]})}),"\n",(0,a.jsxs)(n.p,{children:["nf-core is one of the largest and most active bioinformatics workflow communities. However, their pipelines often use generic resource labels to define CPU and memory requirements for each process. For instance, the ",(0,a.jsx)(n.a,{href:"https://github.com/nf-core/rnaseq/blob/master/conf/base.config",children:(0,a.jsx)(n.strong,{children:"nf-core/rnaseq"})})," pipeline assigns default resources using labels like medium for tools such as FASTQC and FASTP."]}),"\n",(0,a.jsx)(n.p,{children:"Without tuning these settings for each specific tool, your workflow can waste resources and increase costs. In practice, both FASTQC and FASTP typically only need 2 CPUs and 4 GB RAM\u2014much less than the default medium allocation."}),"\n",(0,a.jsxs)(n.p,{children:["This is especially important when running batch jobs in the cloud (AWS, Google Cloud, Azure, etc.), where virtual machines are often configured with a CPU",":Memory"," ratio of 1:2. Requesting resources with a higher ratio, like 1:6, can increase queue times and lead to inefficient use of resources, as you may wait longer for an available instance and not fully utilize the extra memory."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"withLabel:process_single {\n    cpus   = { 1                   }\n    memory = { 6.GB * task.attempt }\n    time   = { 4.h  * task.attempt }\n}\nwithLabel:process_low {\n    cpus   = { 2     * task.attempt }\n    memory = { 12.GB * task.attempt }\n    time   = { 4.h   * task.attempt }\n}\nwithLabel:process_medium {\n    cpus   = { 6     * task.attempt }\n    memory = { 36.GB * task.attempt }\n    time   = { 8.h   * task.attempt }\n}\nwithLabel:process_high {\n    cpus   = { 12    * task.attempt }\n    memory = { 72.GB * task.attempt }\n    time   = { 16.h  * task.attempt }\n}\nwithLabel:process_long {\n    time   = { 20.h  * task.attempt }\n}\nwithLabel:process_high_memory {\n    memory = { 200.GB * task.attempt }\n}\n"})}),"\n",(0,a.jsx)(n.p,{children:"To overwrite the existing worflow process cpus and memory, use the withName for specific tasks or adjust the label resources"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"process{\n    cpus   = { 1      * task.attempt }\n    memory = { 2.GB   * task.attempt }\n    time   = { 4.h    * task.attempt }\n\n    errorStrategy = { task.exitStatus in ((130..145) + 104 + 175) ? 'retry' : 'finish' }\n    maxRetries    = 1\n    maxErrors     = '-1'\n\n    withLabel:process_single {\n        cpus   = { 1                   }\n        memory = { 2.GB * task.attempt }\n        time   = { 4.h  * task.attempt }\n    }\n    withLabel:process_low {\n        cpus   = { 2     * task.attempt }\n        memory = { 4.GB * task.attempt }\n        time   = { 4.h   * task.attempt }\n    }\n    withLabel:process_medium {\n        cpus   = { 8     * task.attempt }\n        memory = { 16.GB * task.attempt }\n        time   = { 8.h   * task.attempt }\n    }\n    withLabel:process_high {\n        cpus   = { 16    * task.attempt }\n        memory = { 32.GB * task.attempt }\n        time   = { 16.h  * task.attempt }\n    }\n    withLabel:process_long {\n        time   = { 20.h  * task.attempt }\n    }\n    withLabel:process_high_memory {\n        memory = { 32.GB * task.attempt }\n    }\n    withLabel:error_ignore {\n        errorStrategy = 'ignore'\n    }\n    withLabel:error_retry {\n        errorStrategy = 'retry'\n        maxRetries    = 2\n    }\n    withLabel: process_gpu {\n        ext.use_gpu = { workflow.profile.contains('gpu') }\n        accelerator = { workflow.profile.contains('gpu') ? 1 : null }\n    }\n    withName: FASTQC{\n        cpus   = { 2     * task.attempt }\n        memory = { 4.GB  * task.attempt }\n        time   = { 4.h   * task.attempt }\n    }\n    withName: '.*:ALIGN_STAR:STAR_ALIGN|.*:ALIGN_STAR:STAR_ALIGN_IGENOMES'{\n        container = \"docker.io/nttg8100/star:2.7.11b\"\n    }\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"lessons-learned",children:"Lessons Learned"}),"\n",(0,a.jsx)(n.p,{children:"The two simple modules above help us select CPU and memory more easily. To optimize based on cost, here are some tips:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"For tasks shorter than 20 minutes, increasing CPUs and memory doesn't significantly reduce running time\u2014use minimal resources instead"}),"\n",(0,a.jsx)(n.li,{children:"Some tools benefit greatly from multiple CPUs and memory. For these, you should increase computing resources."}),"\n",(0,a.jsx)(n.li,{children:"Using 80% of available resources is typically sufficient. Sometimes when the input is slightly larger, tools can still run without crashing."}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>r,x:()=>o});var s=t(6540);const a={},i=s.createContext(a);function r(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(i.Provider,{value:n},e.children)}},8506(e,n,t){t.d(n,{A:()=>s});const s=t.p+"assets/images/trimgalore-3ba46f53dec2b72135acd67d8859f450.png"},9516(e){e.exports=JSON.parse('{"permalink":"/river-docs/blog/bioinformatics-computing-resource-optimization-part1","source":"@site/blog/2026-01/2026-01-18.md","title":"Bioinformatics Cost Optimization for Computing Resources Using Nextflow (Part 1)","description":"Many bioinformatics tools provide options to adjust the number of threads or CPU cores, which can reduce execution time with a modest increase in resource cost. But does doubling computational resources always result in processes running twice as fast? In practice, the speed-up is often less than linear, and each tool behaves differently.","date":"2026-01-18T00:00:00.000Z","tags":[{"inline":true,"label":"nextflow","permalink":"/river-docs/blog/tags/nextflow"},{"inline":true,"label":"hpc","permalink":"/river-docs/blog/tags/hpc"},{"inline":true,"label":"workflow-optimization","permalink":"/river-docs/blog/tags/workflow-optimization"}],"readingTime":12.55,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"bioinformatics-computing-resource-optimization-part1","title":"Bioinformatics Cost Optimization for Computing Resources Using Nextflow (Part 1)","authors":["river"],"tags":["nextflow","hpc","workflow-optimization"],"image":"./imgs_17_01/nextflow_optimization.svg"},"unlisted":false,"prevItem":{"title":"Bioinformatics Cost Optimization For Input Using Nextflow (Part 2)","permalink":"/river-docs/blog/bioinformatics-computing-resource-optimization-part2"},"nextItem":{"title":"Pixi- New conda era","permalink":"/river-docs/blog/pixi-is-new-conda-based-era"}}')}}]);