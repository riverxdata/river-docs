"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[6959],{4924(n){n.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"how-to-version-control-git-bioinformatics-part-2","metadata":{"permalink":"/river-docs/blog/how-to-version-control-git-bioinformatics-part-2","source":"@site/blog/2026-01/2026-01-16.md","title":"The Evolution of Version Control - Git\'s Role in Reproducible Bioinformatics (Part 2)","description":"Welcome to Part 2 of our series on version control in bioinformatics. In Part 1, we introduced Git fundamentals, branching strategies, and collaborative workflows. In this post, we\'ll dive into how Continuous Integration and Continuous Deployment (CI/CD) can transform your bioinformatics projects. If these concepts are new to you, don\'t worry\u2014this guide will walk you through managing your bioinformatics repository to ensure your work is easily reproducible on any machine. Whether your server is wiped or you need to spin up a new virtual machine, you\'ll be able to quickly rerun your pipeline. With CI/CD, every code update can automatically trigger tests on a small dataset to verify everything works before scaling up, ensuring that new changes don\'t break your results or workflows.","date":"2026-01-16T00:00:00.000Z","tags":[{"inline":true,"label":"git","permalink":"/river-docs/blog/tags/git"},{"inline":true,"label":"version-control","permalink":"/river-docs/blog/tags/version-control"},{"inline":true,"label":"bioinformatics","permalink":"/river-docs/blog/tags/bioinformatics"},{"inline":true,"label":"reproducibility","permalink":"/river-docs/blog/tags/reproducibility"},{"inline":true,"label":"pipelines","permalink":"/river-docs/blog/tags/pipelines"},{"inline":true,"label":"ci-cd","permalink":"/river-docs/blog/tags/ci-cd"},{"inline":true,"label":"github-actions","permalink":"/river-docs/blog/tags/github-actions"}],"readingTime":13.21,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"how-to-version-control-git-bioinformatics-part-2","title":"The Evolution of Version Control - Git\'s Role in Reproducible Bioinformatics (Part 2)","authors":["river"],"tags":["git","version-control","bioinformatics","reproducibility","pipelines","ci-cd","github-actions"],"image":"./imgs/github_intro.svg"},"unlisted":false,"nextItem":{"title":"The Evolution of Version Control - Git\'s Role in Reproducible Bioinformatics (Part 1)","permalink":"/river-docs/blog/how-to-version-control-git-bioinformatics-part-1"}},"content":"Welcome to Part 2 of our series on version control in bioinformatics. In [Part 1](./2026-01-15.md), we introduced Git fundamentals, branching strategies, and collaborative workflows. In this post, we\'ll dive into how Continuous Integration and Continuous Deployment (CI/CD) can transform your bioinformatics projects. If these concepts are new to you, don\'t worry\u2014this guide will walk you through managing your bioinformatics repository to ensure your work is easily reproducible on any machine. Whether your server is wiped or you need to spin up a new virtual machine, you\'ll be able to quickly rerun your pipeline. With CI/CD, every code update can automatically trigger tests on a small dataset to verify everything works before scaling up, ensuring that new changes don\'t break your results or workflows.\\n\\n\x3c!--truncate--\x3e\\n- **Part 1**: History, basics, GitHub integration, and hands-on tutorials\\n- **Part 2 (This Post)**: Local testing, CI/CD implementation, automated testing, and reproducible pipelines\\n\\n:::info\\nIn this tutorial, we will:\\n+ Add code to download data\\n+ Write scripts to use FastQC and MultiQC\\n:::\\n\\n## Git commands\\nThis tutorial is fully committed via a GitHub repository. You can clone and checkout to specific branches to see how it is developed and committed.\\nFirst, clone the repository, then:\\n+ Learn how to view commit history (branches are referenced by their names)\\n+ Learn how to checkout to a specific commit\\n+ Learn how to see the difference between the current commit and a specific commit\\n\\n:::tip\\n+ To quit the git prompt, type `q`\\n+ Create your own repository for a more practical approach\\n+ Check the differences in files, then copy them to your own repository to follow the tutorial\\n:::\\n\\n```bash\\n# clone\\ngit clone git@github.com:nttg8100/ngs-quality-control-for-blog-example.git\\ncd ngs-quality-control-for-blog-example\\n\\n# checkout to branch for the relevant tutorial step, e.g. feature/add-data\\ngit checkout feature/add-data\\n```\\n\\n### How to show commit log\\nGit provides commands to view the history and check the difference between two commits.\\n```bash\\ngit log\\n```\\nIt will show details as below:\\n```bash\\ncommit ca29351295a4148bda73b6bf76a42cc7b834d1b6 (HEAD -> feature/add-data, origin/feature/add-data)\\nAuthor: Thanh-Giang (River) Tan Nguyen <nttg8100@gmail.com>\\nDate:   Mon Jan 19 23:34:58 2026 +0700\\n\\n    feat: add makefile to get data\\n\\ncommit 70d584a726fb5418274448a4f51ca07d94b104be (origin/main, origin/HEAD, main, feature/testing-rebase)\\nMerge: c731b9a 230795b\\nAuthor: Thanh Giang Nguyen Tan <64969412+nttg8100@users.noreply.github.com>\\nDate:   Sat Jan 17 16:42:11 2026 +0700\\n\\n    Merge pull request #1 from nttg8100/feature/environment-setup\\n    \\n    feat: add pixi setup\\n\\ncommit 230795b7b8b03087ef7ee938d333161c9fcc0731 (origin/feature/environment-setup)\\nAuthor: Thanh-Giang (River) Tan Nguyen <nttg8100@gmail.com>\\nDate:   Sat Jan 17 15:05:28 2026 +0700\\n\\n    feat: add pixi setup\\n\\ncommit c731b9a80dfb22936bfcf94442bdf9b488a90d3c (feature/tesing-rebase)\\nAuthor: Thanh Giang Nguyen Tan <64969412+nttg8100@users.noreply.github.com>\\nDate:   Wed Jan 14 23:09:30 2026 +0700\\n```\\n\\nTo see more precisely what you need:\\n```bash\\ngit log --oneline\\n```\\n\\nThe one-line format is good enough:\\n```bash\\nca29351 (HEAD -> feature/add-data, origin/feature/add-data) feat: add makefile to get data\\n70d584a (origin/main, origin/HEAD, main, feature/testing-rebase) Merge pull request #1 from nttg8100/feature/environment-setup\\n230795b (origin/feature/environment-setup) feat: add pixi setup\\nc731b9a (feature/tesing-rebase) Initial commit\\n```\\n\\nNow you can see what I did on the repository. Following this tutorial, you can change to a specific commit to track it.\\n\\n### How to checkout to specific commit\\nYou can simply use the commit hash. For example, if I want to see all files from commit `230795b` which is on the branch `feature/testing-rebase`:\\n```bash\\ngit checkout 230795b\\n```\\nNow your code editor will show the files with content related to this commit.\\n\\n\\n### How to view the difference between two commits or your current state\\nIf you have made changes to your files but have not committed them yet:\\n```bash\\ngit diff\\n```\\n\\nI made a change and have not committed it here:\\n```bash\\ndiff --git a/Makefile b/Makefile\\nindex ae08113..c2e5b87 100644\\n--- a/Makefile\\n+++ b/Makefile\\n@@ -10,4 +10,6 @@ data/GSE110004:\\n        wget https://raw.githubusercontent.com/nf-core/test-datasets/rnaseq/testdata/GSE110004/SRR6357070_2.fastq.gz\\n \\n clean:\\n-       rm -rf data/GSE110004\\n\\\\ No newline at end of file\\n+       rm -rf data/GSE110004\\n+\\n+Change here\\n\\\\ No newline at end of file\\n```\\n\\nIf you want to compare between two branches/commits:\\n```bash\\ngit diff ca29351..70d584a\\n```\\n\\nThe changes between the above commits:\\n```bash\\ndiff --git a/.gitignore b/.gitignore\\nindex cfca1e8..1c3ecb3 100644\\n--- a/.gitignore\\n+++ b/.gitignore\\n@@ -19,5 +19,3 @@ bin-release/\\n # pixi environments\\n .pixi/*\\n !.pixi/config.toml\\n-data\\n-\\ndiff --git a/Makefile b/Makefile\\nindex ae08113..bcaa6d7 100644\\n--- a/Makefile\\n+++ b/Makefile\\n@@ -1,13 +1,3 @@\\n ${HOME}/.pixi/bin/pixi:\\n        mkdir -p ${HOME}/.pixi/bin\\n-       curl -sSL https://pixi.dev/install.sh | sh\\n-\\n-\\n-data/GSE110004:\\n-       mkdir -p data/GSE110004\\n-       cd data/GSE110004 && \\\\\\n-       wget https://raw.githubusercontent.com/nf-core/test-datasets/rnaseq/testdata/GSE110004/SRR6357070_1.fastq.gz && \\\\\\n-       wget https://raw.githubusercontent.com/nf-core/test-datasets/rnaseq/testdata/GSE110004/SRR6357070_2.fastq.gz\\n-\\n-clean:\\n-       rm -rf data/GSE110004\\n\\\\ No newline at end of file\\n+       curl -sSL https://pixi.dev/install.sh | sh\\n\\\\ No newline at end of file\\n```\\n### How to roll back\\nIf you checkout a specific commit and add a few changes to test according to the tutorial, `git` requires you to commit your changes.\\nYou can simply roll back to the original version:\\n```bash\\ngit checkout -- .\\n```\\n\\nSometimes you want to get files from another branch. You can use:\\n```bash\\ngit checkout <branch name> <relative file path>\\n# ex: git checkout main .gitignore\\n```\\n\\n## Add data\\nWe need to create a new branch from updated main\\n```bash\\ngit checkout main\\ngit pull origin main\\ngit switch -c feature/add-data\\n```\\n\\nNow we\'ll add a few target setups to the Makefile to automate data download and cleanup tasks for your pipeline:\\n\\n- **`data/GSE110004` target**: Downloads example FASTQ files from a public test dataset into the `data/GSE110004` directory. This ensures everyone working on the project uses the same input data, improving reproducibility.\\n- **`clean` target**: Removes the downloaded data directory, making it easy to reset your environment or save disk space.\\n\\nThis approach helps standardize data management and makes it easier for collaborators (and CI/CD systems) to set up and test the pipeline with minimal manual steps.\\n\\nUpdate your `Makefile` with the following:\\n\\n```bash\\n${HOME}/.pixi/bin/pixi:\\n        mkdir -p ${HOME}/.pixi/bin\\n        curl -sSL https://pixi.dev/install.sh | sh\\n\\ndata:\\n  mkdir -p data/GSE110004\\n  wget -O data/GSE110004/SRR6351937_1.fastq.gz https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR635/007/SRR6351937/SRR6351937_1.fastq.gz\\n  wget -O data/GSE110004/SRR6351937_2.fastq.gz https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR635/007/SRR6351937/SRR6351937_2.fastq.gz\\n\\nclean:\\n  rm -rf data\\n```\\n\\nFollowing tutorial 1, you can commit, push, create a PR, and merge to main:\\n```bash\\ngit commit -m \\"feat: add makefile to get data\\"\\ngit push origin feature/add-data\\n```\\n\\n## Add workflow\\nCreate new branch\\n```bash\\ngit checkout main\\ngit pull\\ngit switch -c feature/workflow\\n```\\n### Add workflow main.sh script and testing\\nHere I create a simple bash script that will run using command lines with inputs as arguments.\\n```bash\\n# create workflow file using bash script, this runs for only paired reads of 1 sample\\ncat << EOF >> main.sh\\n#!/bin/bash\\n\\n# Usage: ./main.sh <read1.fastq.gz> <read2.fastq.gz> <output_folder>\\n\\nset -e\\n\\nif [ \\"\\\\$#\\" -ne 3 ]; then\\n    echo \\"Usage: $0 <read1.fastq.gz> <read2.fastq.gz> <output_folder>\\"\\n    exit 1\\nfi\\n\\nREAD1=\\"\\\\$1\\"\\nREAD2=\\"\\\\$2\\"\\nOUTDIR=\\"\\\\$3\\"\\n\\nmkdir -p \\"\\\\$OUTDIR\\"\\n\\necho \\"Running FastQC on \\\\$READ1 and \\\\$READ2...\\"\\nfastqc \\"\\\\$READ1\\" \\"\\\\$READ2\\" -o \\"\\\\$OUTDIR\\"\\n\\necho \\"Running MultiQC in \\\\$OUTDIR...\\"\\nmultiqc \\"\\\\$OUTDIR\\" -o \\"\\\\$OUTDIR\\"\\n\\necho \\"All done. Results are in \\\\$OUTDIR\\"\\nEOF\\n\\n# chmod to be executed\\nchmod +x main.sh\\n\\n# get help script\\nbash main.sh\\n# Usage: zsh <read1.fastq.gz> <read2.fastq.gz> <output_folder>\\n\\n# test it with your data\\n# activate pixi \\npixi shell\\n# run interactively\\nbash main.sh data/GSE110004/*_1.fastq.gz data/GSE110004/*_2.fastq.gz result\\n\\n# run with pixi directly\\npixi run bash main.sh data/GSE110004/*_1.fastq.gz data/GSE110004/*_2.fastq.gz result\\n```\\n![qc_wf](./imgs_16_01/qc_workflow.png)\\n\\n\\n### Use Makefile to add tests\\nNow we can quickly run tests after updating the `Makefile`:\\n```Makefile\\n.PHONY: install-pixi test-e2e clean\\n${HOME}/.pixi/bin/pixi:\\n\\tmkdir -p ${HOME}/.pixi/bin\\n\\tcurl -sSL https://pixi.dev/install.sh | sh\\n\\ndata/GSE110004:\\n\\tmkdir -p data/GSE110004\\n\\tcd data/GSE110004 && \\\\\\n\\twget https://raw.githubusercontent.com/nf-core/test-datasets/rnaseq/testdata/GSE110004/SRR6357070_1.fastq.gz && \\\\\\n\\twget https://raw.githubusercontent.com/nf-core/test-datasets/rnaseq/testdata/GSE110004/SRR6357070_2.fastq.gz\\n\\ntest-e2e: data/GSE110004\\n\\t${HOME}/.pixi/bin/pixi run bash main.sh data/GSE110004/*_1.fastq.gz data/GSE110004/*_2.fastq.gz result\\n\\nclean:\\n\\trm -rf data/GSE110004\\n```\\n\\n\\nQuickly run the test:\\n```bash\\nmake test-e2e\\n```\\n\\nUpdate your `.gitignore` to avoid committing the results:\\n```bash\\n# Build and Release Folders\\nbin-debug/\\nbin-release/\\n[Oo]bj/\\n[Bb]in/\\n\\n# Other files and folders\\n.settings/\\n\\n# Executables\\n*.swf\\n*.air\\n*.ipa\\n*.apk\\n\\n# Project files, i.e. `.project`, `.actionScriptProperties` and `.flexProperties`\\n# should NOT be excluded as they contain compiler settings and other important\\n# information for Eclipse / Flash Builder.\\n# pixi environments\\n.pixi/*\\n!.pixi/config.toml\\ndata\\nresult\\n```\\n\\nFollowing tutorial 1, you can commit, push, create a PR, and merge to main:\\n```bash\\ngit add .\\ngit commit -m \\"feat: add workflow\\"\\ngit push origin feature/workflow \\n```\\n\\nThen, in your repository root folder, run these commands to get the data:\\n:::tip\\n+ Currently, a few make target commands are simple and can be run directly, but in the future, it would be better to update them with more commands\\n+ Ex: `make clean` can be used to delete more files/folders\\n:::\\n```bash\\ngit checkout \\n# download data\\nmake data\\n# test that data is downloaded\\nls -lah  data/GSE110004\\n# total 4.3M\\n# drwxrwxr-x 2 giangnguyen giangnguyen 4.0K Jan 19 01:00 .\\n# drwxrwxr-x 3 giangnguyen giangnguyen 4.0K Jan 19 01:00 ..\\n# -rw-rw-r-- 1 giangnguyen giangnguyen 2.2M Jan 19 01:00 SRR6357070_1.fastq.gz\\n# -rw-rw-r-- 1 giangnguyen giangnguyen 2.2M Jan 19 01:00 SRR6357070_2.fastq.gz\\n\\n# delete data folder\\nmake clean\\n```\\n## Configure testing\\nSo far, everything has been done locally. However, when you add new features\u2014for example, adding `trimmomatic` to trim the FastQC files\u2014you need to run the tests again to ensure they can be executed without errors. This is the simplest testing practice for developing bioinformatics projects.\\n\\nGitHub provides GitHub Actions to help with this:\\n:::info\\n+ For more detailed examples on GitHub Actions, follow the official documentation: https://docs.github.com/en/actions/get-started/quickstart\\n+ Here, we introduce how it works and how we can apply it simply\\n:::\\n\\n\\n### Create GitHub Action files\\nCreate a new branch for CI/CD:\\n```bash\\ngit checkout main\\ngit pull\\ngit switch -c feature/cicd\\n```\\nConfigure the workflow:\\n```bash\\nmkdir -p .github/workflows\\n# create workflow file\\ncat << EOF >> .github/workflows/test-e2e.yaml\\nname: Testing e2e\\n\\non:\\n  push:\\n    branches: [ main ]\\n  pull_request:\\n    branches: [ main ]\\n\\njobs:\\n  clone:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - name: Checkout repository\\n        uses: actions/checkout@v4\\n\\n      - name: Running testing\\n        run: |\\n          make test-e2e\\nEOF\\n```\\n\\nWhat it does: \\n+ Runs workflow named Testing e2e\\n+ Triggers on pull requests to main\\n+ Triggers on push or merge into main\\n+ Starts a job on ubuntu-latest runner\\n+ Clones the repository using actions/checkout\\n+ Runs end-to-end tests via make test-e2e\\n+ Passes if tests succeed\\n+ Fails if tests return a non-zero exit code\\n\\n### Running GitHub Actions locally\\nIf we only run tests when creating a PR or pushing to `main`, you may make mistakes and create many commits just to trigger testing. Here, we can use `act`, a local command line tool that can help:\\n:::info\\n+ For more details on how to use `act`: https://nektosact.com\\n+ Install Docker on your local machine first\\n:::\\n\\n```bash\\n# install\\npixi add act\\n# run e2e test locally, it will run this on your local machine\\nact -W .github/workflows/test-e2e.yaml\\n```\\n![act_failed](./imgs_16_01/act_failed.png)\\n\\n:::warning\\n+ As you can see, it failed because pixi is not installed. This is because the `make test-e2e` target does not trigger pixi installation\\n+ Also, there is a wrong URL for installing pixi: `https://pixi.dev/install.sh` should be replaced with `https://pixi.sh/install.sh`\\n+ You should add pixi installation as a dependency for `test-e2e`\\n:::\\n\\nUpdate dependencies of `test-e2e` in the `Makefile`:\\n```bash\\n.PHONY: test-e2e clean\\n${HOME}/.pixi/bin/pixi:\\n\\tmkdir -p ${HOME}/.pixi/bin\\n\\tcurl -sSL https://pixi.sh/install.sh | sh\\n\\ndata/GSE110004:\\n\\tmkdir -p data/GSE110004\\n\\tcd data/GSE110004 && \\\\\\n\\twget https://raw.githubusercontent.com/nf-core/test-datasets/rnaseq/testdata/GSE110004/SRR6357070_1.fastq.gz && \\\\\\n\\twget https://raw.githubusercontent.com/nf-core/test-datasets/rnaseq/testdata/GSE110004/SRR6357070_2.fastq.gz\\n\\ntest-e2e: data/GSE110004 ${HOME}/.pixi/bin/pixi\\n\\t${HOME}/.pixi/bin/pixi run bash main.sh data/GSE110004/*_1.fastq.gz data/GSE110004/*_2.fastq.gz result\\n\\nclean:\\n\\trm -rf data/GSE110004\\n```\\n\\nNow you can rerun to test successfully, as shown below:\\n![act_success1](./imgs_16_01/act_success1.png)\\n\\n![act_success2](./imgs_16_01/act_success2.png)\\n\\n## What is CI/CD in bioinformatics?\\n### What is CI/CD in software development\\n![cicd](./imgs_16_01/cicd.webp)\\nReference: https://viblo.asia/p/cau-hinh-cicd-voi-github-phan-1-mot-it-ly-thuyet-Qbq5Q9NL5D8\\n\\nWhat exactly are CI (Continuous Integration) and CD (Continuous Delivery)? These terms are commonly used in software development.\\nIn the figure above, software can be developed with many new features, which will trigger specific jobs with actions.\\nFor example, we can the workflow to clone the repository on a GitHub Actions instance and run tests. This approach depends on many use cases.\\n\\n### How does it work in bioinformatics?\\nThat is exactly what I showed you above. If you want to develop software, follow the standard CI/CD software development process. In contrast,\\nbioinformatics typically combines these tools together. This means they will run scripts (bash, Nextflow, WDL, Snakemake, etc.) to combine tools\\nin a workflow manner. The release is the repository itself with the scripts inside (CD), while the CI is the process we set up to run\\nthe pipeline with testing datasets.\\n\\nNow you can create a PR, which will always trigger the testing that helps you validate whether your changes may fail the pipeline.\\n![PR_GHA](./imgs_16_01/PR_GHA.png)\\n\\n:::tip\\n+ Use secrets in `Settings` to work with credentials (passwords, usernames, keys)\\n+ Use the Makefile to help sync the local environment with the GitHub Actions environment. This makes it easy to change the CI/CD runner\\n+ Use matrix strategy to run tests faster\\n:::\\n\\nCheck GitHub Actions logs for debugging:\\n![GHA](./imgs_16_01/GHA_log.png)\\n\\n## Review\\n\\nIn this tutorial, we explored how to transform a basic bioinformatics project into a reproducible, testable pipeline using Git, Makefiles, and CI/CD practices. Here are the key takeaways:\\n\\n### Git Commands for Project Management\\n- Use `git log` and `git log --oneline` to view commit history\\n- Checkout to specific commits to track development progress\\n- Use `git diff` to compare changes between commits or branches\\n- Roll back to original states with `git checkout -- .`\\n- Copy files from other branches using `git checkout <branch> <file>`\\n\\n### Makefile Automation\\n- Create targets for data download (`make data`) and cleanup (`make clean`)\\n- Add test targets (`make test-e2e`) that chain dependencies together\\n- Ensure reproducibility by standardizing commands across environments\\n\\n### Workflow Scripts\\n- Create bash scripts that accept command-line arguments\\n- Combine tools like FastQC and MultiQC in a workflow\\n- Test workflows locally before pushing to remote\\n\\n### CI/CD with GitHub Actions\\n- Create workflow files in `.github/workflows/`\\n- Trigger tests on push to main or pull requests\\n- Use the same Makefile commands in CI as in local development\\n- Check GitHub Actions logs for debugging failed tests\\n\\n### Local Testing with `act`\\n- Test GitHub Actions workflows locally using `act`\\n- Catch issues before pushing to remote repositories\\n- Reduce the number of commits needed to fix CI failures\\n\\n### CI/CD in Bioinformatics Context\\n- Unlike traditional software, bioinformatics CI/CD often focuses on pipeline validation\\n- The \\"delivery\\" is often the repository itself with reproducible workflows\\n- Testing involves running entire pipelines on test datasets\\n- This ensures changes don\'t break existing analyses\\n\\nBy implementing these practices, you create a robust bioinformatics project that is:\\n- **Reproducible**: Anyone can reproduce your results on any machine\\n- **Testable**: Automated tests catch errors before they reach production\\n- **Maintainable**: Clear history and documentation make updates easier\\n- **Collaborative**: CI/CD ensures all changes are validated before merging\\n\\nReady to implement CI/CD in your own bioinformatics projects? Start by adding a simple Makefile target, then gradually build up your testing and automation capabilities."},{"id":"how-to-version-control-git-bioinformatics-part-1","metadata":{"permalink":"/river-docs/blog/how-to-version-control-git-bioinformatics-part-1","source":"@site/blog/2026-01/2026-01-15.md","title":"The Evolution of Version Control - Git\'s Role in Reproducible Bioinformatics (Part 1)","description":"In Part 1 (this post), we explore the history of Git, its integration with GitHub, and basic hands-on tutorials. Part 2 (coming soon) will cover real-world bioinformatics examples and advanced workflows with best practices.","date":"2026-01-15T00:00:00.000Z","tags":[{"inline":true,"label":"git","permalink":"/river-docs/blog/tags/git"},{"inline":true,"label":"version-control","permalink":"/river-docs/blog/tags/version-control"},{"inline":true,"label":"bioinformatics","permalink":"/river-docs/blog/tags/bioinformatics"},{"inline":true,"label":"reproducibility","permalink":"/river-docs/blog/tags/reproducibility"},{"inline":true,"label":"pipelines","permalink":"/river-docs/blog/tags/pipelines"}],"readingTime":12.08,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"how-to-version-control-git-bioinformatics-part-1","title":"The Evolution of Version Control - Git\'s Role in Reproducible Bioinformatics (Part 1)","authors":["river"],"tags":["git","version-control","bioinformatics","reproducibility","pipelines"],"image":"./imgs/github_intro.svg"},"unlisted":false,"prevItem":{"title":"The Evolution of Version Control - Git\'s Role in Reproducible Bioinformatics (Part 2)","permalink":"/river-docs/blog/how-to-version-control-git-bioinformatics-part-2"},"nextItem":{"title":"Building a Slurm HPC Cluster (Part 3) - Administration and Best Practices","permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-3"}},"content":"In Part 1 (this post), we explore the history of Git, its integration with GitHub, and basic hands-on tutorials. Part 2 (coming soon) will cover real-world bioinformatics examples and advanced workflows with best practices.\\n\\nThis part focuses on practical applications, including NGS quality control using multiqc and fastqc.\\n\\n\x3c!--truncate--\x3e\\n\\n- **Part 1 (This Post)**: History, basics, GitHub integration, and hands-on tutorials\\n- **Part 2 (Coming Soon)**: Real-world bioinformatics examples and advanced workflows with best practices\\n\\nHere I provide a real-world example of how to create a simple repository that enables reproducible work for NGS short read data quality control. I\'ll explain each step using a GitHub repository.\\n\\n## Create a Repository\\n\\nAs the creator, you are the admin of the repository. You can configure permissions to control who can read or modify it. Here are some common setup tips:\\n\\n:::tip\\n+ **Visibility**: If you want to share your work with the community, use `public`. Otherwise, use `private`\\n+ **Add README**: A README file helps others understand how your repository is structured. Beyond the standard `README.md` for general users, consider adding `README.developer.md` for developer-specific information\\n+ **.gitignore**: This file specifies which files or folders should not be pushed to GitHub\\n+ **License**: This determines how others can use your work. For example, MIT allows free usage, while other licenses may require purchase for commercial use\\n:::\\n\\n![create_repo](./imgs_16_01/create_repository.png)\\n\\n## Clone\\n\\nCloning creates a copy of your repository on your local computer. When working in a team, each member has their own local copy. GitHub serves as a central location, similar to Google Drive, that allows you to sync your source code.\\n\\nGitHub provides SSH protocol authentication for downloading resources, but it doesn\'t work like a remote server with shell access.\\n\\n:::info\\n+ After creating a repository, clone it to start working\\n+ You\'ll need to authenticate with GitHub on your local computer or HPC first. I recommend using SSH keys\\n+ Click the green \\"Code\\" button, select the \\"SSH\\" tab, copy the URL, and use it to clone the repository\\n:::\\n\\n![created](./imgs_16_01/created_clone.png)\\n\\nFor authentication, follow the official GitHub documentation: https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account\\n\\n:::tip\\n+ This site also has documentation on connecting to remote servers using SSH: [**SSH setup**](/docs/resources/administration/ssh-remote-server)\\n+ You can stop after creating the key pair\\n:::\\n\\nAdd your public key to GitHub by navigating to `Settings` \u2192 `SSH` section\\n\\n![ssh](./imgs_16_01/add_ssh_key.png)\\n\\nNow test your connection to GitHub:\\n\\n```bash\\nssh git@github.com\\n# PTY allocation request failed on channel 0\\n# Hi nttg8100! You\'ve successfully authenticated, but GitHub does not provide shell access.\\n# Connection to github.com closed.\\n```\\n\\nYou\'re ready to clone. Open your terminal, navigate to your working directory, and clone the repository:\\n\\n:::tip\\n+ To clone a specific branch or tag, add `-b <branch-name>`\\n:::\\n\\n```bash\\n# clone (equivalent to: git clone git@github.com:nttg8100/ngs-quality-control-for-blog-example.git -b main)\\ngit clone git@github.com:nttg8100/ngs-quality-control-for-blog-example.git\\ncd ngs-quality-control-for-blog-example\\n```\\n\\nOpen the code with your editor. Here I use Visual Studio Code:\\n\\n:::info\\nDownload and install Visual Studio Code: https://code.visualstudio.com/download\\n:::\\n\\n![download_vscode](./imgs_16_01/visual_studio_code.png)\\n\\n## Your Code Editor\\n\\nIn your terminal, you can open the entire repository (which is just a folder). Simply use your mouse to select the terminal icon in the footer.\\n\\n:::tip\\n+ Now you can select any file you want to edit\u2014everything is a text file\\n:::\\n\\n```bash\\ncode .\\n```\\n\\n![editor](./imgs_16_01/editor.png)\\n\\nNow follow these steps to establish good version control practices.\\n\\n## Branching Strategy\\n\\n![branch](./imgs_16_01/git_branch.png)\\n\\nA branching strategy is a workflow that helps teams manage codebase changes in a structured way. Git branches allow you to work on new features, bug fixes, or experiments without affecting the main codebase.\\n\\n**Common branching strategies:**\\n\\n- **Main (or `master`) branch:**\\n  The stable branch that always contains production-ready code\\n\\n- **Feature branches:**\\n  Create a new branch for each feature or task (e.g., `feature/quality-control`). This keeps development isolated until the feature is complete\\n\\n- **Development branch:**\\n  Some teams use a `develop` branch as an integration branch for features before merging into `main`\\n\\n- **Hotfix/bugfix branches:**\\n  For urgent fixes, create a branch from `main` (e.g., `hotfix/fix-typo`) and merge back after testing\\n\\n**Typical workflow:**\\n1. Create a new branch from `main` for your feature or fix\\n2. Work on your changes and commit them to your branch\\n3. Open a pull request (PR) to merge your branch into `main` (or `develop`)\\n4. Review, test, and merge the PR\\n\\nThis approach:\\n- Keeps the `main` branch stable\\n- Makes collaboration easier\\n- Helps track changes and review code before merging\\n\\nNow let\'s create a new branch. Our goal is to set up easy installation of fastqc and multiqc. We\'ll write a bash script for analysis later. For simplicity, we\'ll develop feature branches and merge them directly to main, which works well for small projects like this.\\n\\n```bash\\n# Create and switch to a new branch from main\\ngit switch -c feature/environment-setup\\n```\\n\\n## New Branch: Environment Setup\\n\\nInstead of conda, I prefer to install pixi, then install multiqc and fastqc. Create a Makefile for installation\u2014you can either edit a new `Makefile` manually or generate it with bash.\\n\\n:::info\\nA Makefile is a special file used by the make build automation tool. It defines rules and instructions for building and managing projects, especially in languages like C/C++. However, it can be used for any project, including Node.js, React, and TypeScript, to automate repetitive tasks.\\n\\n**Key Concepts:**\\n- **Targets**: What you want to build (e.g., a file or a label for a task)\\n- **Dependencies**: Files or targets that must be up-to-date before the target can be built\\n- **Recipes**: Shell commands to run to build the target\\n:::\\n\\n```bash\\ncat << EOF >> Makefile\\n${HOME}/.pixi/bin/pixi:\\n  mkdir -p ${HOME}/.pixi/bin\\n  curl -sSL https://pixi.dev/install.sh | sh\\nEOF\\n\\n# Verify the newly created file content\\ncat Makefile\\n```\\n\\n:::tip\\n**Pro Tip:**\\nWhen writing your `Makefile`, use `.PHONY` targets for commands that don\'t produce files with the same name. This prevents conflicts and ensures your commands always run as expected. Keep installation steps modular\u2014separate environment setup, tool installation, and analysis scripts into different targets for clarity and reusability.\\n:::\\n\\n```bash\\n# Install pixi\\nmake ~/.pixi/bin/pixi\\n\\n# Initialize pixi in your current repository\\npixi init --channel conda-forge --channel bioconda\\n\\n# Install multiqc and fastqc\\npixi add fastqc multiqc\\n\\n# Activate the pixi shell\\npixi shell\\n\\n# Verify tools are installed\\nwhich fastqc\\nwhich multiqc\\n# /home/giangnguyen/Documents/dev/docs/ngs-quality-control-for-blog-example/.pixi/envs/default/bin/fastqc\\n# /home/giangnguyen/Documents/dev/docs/ngs-quality-control-for-blog-example/.pixi/envs/default/bin/multiqc\\n```\\n\\n## Common Git Commands\\n\\n:::warning\\nYou can check git history to see who made changes\u2014useful for accountability when needed\\n:::\\n\\nCheck your current status to see uncommitted files. Use command line to add files to your commit:\\n\\n```bash\\ngit status\\n```\\n\\n![uncommited](./imgs_16_01/status_uncommit.png)\\n\\nAdd files to commit and review changes:\\n\\n```bash\\n# Add files one by one\\ngit add .gitignore\\ngit add .gitattributes\\ngit add Makefile\\ngit add pixi.lock\\ngit add pixi.toml\\n\\n# Or add all files/folders in the current repository\\ngit add .\\n\\n# Verify staged files\\ngit status\\n```\\n\\n![committed](./imgs_16_01/status_after_add.png)\\n\\nSometimes you might accidentally stage the wrong files. Remove them from the current commit:\\n\\n```bash\\n# Create a wrong file\\ntouch wrong_file.txt\\ngit add wrong_file.txt\\n\\n# Restore it to unstaged status\\ngit restore --staged wrong_file.txt\\n```\\n\\n![restore](./imgs_16_01/restore.png)\\n\\nIf you need files locally but don\'t want to commit them, add them to .gitignore. This prevents them from being committed even when using `git add .`. .gitignore can ignore both files and folders.\\n\\n:::tip\\n+ For better control, create .gitignore in the specific folder/subfolder where unwanted files/folders are located\\n+ Common .gitignore patterns: environment folders (`.env`, `.pixi`), `node_modules`, data folders (`data`), `*.tar.gz`, cache folders (`__pycache__`)\\n+ To ignore an entire folder but keep specific files, use `!<filename>` to keep only needed files\\n```\\nfolder\\n!folder/file.txt\\n```\\n:::\\n\\n```bash\\n# Add to gitignore\\necho \\"wrong_file.txt\\" > .gitignore\\n\\n# View the file\\ncat .gitignore\\n\\n# Try to add the file again\u2014it won\'t be staged now and will show a warning\\ngit add wrong_file.txt\\n\\n# Check status\\ngit status\\n```\\n\\n![gitignore](./imgs_16_01/gitignore.png)\\n\\nNow create a commit with all your staged files:\\n\\n```bash\\ngit commit -m \\"feat: add pixi setup\\"\\n```\\n\\n![commit](./imgs_16_01/commit.png)\\n\\nPush your changes to GitHub, the remote server that tracks your development progress:\\n\\n```bash\\n# Push to remote\\ngit push origin feature/environment-setup\\n\\n# View your remote origin to confirm where updates are pushed\\n# This shows I cloned using the SSH credential\\ngit config --get remote.origin.url\\n# git@github.com:nttg8100/ngs-quality-control-for-blog-example.git\\n```\\n\\n![push](./imgs_16_01/push_origin.png)\\n\\n## Check Updates on GitHub\\n\\nVisit your GitHub repository to see the new branch. In this example:\\nhttps://github.com/nttg8100/ngs-quality-control-for-blog-example\\n\\n![updated_branch](./imgs_16_01/new_branch_created.png)\\n\\nNow let\'s move on to remote collaboration with your team.\\n\\n## Pull Request\\n\\nA Pull Request (PR) compares your changes with the target branch. GitHub provides a concise PR interface that shows what\'s been added or removed. You can review your own PR, use Copilot, or have teammates review. They can comment on your PR to suggest improvements or identify issues.\\n\\n**Best practices for creating a Pull Request:**\\n+ **Keep PRs Small**: Focus on a single feature or fix. Small PRs are easier to review and merge\\n+ **Clear Description**: Write a concise summary of what the PR does, why it\'s needed, and any relevant context\\n+ **Reference Issues**: Link related issues or tickets (e.g., Closes #123)\\n+ **Self-Review**: Check your code for errors, style, and unnecessary changes before submitting\\n+ **Add Tests**: Include or update tests to cover your changes\\n+ **Follow Conventions**: Stick to your team\'s coding and commit message guidelines\\n\\nTo create a PR, select the \\"Pull requests\\" tab, click \\"New pull request\\", then select your current branch and target branch. We\'ll continue with our current branch:\\n\\n![new_pr](./imgs_16_01/new_pr.png)\\n\\nThe PR is now created:\\n\\n![created_pr](./imgs_16_01/created_pr.png)\\n\\n## Review\\n\\n**Best practices for reviewing a Pull Request:**\\n+ **Understand the Context**: Read the PR description and related issues\\n+ **Check for Clarity**: Ensure code is readable, well-structured, and commented where necessary\\n+ **Test Locally**: Run the code if possible to verify it works as intended\\n+ **Suggest Improvements**: Be constructive and specific in your feedback\\n+ **Check for Side Effects**: Make sure changes don\'t break existing functionality\\n+ **Be Respectful**: Keep feedback positive and focused on the code, not the person\\n\\nClick on \\"File changes\\" to start your review. You can comment on specific lines of changed files. In the end, comment on whether changes should be modified or merged. Here I\'m the PR creator, so GitHub disables self-approval. In best practice, another person should review your work. `LGTM` (Looks Good To Me) indicates approval and allows merging your changes into the target branch.\\n\\n:::tip\\n+ Request PR review from teammates\\n+ PR must be approved before merging\\n:::\\n\\n![review](./imgs_16_01/review.png)\\n\\n## Merge\\n\\nAlthough your PR is ready to merge without approval in this example, you should configure branch protection rules in your repository settings:\\nhttps://docs.github.com/en/repositories/configuring-branches-and-merges-in-your-repository/configuring-pull-request-merges\\n\\nWhen merging a pull request on GitHub, you can choose from several strategies. Each has advantages depending on your workflow:\\n\\n| Strategy             | Description                                                                                   | Use Case                                   |\\n| -------------------- | --------------------------------------------------------------------------------------------- | ------------------------------------------ |\\n| **Merge Commit**     | Combines all commits from the feature branch into the target branch, preserving full history. | Default; keeps detailed commit history.    |\\n| **Squash and Merge** | Combines all changes from the feature branch into a single commit on the target branch.       | Clean, concise history for small features. |\\n| **Rebase and Merge** | Reapplies your branch\'s commits on top of the target branch, creating a linear history.       | Linear history, avoids merge commits.      |\\n\\n![merge](./imgs_16_01/merge.png)\\n\\nWhen working with a large team, using a merge commit directly integrates your branch into the target branch. This works well if the target branch hasn\'t changed significantly since you started. For example, if you branched off `main` when it only had `feature A` and `feature B`, and you\'re adding `feature C`, merging is straightforward.\\n\\nHowever, if other team members have merged new features (e.g., `feature C1`) into `main` that modify the same files you\'re working on, you may encounter conflicts. In such cases, it\'s best to use rebase to update your branch with the latest changes from the target branch before merging. If your branch contains many small, incremental commits, consider using squash and merge to combine them into a single, clean commit for a tidier history.\\n\\n![merge_type](./imgs_16_01/merge-types.png)\\n\\nNow the main branch shows the update from your PR:\\n\\n![updated_main](./imgs_16_01/merged_to_main.png)\\n\\n## Rebase\\n\\nWhen working in a team, the `main` branch (or your target branch) may receive updates from other contributors while you\'re developing your feature branch. To keep your branch up-to-date and avoid conflicts later, regularly update your branch with the latest changes from `main`. This is where `rebase` is useful.\\n\\n**Use case:**\\nSuppose you started working on a feature branch (`feature/testing-rebase`) based on an older version of `main`. Meanwhile, new commits have been added to `main` by others. Rebasing allows you to \\"replay\\" your changes on top of the latest `main`, resulting in a linear and clean history.\\n\\n**How to update your feature branch after `main` is updated:**\\n\\n```bash\\n# Make sure you\'re on your feature branch\\ngit checkout feature/testing-rebase\\n\\n# Fetch the latest changes from the remote repository\\ngit fetch origin\\n\\n# Rebase your branch onto the updated main branch\\ngit rebase origin/main\\n```\\n\\nDuring the rebase, if there are conflicts, Git will pause and let you resolve them. After fixing conflicts, continue the rebase:\\n\\n```bash\\ngit add <resolved-files>\\ngit rebase --continue\\n```\\n\\nIf you want to abort the rebase at any point:\\n\\n```bash\\ngit rebase --abort\\n```\\n\\n**Why use rebase?**\\n- Keeps your branch history clean and linear\\n- Makes it easier to review and merge changes\\n- Reduces the chance of complex merge conflicts later\\n\\n**Tip:**\\nAfter rebasing and before pushing, you may need to force-push your branch since the commit history has changed:\\n\\n```bash\\ngit push --force-with-lease\\n```\\n\\nUse force-push with care, especially on shared branches.\\n\\n![rebase](./imgs_16_01/rebase_on_local.png)\\n\\n## New branch again\\nNow you can sync with your team, next step is creating the new branch again to have more updates (features, fix bug, documentation, testing, etc)\\n```bash\\n# check out on base branch where feature is updated\\ngit checkout main\\n\\n# pull to update, add rebase for better organization on commits\\ngit pull --rebase origin main\\n\\n# create new branch\\ngit switch -c feature/add-cicd-setup\\n```\\n\\n*This concludes Part 1 of the RiverXData series on version control in bioinformatics. Stay tuned for Part 2!*"},{"id":"how-to-build-slurm-hpc-part-3","metadata":{"permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-3","source":"@site/blog/2026-01/2026-01-14.md","title":"Building a Slurm HPC Cluster (Part 3) - Administration and Best Practices","description":"In Part 1 and Part 2, we built a complete Slurm HPC cluster from a single node to a production-ready multi-node system. Now let\'s learn how to manage, maintain, and secure it effectively.","date":"2026-01-14T00:00:00.000Z","tags":[{"inline":true,"label":"slurm","permalink":"/river-docs/blog/tags/slurm"},{"inline":true,"label":"hpc","permalink":"/river-docs/blog/tags/hpc"},{"inline":true,"label":"administration","permalink":"/river-docs/blog/tags/administration"},{"inline":true,"label":"security","permalink":"/river-docs/blog/tags/security"},{"inline":true,"label":"best-practices","permalink":"/river-docs/blog/tags/best-practices"}],"readingTime":12.14,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"how-to-build-slurm-hpc-part-3","title":"Building a Slurm HPC Cluster (Part 3) - Administration and Best Practices","authors":["river"],"tags":["slurm","hpc","administration","security","best-practices"],"image":"./imgs/hpc_3.svg"},"unlisted":false,"prevItem":{"title":"The Evolution of Version Control - Git\'s Role in Reproducible Bioinformatics (Part 1)","permalink":"/river-docs/blog/how-to-version-control-git-bioinformatics-part-1"},"nextItem":{"title":"Building a Slurm HPC Cluster (Part 2) - Scaling to Production with Ansible","permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-2"}},"content":"In [Part 1](/blog/how-to-build-slurm-hpc-part-1) and [Part 2](/blog/how-to-build-slurm-hpc-part-2), we built a complete Slurm HPC cluster from a single node to a production-ready multi-node system. Now let\'s learn how to manage, maintain, and secure it effectively.\\n\\nThis final post covers daily administration tasks, troubleshooting, security hardening, and integration with data processing frameworks.\\n\\n\x3c!--truncate--\x3e\\n\\n## Series Overview\\n\\n- **[Part 1](/blog/how-to-build-slurm-hpc-part-1)**: Introduction, Architecture, and Single Node Setup\\n- **[Part 2](/blog/how-to-build-slurm-hpc-part-2)**: Scaling to Production with Ansible\\n- **Part 3 (This Post)**: Administration and Best Practices\\n\\n## Administration Overview\\n\\nManaging a Slurm cluster involves several key areas:\\n\\n- **Cluster Management**: Build, maintain, and update the cluster via Ansible\\n- **User Management**: Synchronize users across nodes with proper permissions\\n- **Login Security**: Implement SSH hardening with 2FA or key pairs\\n- **Resource Management**: Enforce limits and fair-share policies\\n- **Monitoring**: Track performance and resource utilization\\n- **Troubleshooting**: Diagnose and resolve issues\\n\\n## User and Resource Management\\n\\n### Adding Users and Groups\\n\\nSlurm uses accounts (groups) to organize users and apply resource policies:\\n\\n```bash\\n# Add a new account/group\\nsacctmgr add account research_team Description=\\"Research Team\\"\\n\\n# Add a user to an account\\nsacctmgr add user john account=research_team\\n\\n# Add user with multiple accounts\\nsacctmgr add user alice account=research_team,dev_team DefaultAccount=research_team\\n\\n# View accounts\\nsacctmgr show account\\n\\n# View users\\nsacctmgr show user\\n```\\n\\n### Setting Resource Limits\\n\\n#### Account-Level Limits\\n\\nControl resources for entire groups:\\n\\n```bash\\n# Limit CPU minutes (prevents monopolizing cluster)\\nsacctmgr modify account research_team set GrpCPUMins=100000\\n\\n# Limit memory (in MB)\\nsacctmgr modify account research_team set GrpMem=500000\\n\\n# Limit concurrent jobs\\nsacctmgr modify account research_team set GrpJobs=50\\n\\n# Limit concurrent running jobs\\nsacctmgr modify account research_team set GrpJobsRun=20\\n\\n# Limit number of nodes\\nsacctmgr modify account research_team set GrpNodes=10\\n```\\n\\n#### User-Level Limits\\n\\nControl individual user behavior:\\n\\n```bash\\n# Limit jobs in queue\\nsacctmgr modify user john set MaxJobs=10\\n\\n# Limit running jobs\\nsacctmgr modify user john set MaxJobsRun=5\\n\\n# Limit wall time (in minutes)\\nsacctmgr modify user john set MaxWall=1440  # 24 hours\\n\\n# Limit CPUs per job\\nsacctmgr modify user john set MaxCPUs=32\\n\\n# View user limits\\nsacctmgr show user john withassoc format=user,account,maxjobs,maxsubmit,maxwall\\n```\\n\\n### Quality of Service (QoS)\\n\\nQoS allows you to create service tiers with different priorities:\\n\\n```bash\\n# Create QoS levels\\nsacctmgr add qos normal priority=100\\nsacctmgr add qos high priority=500 MaxWall=2-00:00:00 MaxJobs=5\\nsacctmgr add qos low priority=50\\n\\n# Assign QoS to account\\nsacctmgr modify account research_team set qos=normal,high\\n\\n# Users can specify QoS when submitting\\nsbatch --qos=high job_script.sh\\n```\\n\\n### Fair-Share Scheduling\\n\\nEnsure equitable resource distribution:\\n\\n```bash\\n# Set fair-share values (higher = more priority)\\nsacctmgr modify account research_team set fairshare=100\\nsacctmgr modify account dev_team set fairshare=50\\n\\n# View fair-share tree\\nsshare -a\\n\\n# View detailed fair-share info\\nsshare -A research_team --all\\n```\\n\\n## Node Management\\n\\n### Checking Node Status\\n\\n```bash\\n# View all nodes\\nsinfo\\n\\n# Detailed node information\\nsinfo -Nel\\n\\n# Show node states\\nsinfo -N -o \\"%N %T %C %m %e %f\\"\\n\\n# View specific node details\\nscontrol show node worker-01\\n```\\n\\n<figure markdown=\\"span\\">\\n    ![Job sinfo](./imgs/job_sinfo.png)\\n</figure>\\n\\nNode states you\'ll encounter:\\n- **IDLE**: Available for jobs\\n- **ALLOCATED**: Running jobs\\n- **MIXED**: Partially allocated\\n- **DRAIN**: Won\'t accept new jobs (draining)\\n- **DRAINED**: Fully drained\\n- **DOWN**: Not responding\\n\\n### Node Maintenance\\n\\n#### Draining a Node\\n\\nWhen you need to perform maintenance:\\n\\n```bash\\n# Drain node (won\'t accept new jobs, allows running jobs to finish)\\nscontrol update NodeName=worker-01 State=drain Reason=\\"Hardware upgrade\\"\\n\\n# Force drain (terminate running jobs)\\nscontrol update NodeName=worker-01 State=drain Reason=\\"Emergency maintenance\\"\\n\\n# Check drain reason\\nsinfo -R\\n```\\n\\n#### Resuming a Node\\n\\nAfter maintenance:\\n\\n```bash\\n# Resume node\\nscontrol update NodeName=worker-01 State=resume\\n\\n# Verify it\'s back\\nsinfo -n worker-01\\n```\\n\\n#### Forcing Node Down\\n\\nIf a node is misbehaving:\\n\\n```bash\\n# Mark node as down\\nscontrol update NodeName=worker-01 State=down Reason=\\"Hardware failure\\"\\n\\n# When fixed, resume\\nscontrol update NodeName=worker-01 State=resume\\n```\\n\\n### Adding New Compute Nodes\\n\\n1. Update Ansible inventory (`inventories/hosts`):\\n\\n```ini\\n[slurm_worker]\\nworker-01 ansible_host=192.168.58.11\\nworker-02 ansible_host=192.168.58.12\\nworker-03 ansible_host=192.168.58.13  # NEW\\n```\\n\\n2. Run Ansible playbook:\\n\\n```bash\\nansible-playbook -i inventories/hosts river_cluster.yml --limit worker-03\\n```\\n\\n3. Update `slurm.conf` on controller and all nodes (Ansible handles this)\\n\\n4. Restart slurmctld:\\n\\n```bash\\nsudo systemctl restart slurmctld\\n```\\n\\n5. Verify the new node:\\n\\n```bash\\nsinfo\\nscontrol show node worker-03\\n```\\n\\n## Monitoring and Troubleshooting\\n\\n### Checking Slurm Logs\\n\\nLogs are essential for diagnosing issues:\\n\\n```bash\\n# Controller logs\\nsudo tail -f /var/log/slurm/slurmctld.log\\n\\n# Worker node logs (on compute nodes)\\nsudo tail -f /var/log/slurm/slurmd.log\\n\\n# Database logs\\nsudo tail -f /var/log/slurm/slurmdbd.log\\n\\n# Filter for errors\\nsudo grep \\"error\\" /var/log/slurm/*.log\\n\\n# Filter for specific node\\nsudo grep \\"worker-01\\" /var/log/slurm/slurmctld.log\\n\\n# Last 100 lines with context\\nsudo tail -100 /var/log/slurm/slurmctld.log\\n```\\n\\n### Common Issues and Solutions\\n\\n#### Issue: Node Shows as DOWN\\n\\n**Diagnosis:**\\n```bash\\nsinfo\\n# OUTPUT: worker-01    down   ...\\n\\nscontrol show node worker-01\\n# Check \\"Reason\\" field\\n```\\n\\n**Solutions:**\\n```bash\\n# 1. Check if slurmd is running\\nssh worker-01 \\"sudo systemctl status slurmd\\"\\n\\n# 2. Restart slurmd\\nssh worker-01 \\"sudo systemctl restart slurmd\\"\\n\\n# 3. Check network connectivity\\nping worker-01\\n\\n# 4. Check logs\\nssh worker-01 \\"sudo tail -50 /var/log/slurm/slurmd.log\\"\\n\\n# 5. Resume the node\\nscontrol update NodeName=worker-01 State=resume\\n```\\n\\n#### Issue: Jobs Stuck in Pending\\n\\n**Diagnosis:**\\n```bash\\nsqueue\\n# See jobs in PD (pending) state\\n\\n# Check why job is pending\\nsqueue --start -j JOB_ID\\n\\n# View detailed job info\\nscontrol show job JOB_ID\\n```\\n\\n**Common reasons:**\\n- `Resources`: Not enough resources available\\n- `Priority`: Lower priority than other jobs\\n- `Dependency`: Waiting for another job to complete\\n- `QOSMaxJobsPerUser`: User has too many jobs running\\n\\n**Solutions:**\\n```bash\\n# 1. Check available resources\\nsinfo -o \\"%P %a %l %D %N %C\\"\\n\\n# 2. View job requirements\\nscontrol show job JOB_ID | grep -E \\"Partition|NumNodes|MinMemory\\"\\n\\n# 3. Cancel job if needed\\nscancel JOB_ID\\n\\n# 4. Modify pending job\\nscontrol update JobId=JOB_ID NumNodes=1\\n```\\n\\n#### Issue: Jobs Failing Immediately\\n\\n**Diagnosis:**\\n```bash\\n# Check job status\\nsacct -j JOB_ID\\n\\n# View job output files\\ncat slurm-JOB_ID.out\\ncat slurm-JOB_ID.err\\n```\\n\\n**Common causes:**\\n- Script errors (check shebang line)\\n- Missing executables\\n- Resource limits exceeded\\n- Permission issues\\n\\n#### Issue: Accounting Database Not Working\\n\\n**Diagnosis:**\\n```bash\\n# Check slurmdbd status\\nsudo systemctl status slurmdbd\\n\\n# Test database connection\\nsudo mysql -u slurm -p slurm_acct_db -e \\"SHOW TABLES;\\"\\n\\n# Check slurmdbd logs\\nsudo tail -50 /var/log/slurm/slurmdbd.log\\n```\\n\\n**Solutions:**\\n```bash\\n# 1. Restart slurmdbd\\nsudo systemctl restart slurmdbd\\n\\n# 2. Verify database credentials in slurmdbd.conf\\nsudo cat /etc/slurm-llnl/slurmdbd.conf\\n\\n# 3. Check database permissions\\nsudo mysql -e \\"SHOW GRANTS FOR \'slurm\'@\'localhost\';\\"\\n\\n# 4. Restart slurmctld to reconnect\\nsudo systemctl restart slurmctld\\n```\\n\\n### System Logs with rsyslog\\n\\nOur Ansible setup configures centralized logging:\\n\\n```bash\\n# On controller (rsyslog server)\\nsudo tail -f /var/log/syslog\\n\\n# Filter by hostname\\nsudo grep \\"worker-01\\" /var/log/syslog\\n\\n# Filter by service\\nsudo grep \\"slurmd\\" /var/log/syslog\\n\\n# Check authentication logs\\nsudo tail -f /var/log/auth.log\\n```\\n\\n## Security Best Practices\\n\\n### SSH Hardening\\n\\n:::warning\\nSecure your login nodes! HPC clusters are attractive targets for attackers.\\n:::\\n\\nFor detailed SSH security setup, see our [SSH Remote Server documentation](/docs/resources/administration/ssh-remote-server).\\n\\nKey recommendations:\\n\\n1. **Disable Password Authentication**:\\n```bash\\n# /etc/ssh/sshd_config\\nPasswordAuthentication no\\nPubkeyAuthentication yes\\n```\\n\\n2. **Implement 2FA** with Google Authenticator or similar\\n\\n3. **Use SSH Key Pairs**:\\n```bash\\n# Generate key on your machine\\nssh-keygen -t ed25519 -C \\"your_email@example.com\\"\\n\\n# Copy to cluster\\nssh-copy-id user@controller-node\\n```\\n\\n4. **Limit SSH Access**:\\n```bash\\n# /etc/ssh/sshd_config\\nAllowUsers alice bob charlie\\nAllowGroups cluster_users\\n\\n# Or deny specific users\\nDenyUsers baduser\\n```\\n\\n5. **Change Default Port** (security through obscurity):\\n```bash\\n# /etc/ssh/sshd_config\\nPort 2222\\n```\\n\\n### Munge Authentication\\n\\nMunge provides authentication between Slurm components:\\n\\n```bash\\n# Verify munge is running\\nsudo systemctl status munge\\n\\n# Test munge\\nmunge -n | unmunge\\n\\n# Generate new key (do this on controller, then distribute)\\nsudo /usr/sbin/create-munge-key\\n\\n# Copy key to all nodes (Ansible does this automatically)\\nsudo scp /etc/munge/munge.key worker-01:/etc/munge/\\n\\n# Restart munge on all nodes\\nsudo systemctl restart munge\\n```\\n\\n:::warning\\n**Critical**: The munge key must be identical on all nodes and have proper permissions (0400, owned by munge:munge).\\n:::\\n\\n### Docker Security\\n\\n:::danger\\n**Critical Security Issue**: Users in the `docker` group can gain root privileges!\\n\\n```bash\\n# DON\'T DO THIS (unless they\'re admins)\\nsudo usermod -aG docker regular_user\\n```\\n\\nWhy? Because they can run:\\n```bash\\ndocker run -v /:/hostfs --privileged -it ubuntu bash\\n# Now they have root access to the host filesystem!\\n```\\n:::\\n\\n**Solutions**:\\n\\n1. **Use Docker Rootless Mode**:\\n```bash\\n# Install rootless docker\\ncurl -fsSL https://get.docker.com/rootless | sh\\n```\\n\\n2. **Use Apptainer/Singularity** (designed for HPC):\\n```bash\\n# Install Apptainer\\nsudo apt-get install apptainer\\n\\n# Run containers without root\\napptainer run docker://ubuntu:latest\\n```\\n\\n3. **Restrict Docker Group**: Only add administrators to docker group\\n\\n### Firewall Configuration\\n\\nRestrict access to Slurm ports:\\n\\n```bash\\n# Allow Slurm ports only from cluster network\\nsudo ufw allow from 192.168.58.0/24 to any port 6817  # slurmctld\\nsudo ufw allow from 192.168.58.0/24 to any port 6818  # slurmd\\nsudo ufw allow from 192.168.58.0/24 to any port 6819  # slurmdbd\\n\\n# Allow SSH from anywhere\\nsudo ufw allow 22/tcp\\n\\n# Enable firewall\\nsudo ufw enable\\n```\\n\\n## Shared Storage Best Practices\\n\\n### NFS Performance Tuning\\n\\nOptimize NFS for your workload:\\n\\n```bash\\n# /etc/fstab on compute nodes\\ncontroller-01:/home /home nfs4 rw,soft,rsize=262144,wsize=262144,timeo=14,intr 0 0\\n```\\n\\nParameters explained:\\n- `soft`: Timeout after retry (vs `hard` which waits forever)\\n- `rsize/wsize`: Read/write buffer size (larger = better performance)\\n- `timeo`: Timeout value\\n- `intr`: Allow interrupts\\n\\n### Storage Layout\\n\\nRecommended directory structure:\\n\\n```\\n/home/          # User home directories (SSD/NVMe)\\n  \u251c\u2500 alice/\\n  \u251c\u2500 bob/\\n  \u2514\u2500 charlie/\\n\\n/mnt/data/      # Large datasets (HDD or object storage)\\n  \u251c\u2500 shared/    # Common datasets\\n  \u251c\u2500 projects/  # Project-specific data\\n  \u2514\u2500 scratch/   # Temporary data (auto-cleanup)\\n\\n/opt/           # Shared software/modules\\n  \u251c\u2500 anaconda/\\n  \u251c\u2500 modules/\\n  \u2514\u2500 apps/\\n```\\n\\n### Quotas\\n\\nPrevent users from filling up shared storage:\\n\\n```bash\\n# Set user quotas\\nsudo setquota -u alice 50G 60G 0 0 /home\\nsudo setquota -u alice 500G 550G 0 0 /mnt/data\\n\\n# Check quotas\\nquota -u alice\\n\\n# View all quotas\\nsudo repquota -a\\n```\\n\\n## Integration with Data Processing Frameworks\\n\\nOne of Slurm\'s greatest strengths is integration with modern computing frameworks:\\n\\n### Apache Spark\\n\\nSubmit Spark jobs to Slurm:\\n\\n```bash\\n#!/bin/bash\\n#SBATCH --job-name=spark-job\\n#SBATCH --nodes=4\\n#SBATCH --ntasks-per-node=1\\n#SBATCH --cpus-per-task=8\\n#SBATCH --mem=32G\\n\\n# Load Spark module\\nmodule load spark/3.5.0\\n\\n# Run Spark application\\nspark-submit \\\\\\n  --master yarn \\\\\\n  --num-executors 4 \\\\\\n  --executor-cores 8 \\\\\\n  --executor-memory 28G \\\\\\n  my_spark_app.py\\n```\\n\\n### Ray (Distributed ML)\\n\\n```bash\\n#!/bin/bash\\n#SBATCH --job-name=ray-job\\n#SBATCH --nodes=2\\n#SBATCH --ntasks-per-node=1\\n#SBATCH --cpus-per-task=16\\n#SBATCH --gpus-per-node=2\\n\\n# Start Ray cluster\\nray start --head --port=6379\\nsrun --nodes=1 --ntasks=1 ray start --address=$HEAD_NODE:6379\\n\\n# Run Ray application\\npython ray_train.py\\n```\\n\\n### Dask\\n\\n```python\\nfrom dask_jobqueue import SLURMCluster\\nfrom dask.distributed import Client\\n\\ncluster = SLURMCluster(\\n    cores=8,\\n    memory=\\"16GB\\",\\n    processes=2,\\n    walltime=\\"02:00:00\\",\\n    queue=\\"compute\\"\\n)\\n\\ncluster.scale(jobs=10)  # Request 10 jobs\\nclient = Client(cluster)\\n\\n# Your Dask code here\\n```\\n\\n### Nextflow (Bioinformatics)\\n\\n```groovy\\n// nextflow.config\\nprocess {\\n    executor = \'slurm\'\\n    queue = \'compute\'\\n    memory = \'8 GB\'\\n    time = \'2h\'\\n}\\n```\\n\\nRun with:\\n```bash\\nnextflow run nf-core/rnaseq -profile slurm\\n```\\n\\n## Maintenance Tasks\\n\\n### Regular Updates\\n\\n```bash\\n# Update cluster via Ansible\\nansible-playbook -i inventories/hosts river_cluster.yml --tags update\\n\\n# Update specific nodes\\nansible-playbook -i inventories/hosts river_cluster.yml --limit worker-01,worker-02\\n```\\n\\n### Backup Critical Data\\n\\n```bash\\n# Backup Slurm configuration\\nsudo cp /etc/slurm-llnl/slurm.conf /backup/slurm.conf.$(date +%Y%m%d)\\n\\n# Backup accounting database\\nsudo mysqldump -u slurm -p slurm_acct_db > slurm_acct_backup_$(date +%Y%m%d).sql\\n\\n# Backup user data (use rsync for efficiency)\\nsudo rsync -av /home/ /backup/home/\\n```\\n\\n### Monitoring Disk Space\\n\\n```bash\\n# Check disk usage on all nodes\\nansible all -i inventories/hosts -m shell -a \\"df -h\\"\\n\\n# Check specific directory\\nansible all -i inventories/hosts -m shell -a \\"du -sh /var/log/slurm\\"\\n\\n# Find large files\\nfind /home -type f -size +1G -exec ls -lh {} \\\\;\\n```\\n\\n## Performance Optimization Tips\\n\\n### 1. Tune Scheduler Parameters\\n\\n```bash\\n# /etc/slurm-llnl/slurm.conf\\n\\n# Increase scheduling frequency\\nSchedulerTimeSlice=30\\n\\n# Prioritize recent submitters less\\nPriorityWeightAge=1000\\nPriorityWeightFairshare=10000\\n\\n# Enable backfill scheduling with larger window\\nSchedulerType=sched/backfill\\nbf_window=1440  # 24 hours\\n```\\n\\n### 2. Optimize Job Packing\\n\\n```bash\\n# Use CR_CPU for CPU-bound jobs\\nSelectType=select/cons_tres\\nSelectTypeParameters=CR_CPU\\n\\n# Or CR_Memory for memory-bound jobs\\nSelectTypeParameters=CR_Memory\\n\\n# Or CR_Core for mixed workloads\\nSelectTypeParameters=CR_Core\\n```\\n\\n### 3. Create Multiple Partitions\\n\\n```bash\\n# /etc/slurm-llnl/slurm.conf\\n\\n# Fast partition for short jobs\\nPartitionName=quick Nodes=worker-[01-02] Default=NO MaxTime=01:00:00 State=UP Priority=100\\n\\n# Standard partition\\nPartitionName=standard Nodes=worker-[01-04] Default=YES MaxTime=2-00:00:00 State=UP Priority=50\\n\\n# Long partition for extended jobs\\nPartitionName=long Nodes=worker-[03-04] Default=NO MaxTime=7-00:00:00 State=UP Priority=25\\n\\n# GPU partition\\nPartitionName=gpu Nodes=gpu-[01-02] Default=NO MaxTime=1-00:00:00 State=UP Priority=75\\n```\\n\\n### 4. Enable Job Arrays for Batch Processing\\n\\n```bash\\n#!/bin/bash\\n#SBATCH --array=1-100%10  # 100 tasks, max 10 concurrent\\n\\n# Process task based on array index\\npython process.py --input data_${SLURM_ARRAY_TASK_ID}.txt\\n```\\n\\n## Conclusion\\n\\nCongratulations! You now have the knowledge to build, deploy, and manage a production Slurm HPC cluster. Let\'s recap the journey:\\n\\n### Part 1: Foundations\\n- Understanding Slurm architecture\\n- Single-node setup for learning\\n- Critical cgroup configuration\\n- Job accounting basics\\n\\n### Part 2: Production Deployment\\n- Ansible automation\\n- Multi-node cluster setup\\n- Monitoring with Grafana\\n- Slack alerting\\n\\n### Part 3: Administration (This Post)\\n- User and resource management\\n- Node maintenance and troubleshooting\\n- Security hardening\\n- Performance optimization\\n- Framework integration\\n\\n## Key Takeaways\\n\\n1. **Start Simple, Scale Smart**: Master single-node before going multi-node\\n2. **Automate Everything**: Use Ansible for reproducible deployments\\n3. **Monitor Proactively**: Set up alerting before problems occur\\n4. **Security First**: SSH hardening, proper permissions, Docker caution\\n5. **Regular Maintenance**: Backups, updates, and log monitoring\\n6. **Documentation**: Document your cluster configuration and procedures\\n\\n## What\'s Next?\\n\\nConsider these advanced topics:\\n\\n- **High Availability**: Redundant controllers with failover\\n- **LDAP Integration**: Centralized authentication for large organizations\\n- **GPU Scheduling**: Optimize for machine learning workloads\\n- **Cloud Bursting**: Expand to cloud resources during peak demand\\n- **Custom Plugins**: Extend Slurm with custom scheduling policies\\n\\n## Resources\\n\\n- **Part 1**: [Single Node Setup](/blog/how-to-build-slurm-hpc-part-1)\\n- **Part 2**: [Production Deployment](/blog/how-to-build-slurm-hpc-part-2)\\n- **Documentation**: [Administration Guide](/docs/resources/high-performance-computing/how-to-build-slurm-scalable-using-ansible/administration)\\n- **GitHub**: [RiverXData Slurm Ansible](https://github.com/riverxdata/river-slurm)\\n- **Official Docs**: [SchedMD Slurm Documentation](https://slurm.schedmd.com/)\\n\\n## Contact\\n\\nHave questions or need help with your cluster? Reach out at: nttg8100@gmail.com\\n\\n---\\n\\n*This concludes the RiverXData series on building Slurm HPC clusters. Thank you for following along! We hope this guide helps you build and manage effective HPC infrastructure.*"},{"id":"how-to-build-slurm-hpc-part-2","metadata":{"permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-2","source":"@site/blog/2026-01/2026-01-12.md","title":"Building a Slurm HPC Cluster (Part 2) - Scaling to Production with Ansible","description":"In Part 1, we learned the fundamentals by building a single-node Slurm cluster. Now it\'s time to scale up to a production-ready, multi-node cluster with automated deployment, monitoring, and alerting.","date":"2026-01-12T00:00:00.000Z","tags":[{"inline":true,"label":"slurm","permalink":"/river-docs/blog/tags/slurm"},{"inline":true,"label":"hpc","permalink":"/river-docs/blog/tags/hpc"},{"inline":true,"label":"ansible","permalink":"/river-docs/blog/tags/ansible"},{"inline":true,"label":"automation","permalink":"/river-docs/blog/tags/automation"},{"inline":true,"label":"devops","permalink":"/river-docs/blog/tags/devops"}],"readingTime":8.87,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"how-to-build-slurm-hpc-part-2","title":"Building a Slurm HPC Cluster (Part 2) - Scaling to Production with Ansible","authors":["river"],"tags":["slurm","hpc","ansible","automation","devops"],"image":"./imgs/hpc_2.svg"},"unlisted":false,"prevItem":{"title":"Building a Slurm HPC Cluster (Part 3) - Administration and Best Practices","permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-3"},"nextItem":{"title":"Building a Slurm HPC Cluster (Part 1) - Single Node Setup and Fundamentals","permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-1"}},"content":"In [Part 1](/blog/how-to-build-slurm-hpc-part-1), we learned the fundamentals by building a single-node Slurm cluster. Now it\'s time to scale up to a production-ready, multi-node cluster with automated deployment, monitoring, and alerting.\\n\\nIn this post, we\'ll use Ansible to automate the entire deployment process, making it reproducible and maintainable.\\n\\n\x3c!--truncate--\x3e\\n\\n## Series Overview\\n\\n- **[Part 1](/blog/how-to-build-slurm-hpc-part-1)**: Introduction, Architecture, and Single Node Setup\\n- **Part 2 (This Post)**: Scaling to Production with Ansible\\n- **[Part 3](/blog/how-to-build-slurm-hpc-part-3)**: Administration and Best Practices\\n\\n## Why Ansible for HPC Clusters?\\n\\nMoving from a single-node setup to a multi-node production cluster involves:\\n\\n- Configuring multiple machines identically\\n- Managing dependencies and installation order\\n- Keeping configurations synchronized\\n- Handling user management across nodes\\n- Setting up monitoring and logging infrastructure\\n\\nDoing this manually is error-prone and time-consuming. Infrastructure automation tools like Ansible, Puppet, or Terraform solve this problem. We chose **Ansible** because:\\n\\n- **Agentless**: No software to install on managed nodes\\n- **Declarative**: Describe the desired state, not the steps\\n- **Idempotent**: Safe to run multiple times\\n- **YAML-based**: Easy to read and version control\\n- **Large ecosystem**: Many pre-built roles available\\n\\n### What is Ansible? (Quick Primer)\\n\\nIf you\'re new to Ansible, watch this excellent 100-second introduction:\\n\\n<div style={{ position: \\"relative\\", paddingBottom: \\"56.25%\\", height: 0, overflow: \\"hidden\\", maxWidth: \\"100%\\", background: \\"#000\\" }}>\\n  <iframe \\n    src=\\"https://www.youtube.com/embed/xRMPKQweySE\\" \\n    frameBorder=\\"0\\" \\n    allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" \\n    allowFullScreen\\n    style={{ position: \\"absolute\\", top: 0, left: 0, width: \\"100%\\", height: \\"100%\\" }}\\n  />\\n</div>\\n\\n## Production Cluster Architecture\\n\\nOur production setup includes these components:\\n\\n<figure markdown=\\"span\\">\\n    ![Small HPC Cluster](./imgs/small_HPC.jpg)\\n</figure>\\nSource: https://cs.phenikaa-uni.edu.vn/vi/post/gioi-thieu/co-so-vat-chat/he-thong-tinh-toan-hieu-nang-cao-phenikaa-hpc\\n\\n### Head Node (Controller + Login)\\n\\nThe head node manages the cluster and provides user access:\\n\\n- **slurmctld**: Job scheduling and resource management\\n- **slurmdbd**: Accounting database\\n- **NFS Server**: Shares directories to compute nodes\\n- **Prometheus**: Metrics collection\\n- **Grafana**: Monitoring dashboards\\n- **Alertmanager**: Slack notifications\\n\\n### Compute Nodes\\n\\nWorker nodes that execute jobs:\\n\\n- **slurmd**: Job execution daemon\\n- **NFS Client**: Mounts shared storage\\n- **Node Exporter**: Exposes system metrics\\n- **Slurm Exporter**: Exposes Slurm-specific metrics\\n\\n### Shared Storage (NFS)\\n\\nNFS provides unified file system access across all nodes:\\n\\n<figure markdown=\\"span\\">\\n    ![NFS Architecture](./imgs/NFS.png)\\n</figure>\\nSource: https://thuanbui.me/cai-dat-nfs-server-va-nfs-client-tren-ubuntu-22-04/\\n\\n- `/home`: User home directories (fast SSD/NVMe storage)\\n- `/mnt/data`: Large datasets (high-capacity HDD)\\n\\n### Monitoring Stack\\n\\nComplete observability for your cluster:\\n\\n<figure markdown=\\"span\\">\\n    ![Grafana Dashboard](./imgs/grafana.png)\\n</figure>\\nSource: https://swsmith.cc/posts/grafana-slurm.html\\n\\n- **Prometheus**: Time-series metrics database\\n- **Grafana**: Beautiful dashboards for visualization\\n- **Alertmanager**: Sends alerts to Slack when issues occur\\n- **Node Exporter**: System-level metrics (CPU, memory, disk)\\n- **Slurm Exporter**: Slurm-specific metrics (jobs, partitions, nodes)\\n\\n## Setting Up the RiverXData Slurm Cluster\\n\\nWe\'ve created a comprehensive Ansible playbook that automates everything. Let\'s get started!\\n\\n<figure markdown=\\"span\\">\\n    ![Slurm Deployment Architecture](./imgs/slurm.svg)\\n</figure>\\n\\n### Prerequisites\\n\\n- Multiple Ubuntu 20.04 or 24.04 machines (or VMs)\\n- SSH access to all nodes\\n- Sudo privileges on all nodes\\n- A Slack workspace (for alerts)\\n\\n### Step 1: Clone the Repository\\n\\n```bash\\ngit clone https://github.com/riverxdata/river-slurm.git -b 1.0.0\\ncd river-slurm\\n```\\n\\n### Step 2: Install Ansible and Dependencies\\n\\n```bash\\n# Ubuntu 24.04, without Vagrant\\nbash scripts/setup.sh 24.04 false\\n\\n# For Ubuntu 20.04\\nbash scripts/setup.sh 20.04 false\\n\\n# For developers: Install with Vagrant support\\nbash scripts/setup.sh 24.04 true\\n```\\n\\nThis script installs:\\n- Ansible and required Python packages\\n- Community Ansible collections\\n- Galaxy roles (geerlingguy.docker, etc.)\\n\\n### Step 3: Set Up Slack Alerts\\n\\n:::info\\nIn production environments, even small teams benefit from proactive monitoring. Slack is perfect for this - you\'ll get notifications when nodes go down, jobs fail, or resources run low.\\n:::\\n\\n#### Create a Slack App\\n\\n1. Go to [Slack API](https://api.slack.com/apps) and create a new app\\n2. Choose \\"From scratch\\"\\n3. Name it (e.g., \\"Slurm Cluster Monitor\\")\\n4. Select your workspace\\n\\n<figure markdown=\\"span\\">\\n    ![Create Slack App](./imgs/create_app.png)\\n</figure>\\n\\n#### Enable Incoming Webhooks\\n\\n1. Navigate to \\"Incoming Webhooks\\" in your app settings\\n2. Activate incoming webhooks\\n3. Click \\"Add New Webhook to Workspace\\"\\n4. Select the channel for notifications (e.g., `#cluster-alerts`)\\n5. Copy the webhook URL\\n\\n<figure markdown=\\"span\\">\\n    ![Slack Config Step 1](./imgs/config_1.png)\\n</figure>\\n\\n<figure markdown=\\"span\\">\\n    ![Slack Config Step 2](./imgs/config_2.png)\\n</figure>\\n\\n<figure markdown=\\"span\\">\\n    ![Slack Config Step 3](./imgs/config_3.png)\\n</figure>\\n\\n#### Test Your Webhook\\n\\n```bash\\ncurl -X POST -H \'Content-type: application/json\' \\\\\\n  --data \'{\\"text\\":\\"Hello from Slurm cluster!\\"}\' \\\\\\n  https://hooks.slack.com/services/YOUR/WEBHOOK/URL\\n```\\n\\nYou should see the message appear in your Slack channel!\\n\\n### Step 4: Configure Your Inventory\\n\\nCreate `inventories/hosts` (or copy from `inventories/hosts.example`):\\n\\n```ini\\n[slurm_master]\\ncontroller-01 ansible_host=192.168.58.10\\n\\n[slurm_worker]\\nworker-01 ansible_host=192.168.58.11\\nworker-02 ansible_host=192.168.58.12\\n\\n[slurm:children]\\nslurm_master\\nslurm_worker\\n\\n[all:vars]\\nansible_user=your_username\\nslurm_password=secure_munge_password\\nslurm_account_db_pass=secure_db_password\\nslack_api_url=https://hooks.slack.com/services/YOUR/WEBHOOK/URL\\nslack_channel=#cluster-alerts\\nadmin_user=admin\\nadmin_password=secure_grafana_password\\n```\\n\\n:::warning\\n**Security Best Practice**: Use [Ansible Vault](https://docs.ansible.com/ansible/latest/user_guide/vault.html) to encrypt sensitive variables like passwords and API keys.\\n\\n```bash\\n# Create encrypted vault\\nansible-vault create inventories/vault.yml\\n\\n# Or encrypt existing file\\nansible-vault encrypt inventories/hosts\\n```\\n:::\\n\\n#### Optional Parameters\\n\\n```ini\\ndefault_password=temporary_user_password  # Forces change on first login\\nusers=alice,bob,charlie                   # Comma-separated list\\n```\\n\\n### Step 5: Deploy the Cluster\\n\\nNow for the magic moment - deploy your entire cluster with one command!\\n\\n```bash\\n# If you have passwordless sudo configured\\nansible-playbook -i inventories/hosts river_cluster.yml\\n\\n# If you need to enter sudo password\\nansible-playbook -i inventories/hosts river_cluster.yml --ask-become-pass\\n```\\n\\nWhat this playbook does:\\n\\n1. **Prepares all nodes**:\\n   - Updates packages\\n   - Installs dependencies\\n   - Configures firewalls\\n\\n2. **Sets up the controller**:\\n   - Installs slurmctld and slurmdbd\\n   - Configures MariaDB for accounting\\n   - Sets up NFS server\\n   - Installs monitoring stack\\n\\n3. **Configures compute nodes**:\\n   - Installs slurmd\\n   - Mounts NFS shares\\n   - Configures metrics exporters\\n\\n4. **Deploys monitoring**:\\n   - Prometheus for metrics collection\\n   - Grafana with pre-configured dashboards\\n   - Alertmanager with Slack integration\\n\\n5. **Synchronizes configurations**:\\n   - Copies slurm.conf to all nodes\\n   - Sets up Munge authentication\\n   - Configures log aggregation\\n\\n### Step 6: Add Users\\n\\n```bash\\nansible-playbook -i inventories/hosts river_users.yml\\n```\\n\\nThis creates Linux users on all nodes with:\\n- Synchronized UID/GID across nodes\\n- Home directories on shared NFS\\n- Slurm accounting associations\\n\\n:::info\\n**Note on User Management**: For production, consider integrating with LDAP or Active Directory. However, NIS and LDAP setup can be complex on Ubuntu. Our Ansible approach provides a simpler alternative that works well for small to medium clusters.\\n:::\\n\\n### Step 7: Verify the Setup\\n\\nSSH into the controller node and run:\\n\\n```bash\\n# Check cluster status\\nsinfo\\n\\n# Expected output:\\n# PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\\n# compute*     up   infinite      2   idle worker-01,worker-02\\n\\n# View job queue\\nsqueue\\n\\n# Submit a test job\\nsrun --nodes=1 --ntasks=1 hostname\\n\\n# Check accounting\\nsacct\\n\\n# View cluster configuration\\nscontrol show config | head -20\\n```\\n\\n### Step 8: Access Grafana Dashboards\\n\\nGrafana runs on the controller node at port 3000. To access it securely from your local machine:\\n\\n```bash\\n# Create SSH tunnel\\nssh -N -L 3001:localhost:3000 your_user@controller_ip\\n\\n# Now open in browser: http://localhost:3001\\n# Login: admin / your_grafana_password\\n```\\n\\nYou\'ll see pre-configured dashboards showing:\\n\\n#### Node Metrics Dashboard\\n- CPU usage per node\\n- Memory utilization\\n- Disk I/O\\n- Network traffic\\n- System load\\n\\n<figure markdown=\\"span\\">\\n    ![Node Grafana Dashboard](./imgs/node_grafana.png)\\n</figure>\\n\\n#### Slurm Metrics Dashboard\\n- Active jobs\\n- Job queue length\\n- Node states (idle, allocated, down)\\n- CPU allocation\\n- Memory usage\\n- Job completion rates\\n\\n<figure markdown=\\"span\\">\\n    ![Slurm Grafana Dashboard](./imgs/slurm_grafana.png)\\n</figure>\\n\\n### What About Alerts?\\n\\nAlertmanager is configured to send Slack notifications for:\\n\\n- **Node down**: When a compute node becomes unresponsive\\n- **Node resumed**: When a node comes back online\\n- **High CPU usage**: Sustained high CPU across cluster\\n- **High memory usage**: Memory pressure warnings\\n- **Disk space low**: Storage running out\\n\\nExample alert in Slack when a node goes down:\\n\\n<figure markdown=\\"span\\">\\n    ![Node Resume Alert](./imgs/resume_node.png)\\n</figure>\\n\\nFor detailed information, check the Grafana dashboard:\\n\\n<figure markdown=\\"span\\">\\n    ![Grafana Node Down](./imgs/grafana_down.png)\\n</figure>\\n\\n## Testing Your Cluster\\n\\nLet\'s run some tests to ensure everything works:\\n\\n### Test 1: Simple Job\\n\\n```bash\\nsrun hostname\\n```\\n\\n### Test 2: Multi-node Job\\n\\n```bash\\nsrun --nodes=2 --ntasks=2 hostname\\n```\\n\\n### Test 3: Interactive Session\\n\\n```bash\\nsrun --nodes=1 --cpus-per-task=2 --mem=2G --pty bash\\n\\n# Inside the session\\nhostname\\nnproc\\nfree -h\\nexit\\n```\\n\\n### Test 4: Batch Job\\n\\nCreate `test_job.sh`:\\n\\n```bash\\n#!/bin/bash\\n#SBATCH --job-name=test\\n#SBATCH --output=test_%j.out\\n#SBATCH --error=test_%j.err\\n#SBATCH --nodes=1\\n#SBATCH --ntasks=1\\n#SBATCH --cpus-per-task=2\\n#SBATCH --mem=1G\\n#SBATCH --time=00:05:00\\n\\necho \\"Job started at $(date)\\"\\necho \\"Running on node: $(hostname)\\"\\necho \\"CPUs allocated: $SLURM_CPUS_PER_TASK\\"\\necho \\"Memory allocated: $SLURM_MEM_PER_NODE MB\\"\\n\\n# Do some work\\nsleep 60\\n\\necho \\"Job finished at $(date)\\"\\n```\\n\\nSubmit it:\\n\\n```bash\\nsbatch test_job.sh\\n\\n# Check status\\nsqueue\\n\\n# When done, view output\\ncat test_*.out\\n```\\n\\n### Test 5: Resource Limits\\n\\n```bash\\n# Submit job requesting more resources than available\\nsrun --mem=999999 --pty bash\\n\\n# Should fail with:\\n# srun: error: Unable to allocate resources: Requested node configuration is not available\\n```\\n\\n### Test 6: Accounting\\n\\n```bash\\n# View your jobs\\nsacct\\n\\n# Detailed accounting info\\nsacct --format=JobID,JobName,User,State,Start,End,Elapsed,CPUTime,MaxRSS\\n\\n# Cluster usage summary\\nsreport cluster utilization\\n```\\n\\n## Architecture Diagram\\n\\nHere\'s what you\'ve built:\\n\\n```\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502           Users SSH to Controller           \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n                  \u2502\\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n    \u2502   Controller Node (Head)     \u2502\\n    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\\n    \u2502  \u2502 slurmctld              \u2502  \u2502  Job Scheduling\\n    \u2502  \u2502 slurmdbd + MariaDB     \u2502  \u2502  Accounting\\n    \u2502  \u2502 NFS Server             \u2502  \u2502  Shared Storage\\n    \u2502  \u2502 Prometheus + Grafana   \u2502  \u2502  Monitoring\\n    \u2502  \u2502 Alertmanager           \u2502  \u2502  Alerts\\n    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\\n    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n         \u2502                  \u2502\\n    \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n    \u2502 worker-01\u2502       \u2502 worker-02\u2502\\n    \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502       \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\\n    \u2502 \u2502slurmd\u2502 \u2502       \u2502 \u2502slurmd\u2502 \u2502\\n    \u2502 \u2502NFS \u2191 \u2502 \u2502       \u2502 \u2502NFS \u2191 \u2502 \u2502\\n    \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502       \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\n## For Developers: Local Testing with Vagrant\\n\\nIf you want to test the deployment locally using VMs:\\n\\n```bash\\n# Install Vagrant with libvirt provider\\nbash scripts/setup.sh 24.04 true\\n\\n# Create local VMs and deploy cluster\\nvagrant up\\n\\n# SSH to controller\\nvagrant ssh controller-01\\n\\n# Destroy VMs when done\\nvagrant destroy -f\\n```\\n\\n## Key Takeaways\\n\\nIn this post, we\'ve covered:\\n\\n1. **Why Automation**: The benefits of using Ansible for cluster management\\n2. **Production Architecture**: Multi-node setup with monitoring and alerting\\n3. **Slack Integration**: Proactive monitoring with notifications\\n4. **Automated Deployment**: One command to deploy the entire cluster\\n5. **Verification**: Testing your cluster thoroughly\\n\\n:::info\\n**What\'s Next?**\\n\\nIn [Part 3](/blog/how-to-build-slurm-hpc-part-3), we\'ll cover daily administration tasks, troubleshooting, security best practices, and advanced resource management.\\n:::\\n\\n## Resources\\n\\n- **GitHub Repository**: [RiverXData Slurm Ansible](https://github.com/riverxdata/river-slurm)\\n- **Deployment Docs**: [Scalable Slurm Deployment](/docs/resources/high-performance-computing/how-to-build-slurm-scalable-using-ansible/deployment)\\n- **Architecture Overview**: [Slurm Architecture](/docs/resources/high-performance-computing/how-to-build-slurm-scalable-using-ansible/overview)\\n- **Ansible Documentation**: [docs.ansible.com](https://docs.ansible.com/)\\n\\n## Contact\\n\\nQuestions about the deployment? Reach out at: nttg8100@gmail.com\\n\\n---\\n\\n*This is Part 2 of the RiverXData series on building Slurm HPC clusters. Continue to [Part 3](/blog/how-to-build-slurm-hpc-part-3) for administration and best practices.*"},{"id":"how-to-build-slurm-hpc-part-1","metadata":{"permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-1","source":"@site/blog/2026-01/2026-01-09.md","title":"Building a Slurm HPC Cluster (Part 1) - Single Node Setup and Fundamentals","description":"Building a High-Performance Computing (HPC) cluster can seem daunting, but with the right approach, you can create a robust system for managing computational workloads. This is Part 1 of a 3-part series where we\'ll build a complete Slurm cluster from scratch.","date":"2026-01-09T00:00:00.000Z","tags":[{"inline":true,"label":"slurm","permalink":"/river-docs/blog/tags/slurm"},{"inline":true,"label":"hpc","permalink":"/river-docs/blog/tags/hpc"},{"inline":true,"label":"tutorial","permalink":"/river-docs/blog/tags/tutorial"},{"inline":true,"label":"getting-started","permalink":"/river-docs/blog/tags/getting-started"}],"readingTime":7.65,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"how-to-build-slurm-hpc-part-1","title":"Building a Slurm HPC Cluster (Part 1) - Single Node Setup and Fundamentals","authors":["river"],"tags":["slurm","hpc","tutorial","getting-started"],"image":"./imgs/hpc_1.svg"},"unlisted":false,"prevItem":{"title":"Building a Slurm HPC Cluster (Part 2) - Scaling to Production with Ansible","permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-2"},"nextItem":{"title":"RIVER- A Web Application to Run Nf-Core","permalink":"/river-docs/blog/river-platform-and-nextflow"}},"content":"Building a High-Performance Computing (HPC) cluster can seem daunting, but with the right approach, you can create a robust system for managing computational workloads. This is **Part 1** of a 3-part series where we\'ll build a complete Slurm cluster from scratch.\\n\\nIn this first post, we\'ll cover the fundamentals by setting up a single-node Slurm cluster and understanding the core concepts.\\n\\n\x3c!--truncate--\x3e\\n\\n## Series Overview\\n\\n- **Part 1 (This Post)**: Introduction, Architecture, and Single Node Setup\\n- **[Part 2](/blog/how-to-build-slurm-hpc-part-2)**: Scaling to Production with Ansible\\n- **[Part 3](/blog/how-to-build-slurm-hpc-part-3)**: Administration and Best Practices\\n\\n## Why Slurm?\\n\\nWhen it comes to job scheduling in HPC environments, several options exist including PBS, Grid Engine, and IBM\'s LSF. However, Slurm (Simple Linux Utility for Resource Management) stands out for several compelling reasons:\\n\\n- **Open Source**: Free to use with a large, active community\\n- **Scalability**: Designed to scale from small clusters to the world\'s largest supercomputers\\n- **Flexibility**: Fine-grained control over job scheduling, resource allocation, and priority settings\\n- **Integration**: Works seamlessly with MPI, distributed computing frameworks (Spark, Ray, Dask), and monitoring tools\\n- **Performance**: Optimized for high throughput with minimal overhead\\n\\n## Understanding Slurm Architecture\\n\\nBefore diving into the implementation, it\'s crucial to understand the key components of a Slurm cluster:\\n\\n<figure markdown=\\"span\\">\\n    ![HPC Architecture](./imgs/HPC_architecture.png)\\n</figure>\\nsource: https://www.marquette.edu/high-performance-computing/architecture.php\\n\\n### Core Components\\n\\n1. **slurmctld (Controller Daemon)**: The brain of the cluster, running on the controller node. It handles job scheduling, resource tracking, and communicates with compute nodes.\\n\\n2. **slurmd (Node Daemon)**: Runs on compute nodes to execute jobs and report status back to the controller.\\n\\n3. **slurmdbd (Database Daemon)**: Optional but recommended for storing job accounting data, resource usage tracking, and fair-share scheduling.\\n\\n### Node Types\\n\\n| Node Type  | Services                | Purpose                              |\\n| ---------- | ----------------------- | ------------------------------------ |\\n| Controller | slurmctld               | Manages job scheduling and resources |\\n| Compute    | slurmd                  | Executes submitted jobs              |\\n| Login      | Slurm clients           | User access point for job submission |\\n| Database   | slurmdbd, MySQL/MariaDB | Stores accounting data               |\\n\\n<figure markdown=\\"span\\">\\n    ![Slurm Architecture](./imgs/slurm_arch.gif)\\n</figure>\\nsource: https://www.schedmd.com/\\n\\nFor a deeper understanding of Slurm architecture, check our [Slurm Architecture documentation](/docs/resources/high-performance-computing/slurm-architecture).\\n\\n## Single Node Setup - Understanding the Fundamentals\\n\\nStarting with a single-node setup helps you understand how Slurm works before scaling up. This approach is perfect for learning and local development.\\n\\n<div align=\\"center\\">\\n    <figure markdown=\\"span\\">\\n        ![Slurm Logo](./imgs/Slurm_logo.svg)\\n    </figure>\\n</div>\\n\\n:::info\\nThis setup runs on Ubuntu 20.04 and includes all standard Slurm features. Note that this configuration is for learning purposes - for production environments, you\'ll want the multi-node setup covered in Part 2.\\n:::\\n\\n### Basic Installation\\n\\nFirst, install the required Slurm components:\\n\\n```bash\\nsudo apt-get update -y && sudo apt-get install -y slurmd slurmctld\\n```\\n\\nVerify the installation:\\n\\n```bash\\n# Locate slurmd and slurmctld\\nwhich slurmd\\n# Output: /usr/sbin/slurmd\\n\\nwhich slurmctld\\n# Output: /usr/sbin/slurmctld\\n```\\n\\n### Configuring slurm.conf\\n\\nThe `slurm.conf` file is the heart of your Slurm configuration. This file must be identical across all nodes in a cluster (but for now, we just have one node).\\n\\nCreate your `slurm.conf`:\\n\\n```bash\\ncat <<EOF > slurm.conf\\n# slurm.conf for a single-node Slurm cluster\\nClusterName=localcluster\\nSlurmctldHost=localhost\\nMpiDefault=none\\nProctrackType=proctrack/linuxproc\\nReturnToService=2\\nSlurmctldPidFile=/run/slurmctld.pid\\nSlurmctldPort=6817\\nSlurmdPidFile=/run/slurmd.pid\\nSlurmdPort=6818\\nSlurmdSpoolDir=/var/lib/slurm-llnl/slurmd\\nSlurmUser=slurm\\nStateSaveLocation=/var/lib/slurm-llnl/slurmctld\\nSwitchType=switch/none\\nTaskPlugin=task/none\\n\\n# TIMERS\\nInactiveLimit=0\\nKillWait=30\\nMinJobAge=300\\nSlurmctldTimeout=120\\nSlurmdTimeout=300\\nWaittime=0\\n\\n# SCHEDULING\\nSchedulerType=sched/backfill\\nSelectType=select/cons_tres\\nSelectTypeParameters=CR_Core\\n\\n# ACCOUNTING (not enabled yet)\\nAccountingStorageType=accounting_storage/none\\nJobAcctGatherType=jobacct_gather/none\\nJobAcctGatherFrequency=30\\n\\n# LOGGING\\nSlurmctldDebug=info\\nSlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log\\nSlurmdDebug=info\\nSlurmdLogFile=/var/log/slurm-llnl/slurmd.log\\n\\n# COMPUTE NODES (adjust CPUs and RealMemory to match your system)\\nNodeName=localhost CPUs=2 Sockets=1 CoresPerSocket=2 ThreadsPerCore=1 RealMemory=1024 State=UNKNOWN\\n\\n# PARTITION CONFIGURATION\\nPartitionName=LocalQ Nodes=ALL Default=YES MaxTime=INFINITE State=UP\\nEOF\\n\\nsudo mv slurm.conf /etc/slurm-llnl/slurm.conf\\n```\\n\\n### Starting Services\\n\\nStart the Slurm daemons:\\n\\n```bash\\n# Start slurmd (compute daemon)\\nsudo service slurmd start\\nsudo service slurmd status\\n\\n# Start slurmctld (controller daemon)\\nsudo service slurmctld start\\nsudo service slurmctld status\\n```\\n\\n<figure markdown=\\"span\\">\\n    ![slurmd status](./imgs/slurmd_status.png)\\n</figure>\\n\\n<figure markdown=\\"span\\">\\n    ![slurmctld status](./imgs/slurmctld_status.png)\\n</figure>\\n\\nTest your setup by submitting a simple interactive job:\\n\\n```bash\\nsrun --mem 500MB -c 1 --pty bash\\n\\n# Check job details\\nsqueue -o \\"%i %P %u %T %M %l %D %C %m %R %Z %N\\" | column -t\\n```\\n\\n### Critical: Resource Limiting with cgroups\\n\\n:::warning\\n**This is a critical step that\'s often overlooked!**\\n\\nWithout proper cgroup configuration, jobs can exceed their allocated resources, potentially causing system instability or crashes. The job scheduler will accept resource limits, but won\'t actually enforce them.\\n:::\\n\\nLet\'s test this problem first. Submit a job requesting 500MB and try to allocate much more:\\n\\n```bash\\nsrun --mem 500MB -c 1 --pty bash\\n\\n# Try to allocate 1GB of memory (exceeding the 500MB limit)\\ndeclare -a mem\\ni=0\\nwhile :; do\\n    mem[$i]=$(head -c 100M </dev/zero | tr \'\\\\000\' \'x\') \\n    ((i++))\\n    echo \\"Allocated: $((i * 100)) MB\\"\\ndone\\n```\\n\\nBefore submitting the job, memory usage is less than 200MB:\\n\\n<figure markdown=\\"span\\">\\n    ![Memory before stress](./imgs/memory_before_stress.png)\\n</figure>\\n\\nAfter allocating 1GB, the job is not killed due to missing control group (cgroup) configuration:\\n\\n<figure markdown=\\"span\\">\\n    ![Over resource limit](./imgs/overresource_limit.png)\\n</figure>\\n\\nYou\'ll notice the job continues running even after exceeding 500MB - that\'s the problem!\\n\\nNow let\'s fix it with cgroups:\\n\\n```bash\\ncat <<EOF > cgroup.conf\\nCgroupAutomount=yes\\nCgroupMountpoint=/sys/fs/cgroup\\nConstrainCores=yes\\nConstrainRAMSpace=yes\\nConstrainDevices=yes\\nConstrainSwapSpace=yes\\nMaxSwapPercent=5\\nMemorySwappiness=0\\nEOF\\n\\nsudo mv cgroup.conf /etc/slurm-llnl/cgroup.conf\\n```\\n\\nUpdate `slurm.conf` to use cgroup plugins:\\n\\n```bash\\nsudo sed -i -e \\"s|ProctrackType=proctrack/linuxproc|ProctrackType=proctrack/cgroup|\\" \\\\\\n            -e \\"s|TaskPlugin=task/none|TaskPlugin=task/cgroup|\\" /etc/slurm-llnl/slurm.conf\\n```\\n\\nEnable cgroup in GRUB and reboot:\\n\\n```bash\\nsudo sed -i \'s/^GRUB_CMDLINE_LINUX=\\"/GRUB_CMDLINE_LINUX=\\"cgroup_enable=memory swapaccount=1 /\' /etc/default/grub\\nsudo update-grub\\nsudo reboot\\n```\\n\\nAfter reboot, restart Slurm services:\\n\\n```bash\\nsudo service slurmctld restart\\nsudo service slurmd restart\\n```\\n\\nNow test again with the same memory allocation script - this time, the job will be killed when it exceeds the limit!\\n\\n<figure markdown=\\"span\\">\\n    ![Out of Memory](./imgs/oom.png)\\n</figure>\\n\\n### Enabling Accounting\\n\\nJob accounting is essential for:\\n- Tracking who is using resources\\n- Monitoring job completion and failures\\n- Enforcing resource limits per user/group\\n- Fair-share scheduling\\n\\n<figure markdown=\\"span\\">\\n    ![Accounting disabled](./imgs/sacct_disable.png)\\n</figure>\\n\\nInstall the required packages:\\n\\n```bash\\nsudo apt-get install slurmdbd mariadb-server -y\\n```\\n\\nCreate the database and user:\\n\\n```bash\\nsudo service mysql start\\n\\nsudo mysql -e \\"CREATE DATABASE slurm_acct_db;\\"\\nsudo mysql -e \\"CREATE USER \'slurm\'@\'localhost\' IDENTIFIED BY \'slurm\';\\"\\nsudo mysql -e \\"GRANT ALL PRIVILEGES ON slurm_acct_db.* TO \'slurm\'@\'localhost\';\\"\\nsudo mysql -e \\"FLUSH PRIVILEGES;\\"\\n```\\n\\nVerify the database was created:\\n\\n```bash\\nsudo mysql -e \\"SHOW DATABASES;\\" \\nsudo mysql -e \\"SELECT User, Host FROM mysql.user;\\"\\n```\\n\\n<figure markdown=\\"span\\">\\n    ![Add database](./imgs/add_db.png)\\n</figure>\\n\\nConfigure `slurmdbd`:\\n\\n```bash\\ncat <<EOF > slurmdbd.conf\\nPidFile=/run/slurmdbd.pid\\nLogFile=/var/log/slurm/slurmdbd.log\\nDebugLevel=error\\nDbdHost=localhost\\nDbdPort=6819\\n\\n# DB connection data\\nStorageType=accounting_storage/mysql\\nStorageHost=localhost\\nStoragePort=3306\\nStorageUser=slurm\\nStoragePass=slurm\\nStorageLoc=slurm_acct_db\\nSlurmUser=slurm\\nEOF\\n\\nsudo mv slurmdbd.conf /etc/slurm-llnl/slurmdbd.conf\\nsudo service slurmdbd start\\n```\\n\\nUpdate `slurm.conf` to enable accounting:\\n\\n```bash\\nsudo sed -i -e \\"s|AccountingStorageType=accounting_storage/none|AccountingStorageType=accounting_storage/slurmdbd\\\\nAccountingStorageEnforce=associations,limits,qos\\\\nAccountingStorageHost=localhost\\\\nAccountingStoragePort=6819|\\" /etc/slurm-llnl/slurm.conf \\n\\nsudo sed -i -e \\"s|JobAcctGatherType=jobacct_gather/none|JobAcctGatherType=jobacct_gather/cgroup|\\" /etc/slurm-llnl/slurm.conf\\n\\nsudo systemctl restart slurmctld slurmd\\n```\\n\\nAdd your cluster and user to accounting:\\n\\n```bash\\n# Add cluster\\nsudo sacctmgr -i add cluster localcluster\\n\\n# Add account for your user\\nsudo sacctmgr -i add account $USER Cluster=localcluster\\n\\n# Add your user to the account\\nsudo sacctmgr -i add user $USER account=$USER DefaultAccount=$USER\\n\\nsudo systemctl restart slurmctld slurmd\\n```\\n\\n<figure markdown=\\"span\\">\\n    ![Add account](./imgs/add_account.png)\\n</figure>\\n\\nNow test accounting by submitting a job and viewing its details:\\n\\n```bash\\n# Submit a test job\\nsrun --mem 500MB -c 1 hostname\\n\\n# View accounting information\\nsacct\\n```\\n\\n<figure markdown=\\"span\\">\\n    ![Account usage](./imgs/account_usage.png)\\n</figure>\\n\\n## Key Takeaways\\n\\nIn this first part of our series, we\'ve covered:\\n\\n1. **Why Slurm**: Understanding the advantages of Slurm over alternatives\\n2. **Architecture**: Core components (slurmctld, slurmd, slurmdbd) and their roles\\n3. **Basic Setup**: Installing and configuring a single-node cluster\\n4. **Critical cgroups**: Why resource limiting is essential (and how to enable it)\\n5. **Accounting**: Setting up job tracking and resource monitoring\\n\\n:::info\\n**What\'s Next?**\\n\\nIn [Part 2](/blog/how-to-build-slurm-hpc-part-2), we\'ll take this knowledge and scale to a multi-node production cluster using Ansible automation. We\'ll add monitoring with Grafana, alerting via Slack, and shared storage with NFS.\\n:::\\n\\n## Resources\\n\\n- **Full Documentation**: [Single Node Slurm Setup](/docs/resources/high-performance-computing/how-to-build-slurm-single-node-with-full-functions)\\n- **Architecture Details**: [Slurm Architecture](/docs/resources/high-performance-computing/slurm-architecture)\\n- **HPC Overview**: [High-Performance Computing Overview](/docs/resources/high-performance-computing/high-performance-computing-overview)\\n- **Official Docs**: [SchedMD Slurm Documentation](https://slurm.schedmd.com/)\\n\\n---\\n\\n*This is Part 1 of the RiverXData series on building Slurm HPC clusters. Continue to [Part 2](/blog/how-to-build-slurm-hpc-part-2) to learn about production deployment with Ansible.*"},{"id":"river-platform-and-nextflow","metadata":{"permalink":"/river-docs/blog/river-platform-and-nextflow","source":"@site/blog/2026-01/2026-01-08.md","title":"RIVER- A Web Application to Run Nf-Core","description":"Simply, I just want a web application. This is an all-in-one application similar to Google Drive, but more advanced. It allows me to select files and put them into a workflow obtained directly from GitHub. For example, rnaseq from nf-core. The web application allows me to run and monitor Nextflow jobs. Then, it puts the results back to Google Drive, where I can view the results. It is useful for sharing results with my team or external teams. I can develop pipelines independently. My data can be stored on the cloud for backup. It is a simple and standard procedure for bioinformatics analysis. But I have not found any solution that fits these requirements is EASY, FREE, OPEN-SOURCE","date":"2026-01-08T00:00:00.000Z","tags":[{"inline":true,"label":"s3","permalink":"/river-docs/blog/tags/s-3"},{"inline":true,"label":"data-analysis","permalink":"/river-docs/blog/tags/data-analysis"},{"inline":true,"label":"slurm","permalink":"/river-docs/blog/tags/slurm"},{"inline":true,"label":"hpc","permalink":"/river-docs/blog/tags/hpc"},{"inline":true,"label":"web-platform","permalink":"/river-docs/blog/tags/web-platform"}],"readingTime":2.14,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"river-platform-and-nextflow","title":"RIVER- A Web Application to Run Nf-Core","authors":["river"],"tags":["s3","data-analysis","slurm","hpc","web-platform"],"image":"./imgs/intro.png"},"unlisted":false,"prevItem":{"title":"Building a Slurm HPC Cluster (Part 1) - Single Node Setup and Fundamentals","permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-1"}},"content":"Simply, I just want a web application. This is an all-in-one application similar to Google Drive, but more advanced. It allows me to select files and put them into a workflow obtained directly from GitHub. For example, rnaseq from nf-core. The web application allows me to run and monitor Nextflow jobs. Then, it puts the results back to Google Drive, where I can view the results. It is useful for sharing results with my team or external teams. I can develop pipelines independently. My data can be stored on the cloud for backup. It is a simple and standard procedure for bioinformatics analysis. But I have not found any solution that fits these requirements is `EASY`, `FREE`, `OPEN-SOURCE`\\n\\nIf you feel the same, **RIVER** may be your choice.\\n\\n\x3c!-- truncate --\x3e\\n:::info\\nThis blog post is intended for the general community, show cases only. I\'ll focus on the big picture rather than diving deep into technical details.\\n:::\\n\\nTo use it, you can follow the below tutorial\\n\\n## Quick review\\nFirst, you can take a look at how it work first. In breif, it will allows the users to interact with S3 bucket, select inputs from S3 buckets, run and put back results to s3 that\\nRIVER platform can visualize. After watching this, you are able to configure the RIVER platform as contributor, add your interested workflows and allow you to run tools\\n\\n\\n\\n<div style={{ position: \\"relative\\", paddingBottom: \\"56.25%\\", height: 0, overflow: \\"hidden\\", maxWidth: \\"100%\\", background: \\"#000\\" }}>\\n  <iframe \\n    src=\\"https://www.youtube.com/embed/boabEFNIkNA\\" \\n    frameBorder=\\"0\\" \\n    allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" \\n    allowFullScreen\\n    style={{ position: \\"absolute\\", top: 0, left: 0, width: \\"100%\\", height: \\"100%\\" }}\\n  />\\n</div> \\n\\n## Get started\\nYour raw data should be uploaded to the platform. The fastq files are downloaded from the below url, then upload to the platform\\n\\nThe samplesheet.csv on test profile\\n```bash\\nsample,fastq_1,fastq_2\\nSAMPLE1_PE,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample1_R1.fastq.gz,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample1_R2.fastq.gz\\nSAMPLE2_PE,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample2_R1.fastq.gz,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample2_R2.fastq.gz\\n```\\n\\nThe samplesheet.csv on RIVER\\n```bash\\nsample,fastq_1,fastq_2\\ntest,s3://genomics/demo/resources/fastq_files/sample1_R1.fastq.gz,s3://genomics/demo/resources/fastq_files/sample1_R2.fastq.gz\\n```\\n\\n<div style={{ position: \\"relative\\", paddingBottom: \\"56.25%\\", height: 0, overflow: \\"hidden\\", maxWidth: \\"100%\\", background: \\"#000\\" }}>\\n  <iframe \\n    src=\\"https://www.youtube.com/embed/Fujhi6ZnHQQ\\" \\n    frameBorder=\\"0\\" \\n    allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" \\n    allowFullScreen\\n    style={{ position: \\"absolute\\", top: 0, left: 0, width: \\"100%\\", height: \\"100%\\" }}\\n  />\\n</div>"}]}}')}}]);