"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[8448],{48995(e,n,s){s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>d});var i=s(45308),r=s(74848),t=s(28453);const a={slug:"unix-pipes-bioinformatics-streaming-data",title:"Unix Pipes in Bioinformatics: How Streaming Data Reduces Memory and Storage",authors:["river"],tags:["unix","pipes","bioinformatics","streaming","data-processing","performance"],image:"./imgs/intro.png"},l=void 0,o={image:s(29831).A,authorsImageUrls:[void 0]},d=[{value:"The Problem: Data Explosion in Bioinformatics",id:"the-problem-data-explosion-in-bioinformatics",level:2},{value:"The Solution: Unix Pipes for Streaming Data",id:"the-solution-unix-pipes-for-streaming-data",level:2},{value:"How Pipes Work Under the Hood",id:"how-pipes-work-under-the-hood",level:2},{value:"1. File Descriptors in Unix",id:"1-file-descriptors-in-unix",level:3},{value:"2. Creating a Pipe with the Pipe System Call",id:"2-creating-a-pipe-with-the-pipe-system-call",level:3},{value:"3. The Kernel Manages Data Flow",id:"3-the-kernel-manages-data-flow",level:3},{value:"4. Example: Tracing a Real Pipe",id:"4-example-tracing-a-real-pipe",level:3},{value:"5. Memory Efficiency: Why Pipes Don&#39;t Load Everything into RAM",id:"5-memory-efficiency-why-pipes-dont-load-everything-into-ram",level:3},{value:"Practical Example: Processing a Large FASTQ File",id:"practical-example-processing-a-large-fastq-file",level:2},{value:"Without Pipes (Wasteful)",id:"without-pipes-wasteful",level:3},{value:"With Pipes (Efficient)",id:"with-pipes-efficient",level:3},{value:"Advanced Pipe Patterns in Bioinformatics",id:"advanced-pipe-patterns-in-bioinformatics",level:2},{value:"1. Parallel Processing with GNU Parallel",id:"1-parallel-processing-with-gnu-parallel",level:3},{value:"2. Tee: Branching a Pipeline",id:"2-tee-branching-a-pipeline",level:3},{value:"3. Named Pipes (mkfifo) for Complex Workflows",id:"3-named-pipes-mkfifo-for-complex-workflows",level:3},{value:"4. Buffering and Backpressure Management",id:"4-buffering-and-backpressure-management",level:3},{value:"5. Process Substitution for Multiple Outputs",id:"5-process-substitution-for-multiple-outputs",level:3},{value:"Common Pipe Gotchas and Solutions",id:"common-pipe-gotchas-and-solutions",level:2},{value:"1. Buffering Issues with Pipes",id:"1-buffering-issues-with-pipes",level:3},{value:"2. Error Handling in Pipes",id:"2-error-handling-in-pipes",level:3},{value:"3. Monitoring Pipeline Progress",id:"3-monitoring-pipeline-progress",level:3},{value:"4. Debugging Pipe Failures",id:"4-debugging-pipe-failures",level:3},{value:"Using Pipes in Nextflow Modules",id:"using-pipes-in-nextflow-modules",level:2},{value:"Real Example: BWA_MEM Process from Sarek",id:"real-example-bwa_mem-process-from-sarek",level:3},{value:"Understanding the Pipe in BWA_MEM",id:"understanding-the-pipe-in-bwa_mem",level:3},{value:"Benefits in This Real Workflow",id:"benefits-in-this-real-workflow",level:3},{value:"Flexibility Through Arguments",id:"flexibility-through-arguments",level:3},{value:"Error Handling in Nextflow Processes",id:"error-handling-in-nextflow-processes",level:3},{value:"Pipes for Download, Upload, and Cloud Storage",id:"pipes-for-download-upload-and-cloud-storage",level:2},{value:"Example 1: Download \u2192 Decompress \u2192 Process (No Temp Files)",id:"example-1-download--decompress--process-no-temp-files",level:3},{value:"Example 2: Process \u2192 Compress \u2192 Upload to S3 (Single Pipeline)",id:"example-2-process--compress--upload-to-s3-single-pipeline",level:3},{value:"Example 3: Download from S3 \u2192 Decompress \u2192 Analyze (No Local Copy)",id:"example-3-download-from-s3--decompress--analyze-no-local-copy",level:3},{value:"Example 4: Download Tarball \u2192 Extract \u2192 Index (No Intermediate Files)",id:"example-4-download-tarball--extract--index-no-intermediate-files",level:3},{value:"Example 5: Process Multiple Files from S3 with GNU Parallel",id:"example-5-process-multiple-files-from-s3-with-gnu-parallel",level:3},{value:"Example 6: Bidirectional Piping: Upload Results as They&#39;re Generated",id:"example-6-bidirectional-piping-upload-results-as-theyre-generated",level:3},{value:"Tips for Reliable Download/Upload Pipes",id:"tips-for-reliable-downloadupload-pipes",level:3},{value:"Critical Considerations for Cloud Pipes",id:"critical-considerations-for-cloud-pipes",level:3},{value:"Real-World Bioinformatics Pipes",id:"real-world-bioinformatics-pipes",level:2},{value:"Example 1: RNA-seq Quality Control Pipeline",id:"example-1-rna-seq-quality-control-pipeline",level:3},{value:"Example 2: Variant Calling Pipeline",id:"example-2-variant-calling-pipeline",level:3},{value:"Example 3: FASTA Processing with Decompression",id:"example-3-fasta-processing-with-decompression",level:3},{value:"Summary: Why Pipes Matter in Bioinformatics",id:"summary-why-pipes-matter-in-bioinformatics",level:2},{value:"Key Takeaways",id:"key-takeaways",level:3},{value:"Final Thoughts",id:"final-thoughts",level:2}];function c(e){const n={code:"code",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"Unix pipes (|) are one of the most powerful yet underutilized features in bioinformatics. They allow you to chain multiple commands together, processing data in a streaming fashion that dramatically reduces memory usage and disk I/O. This post explores why pipes are essential for bioinformatics work and shows how they work under the hood."}),"\n",(0,r.jsx)(n.h2,{id:"the-problem-data-explosion-in-bioinformatics",children:"The Problem: Data Explosion in Bioinformatics"}),"\n",(0,r.jsx)(n.p,{children:"Modern sequencing generates massive datasets. A single human genome sequencing run can produce:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Raw reads:"})," 100+ GB of FASTQ files"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Alignments:"})," 50-100 GB of BAM files"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Variants:"})," 1-5 GB of VCF files"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Without pipes, traditional bioinformatics workflows create intermediate files at each step:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Traditional approach (\u274c Wasteful)\nbwa mem reference.fa reads.fastq > aligned.sam    # 200 GB intermediate\nsamtools view -b aligned.sam > aligned.bam        # 100 GB intermediate\nsamtools sort aligned.bam > sorted.bam            # 100 GB intermediate\nsamtools index sorted.bam                         # Already indexed\n# Total disk usage: 400 GB for processing 100 GB of raw data!\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"The costs:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Storage:"})," 4x the original data size just for intermediates"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Time:"})," Writing/reading intermediate files is slow"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Failure:"})," If a step fails midway, you lose everything and restart from scratch"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Complexity:"})," Managing and cleaning up intermediate files is tedious"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"the-solution-unix-pipes-for-streaming-data",children:"The Solution: Unix Pipes for Streaming Data"}),"\n",(0,r.jsxs)(n.p,{children:["Pipes (",(0,r.jsx)(n.code,{children:"|"}),") connect the output of one command directly to the input of the next, processing data in memory as it flows through:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Pipe approach (\u2713 Efficient)\nbwa mem reference.fa reads.fastq | \\\n  samtools view -b - | \\\n  samtools sort -o sorted.bam -\nsamtools index sorted.bam\n# Total disk usage: 100 GB (original data + final output)\n# No intermediate files!\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"The benefits:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Memory efficient:"})," Data flows through without full copies in RAM"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fast:"})," No disk I/O for intermediates"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Resilient:"})," Failures are caught immediately and clearly"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple:"})," Clean, readable pipeline syntax"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Storage efficient:"})," Only keep final outputs"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"how-pipes-work-under-the-hood",children:"How Pipes Work Under the Hood"}),"\n",(0,r.jsx)(n.p,{children:"Understanding how pipes work is key to writing efficient bioinformatics pipelines. Let's explore the mechanics."}),"\n",(0,r.jsx)(n.h3,{id:"1-file-descriptors-in-unix",children:"1. File Descriptors in Unix"}),"\n",(0,r.jsx)(n.p,{children:"Every Unix process has three standard file descriptors:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Descriptor"}),(0,r.jsx)(n.th,{children:"Name"}),(0,r.jsx)(n.th,{children:"Purpose"}),(0,r.jsx)(n.th,{children:"Default Target"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"0"}),(0,r.jsx)(n.td,{children:"stdin"}),(0,r.jsx)(n.td,{children:"Standard input"}),(0,r.jsx)(n.td,{children:"Keyboard"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"1"}),(0,r.jsx)(n.td,{children:"stdout"}),(0,r.jsx)(n.td,{children:"Standard output"}),(0,r.jsx)(n.td,{children:"Terminal/Screen"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"2"}),(0,r.jsx)(n.td,{children:"stderr"}),(0,r.jsx)(n.td,{children:"Standard error"}),(0,r.jsx)(n.td,{children:"Terminal/Screen"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"By default, processes read from stdin (file descriptor 0) and write to stdout (file descriptor 1)."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Example: cat command\ncat myfile.txt    # Reads file, writes to stdout (terminal)\ncat < myfile.txt  # Explicitly redirect stdin from file\ncat myfile.txt 1> output.txt  # Redirect stdout to file\n"})}),"\n",(0,r.jsx)(n.h3,{id:"2-creating-a-pipe-with-the-pipe-system-call",children:"2. Creating a Pipe with the Pipe System Call"}),"\n",(0,r.jsxs)(n.p,{children:["When you type ",(0,r.jsx)(n.code,{children:"command1 | command2"}),", the shell:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Creates a pipe"})," - an unnamed, in-memory buffer that connects two processes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Forks process 1"})," - creates a copy of the shell running ",(0,r.jsx)(n.code,{children:"command1"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Connects stdout of process 1"})," to the pipe's write end (file descriptor 1)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Forks process 2"})," - creates a copy of the shell running ",(0,r.jsx)(n.code,{children:"command2"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Connects stdin of process 2"})," to the pipe's read end (file descriptor 0)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Executes both processes"})," - they run in parallel"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Diagram of a pipe:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"[command1]                    [command2]\n     |                              |\n  stdout (fd 1)              stdin (fd 0)\n     |                              |\n     v                              ^\n    [================================]\n              Pipe (kernel buffer)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"3-the-kernel-manages-data-flow",children:"3. The Kernel Manages Data Flow"}),"\n",(0,r.jsxs)(n.p,{children:["The Unix kernel manages the pipe as a ",(0,r.jsx)(n.strong,{children:"FIFO (First In, First Out) buffer"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Write side:"})," ",(0,r.jsx)(n.code,{children:"command1"})," writes data to the pipe's write end"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pipe buffer:"})," Data sits in kernel memory (typically 64KB-1MB per pipe)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Read side:"})," ",(0,r.jsx)(n.code,{children:"command2"})," reads data from the pipe's read end"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key behaviors:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"If pipe is full:"})," The writing process blocks until space is available"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"If pipe is empty:"})," The reading process blocks until data arrives"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"If reader closes:"})," Writer gets a SIGPIPE signal (broken pipe error)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"If writer closes:"})," Reader gets EOF and can finish processing"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This synchronization happens automatically\u2014you don't need to manage it."}),"\n",(0,r.jsx)(n.h3,{id:"4-example-tracing-a-real-pipe",children:"4. Example: Tracing a Real Pipe"}),"\n",(0,r.jsx)(n.p,{children:"Let's trace what happens when you run:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'cat genome.fasta | grep ">chr1" | wc -l\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Step 1: Shell creates the pipeline"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'User types: cat genome.fasta | grep ">chr1" | wc -l\n         \u2193\nShell creates:\n  - Pipe A (between cat and grep)\n  - Pipe B (between grep and wc)\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Step 2: Processes fork and file descriptors redirect"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'Process: cat genome.fasta\n  stdout (fd 1) \u2192 Pipe A write end\n\nProcess: grep ">chr1"\n  stdin (fd 0)  \u2190 Pipe A read end\n  stdout (fd 1) \u2192 Pipe B write end\n\nProcess: wc -l\n  stdin (fd 0)  \u2190 Pipe B read end\n  stdout (fd 1) \u2192 Terminal\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Step 3: Execution flows"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Pipe A buffer:     Pipe B buffer:\n[data from cat] \u2192 [data to grep] \u2192 [data to wc] \u2192 [count to terminal]\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Step 4: Back pressure synchronization"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"If grep is slow:\n  - Pipe A fills up\n  - cat blocks (can't write)\n  - System automatically waits for grep to catch up\n\nIf wc is slow:\n  - Pipe B fills up\n  - grep blocks (can't write)\n  - System waits for wc to catch up\n"})}),"\n",(0,r.jsx)(n.h3,{id:"5-memory-efficiency-why-pipes-dont-load-everything-into-ram",children:"5. Memory Efficiency: Why Pipes Don't Load Everything into RAM"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Without pipes (writing to disk):"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"command1: Read all data \u2192 Write 100GB to disk\n          \u2193\n          Disk (100GB) \u2190 Slow, uses storage\n          \u2193\ncommand2: Read 100GB from disk \u2192 Process \u2192 Write results\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"With pipes (streaming):"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"command1: Read chunk \u2192 Write chunk to pipe buffer (64KB)\n                      \u2193\n                   Pipe buffer (kernel memory, reused)\n                      \u2193\ncommand2: Read chunk \u2192 Process \u2192 Write chunk to next pipe\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Only a small amount of data (one buffer, ~64KB) sits in memory at any time. The buffer is ",(0,r.jsx)(n.strong,{children:"reused"})," as data flows through, so memory usage stays constant regardless of total data size."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"This is the magic of pipes:"})," Constant memory usage, not linear in data size."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"practical-example-processing-a-large-fastq-file",children:"Practical Example: Processing a Large FASTQ File"}),"\n",(0,r.jsx)(n.p,{children:"Let's apply pipes to a real bioinformatics workflow."}),"\n",(0,r.jsx)(n.h3,{id:"without-pipes-wasteful",children:"Without Pipes (Wasteful)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Step 1: Filter low-quality reads\nfastqc reads.fastq --outdir=qc_before/\nfastq_quality_filter -i reads.fastq -o reads_filtered.fastq -q 20 -p 80\n# File created: reads_filtered.fastq (70 GB)\n\n# Step 2: Count remaining reads\nwc -l reads_filtered.fastq\n# Temporary file: 70 GB on disk\n\n# Step 3: Get sequence length distribution\nawk 'NR%4==2 {print length}' reads_filtered.fastq | \\\n  sort -n | uniq -c > length_dist.txt\n# Processing reads_filtered.fastq again\n\n# Cleanup\nrm reads_filtered.fastq\n\n# Total I/O: Read source (100GB) + Write filtered (70GB) + Read filtered (70GB) = 240GB\n# Total disk: 170 GB (100 source + 70 intermediate)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"with-pipes-efficient",children:"With Pipes (Efficient)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Single streaming pipeline: Filter \u2192 Count \u2192 Length distribution\nfastq_quality_filter -i reads.fastq -q 20 -p 80 | \\\n  tee >(wc -l > read_count.txt) | \\\n  awk 'NR%4==2 {print length}' | \\\n  sort -n | uniq -c > length_dist.txt\n\n# Total I/O: Read source (100GB) = 100GB\n# Total disk: 100 GB (source + final outputs only)\n# Memory: ~100MB (pipe buffers)\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Comparison:"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Metric"}),(0,r.jsx)(n.th,{children:"Without Pipes"}),(0,r.jsx)(n.th,{children:"With Pipes"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Total I/O"})}),(0,r.jsx)(n.td,{children:"240 GB"}),(0,r.jsx)(n.td,{children:"100 GB"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Disk space"})}),(0,r.jsx)(n.td,{children:"170 GB"}),(0,r.jsx)(n.td,{children:"100 GB"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Processing time"})}),(0,r.jsx)(n.td,{children:"Slow (multiple reads from disk)"}),(0,r.jsx)(n.td,{children:"Fast (one sequential read)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Memory usage"})}),(0,r.jsx)(n.td,{children:"Streaming OK"}),(0,r.jsx)(n.td,{children:"Streaming OK"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Failure recovery"})}),(0,r.jsx)(n.td,{children:"Restart entire pipeline"}),(0,r.jsx)(n.td,{children:"Pipeline atomicity issues"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"advanced-pipe-patterns-in-bioinformatics",children:"Advanced Pipe Patterns in Bioinformatics"}),"\n",(0,r.jsx)(n.h3,{id:"1-parallel-processing-with-gnu-parallel",children:"1. Parallel Processing with GNU Parallel"}),"\n",(0,r.jsx)(n.p,{children:"Process multiple files simultaneously while piping:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Process 1000 FASTQ files in parallel\nfind . -name \"*.fastq\" | \\\n  parallel --pipe --block 10M \\\n  'fastq_quality_filter -q 20 -p 80 | gzip' > all_filtered.fastq.gz\n"})}),"\n",(0,r.jsx)(n.h3,{id:"2-tee-branching-a-pipeline",children:"2. Tee: Branching a Pipeline"}),"\n",(0,r.jsxs)(n.p,{children:["Use ",(0,r.jsx)(n.code,{children:"tee"})," to send data to multiple streams:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Simultaneously:\n# 1. Count reads\n# 2. Filter and output\n# 3. Generate statistics\nsamtools view input.bam | \\\n  tee >(samtools flagstat /dev/stdin > flagstat.txt) | \\\n  samtools view -b -F 4 | \\\n  samtools sort -o sorted_aligned.bam -\n\n# The >(command) syntax is "process substitution"\n# It creates a named pipe to a subprocess\n'})}),"\n",(0,r.jsx)(n.h3,{id:"3-named-pipes-mkfifo-for-complex-workflows",children:"3. Named Pipes (mkfifo) for Complex Workflows"}),"\n",(0,r.jsx)(n.p,{children:"For workflows requiring multiple inputs/outputs:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Create named pipes\nmkfifo pipe1 pipe2\n\n# Process A writes to pipe1, reads from pipe2\ncat input.txt > pipe1 &\n\n# Process B reads from pipe1, writes to pipe2\nsort < pipe1 > pipe2 &\n\n# Main process reads final result\nuniq < pipe2\n\n# Cleanup\nrm pipe1 pipe2\n"})}),"\n",(0,r.jsx)(n.h3,{id:"4-buffering-and-backpressure-management",children:"4. Buffering and Backpressure Management"}),"\n",(0,r.jsxs)(n.p,{children:["Sometimes a slow downstream command creates a bottleneck. Use ",(0,r.jsx)(n.code,{children:"buffer"})," to add extra memory:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Without buffer: cat blocks if samtools is slow\ncat large.bam | samtools view -c\n\n# With buffer: Extra memory absorbs the blocking\ncat large.bam | buffer -m 500M | samtools view -c\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"When to use buffer:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Downstream process is much slower than upstream"}),"\n",(0,r.jsx)(n.li,{children:"You have spare RAM and want to minimize blocking"}),"\n",(0,r.jsx)(n.li,{children:"Upstream data is expensive to regenerate"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"5-process-substitution-for-multiple-outputs",children:"5. Process Substitution for Multiple Outputs"}),"\n",(0,r.jsx)(n.p,{children:"Fan out to multiple processes from a single input:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Single BAM file \u2192 Multiple statistics simultaneously\nsamtools view input.bam | \\\n  tee >(samtools view -c > read_count.txt) \\\n      >(awk '{print $3}' | sort | uniq -c > chromosome_dist.txt) \\\n      >(awk '{print $4}' | sort -n > position_stats.txt) \\\n      > /dev/null\n\n# All three statistics generated from a single read of input.bam\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"common-pipe-gotchas-and-solutions",children:"Common Pipe Gotchas and Solutions"}),"\n",(0,r.jsx)(n.h3,{id:"1-buffering-issues-with-pipes",children:"1. Buffering Issues with Pipes"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem:"})," Interactive commands don't flush output in pipes"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# This might hang (python buffers output in pipe mode)\npython long_script.py | tee output.log\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solution:"})," Disable buffering"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Python: Use -u flag or stdbuf\npython -u long_script.py | tee output.log\n# Or use stdbuf\nstdbuf -oL python long_script.py | tee output.log\n"})}),"\n",(0,r.jsx)(n.h3,{id:"2-error-handling-in-pipes",children:"2. Error Handling in Pipes"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem:"})," Errors in the middle command are silent"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# If samtools fails, cat and downstream still succeed!\ncat input.bam | samtools view -b - > output.bam 2>/dev/null\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solution:"})," Set pipefail"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# If ANY command fails, the entire pipeline fails\nset -o pipefail\n\ncat input.bam | samtools view -b - > output.bam\n"})}),"\n",(0,r.jsx)(n.h3,{id:"3-monitoring-pipeline-progress",children:"3. Monitoring Pipeline Progress"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem:"})," Pipes process data silently\u2014hard to see progress"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# No feedback on progress\ncat input.fastq | process_cmd | sort > output.txt\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solution:"})," Use ",(0,r.jsx)(n.code,{children:"pv"})," (pipe viewer) to visualize throughput"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Shows progress, speed, and ETA\ncat input.fastq | pv | process_cmd | sort > output.txt\n\n# Or with file size estimation\npv -N "Reading FASTQ" < input.fastq | process_cmd | \\\n  pv -N "Sorting output" | sort > output.txt\n'})}),"\n",(0,r.jsx)(n.h3,{id:"4-debugging-pipe-failures",children:"4. Debugging Pipe Failures"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem:"})," Which command in the pipeline failed?"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Unclear where failure occurred\ncmd1 | cmd2 | cmd3 | cmd4 failed but which one?\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solution:"})," Use intermediate tee files for debugging"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Save intermediate outputs while piping\ncmd1 | tee /tmp/debug1.txt | \\\n  cmd2 | tee /tmp/debug2.txt | \\\n  cmd3 | tee /tmp/debug3.txt | \\\n  cmd4\n\n# Later, inspect intermediates\ncat /tmp/debug1.txt | head\ncat /tmp/debug2.txt | head\ncat /tmp/debug3.txt | head\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"using-pipes-in-nextflow-modules",children:"Using Pipes in Nextflow Modules"}),"\n",(0,r.jsx)(n.p,{children:"Nextflow pipelines are built from individual processes, and each process can use pipes internally to efficiently chain tools together. This is where pipes truly shine in production bioinformatics workflows."}),"\n",(0,r.jsx)(n.h3,{id:"real-example-bwa_mem-process-from-sarek",children:"Real Example: BWA_MEM Process from Sarek"}),"\n",(0,r.jsxs)(n.p,{children:["The Sarek variant calling pipeline includes a ",(0,r.jsx)(n.code,{children:"BWA_MEM"})," process that demonstrates best practices for using pipes in Nextflow. Let's examine how pipes reduce intermediate files:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-groovy",children:'process BWA_MEM {\n    tag "$meta.id"\n    label \'process_high\'\n\n    conda "${moduleDir}/environment.yml"\n    container "${ workflow.containerEngine == \'singularity\' && !task.ext.singularity_pull_docker_container ?\n        \'https://community-cr-prod.seqera.io/docker/registry/v2/blobs/sha256/bf/bf7890f8d4e38a7586581cb7fa13401b7af1582f21d94eef969df4cea852b6da/data\' :\n        \'community.wave.seqera.io/library/bwa_htslib_samtools:56c9f8d5201889a4\' }"\n\n    input:\n    tuple val(meta) , path(reads)\n    tuple val(meta2), path(index)\n    tuple val(meta3), path(fasta)\n    val   sort_bam\n\n    output:\n    tuple val(meta), path("*.bam")  , emit: bam,    optional: true\n    tuple val(meta), path("*.cram") , emit: cram,   optional: true\n    tuple val(meta), path("*.csi")  , emit: csi,    optional: true\n    tuple val(meta), path("*.crai") , emit: crai,   optional: true\n    path  "versions.yml"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: \'\'\n    def args2 = task.ext.args2 ?: \'\'\n    def prefix = task.ext.prefix ?: "${meta.id}"\n    def samtools_command = sort_bam ? \'sort\' : \'view\'\n    def extension = args2.contains("--output-fmt sam")   ? "sam" :\n                    args2.contains("--output-fmt cram")  ? "cram":\n                    sort_bam && args2.contains("-O cram")? "cram":\n                    !sort_bam && args2.contains("-C")    ? "cram":\n                    "bam"\n    def reference = fasta && extension=="cram"  ? "--reference ${fasta}" : ""\n    if (!fasta && extension=="cram") error "Fasta reference is required for CRAM output"\n    """\n    INDEX=`find -L ./ -name "*.amb" | sed \'s/\\\\.amb\\$//\'`\n\n    bwa mem \\\\\n        $args \\\\\n        -t $task.cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools $samtools_command $args2 ${reference} --threads $task.cpus -o ${prefix}.${extension} -\n\n    cat <<-END_VERSIONS > versions.yml\n    "${task.process}":\n        bwa: \\$(echo \\$(bwa 2>&1) | sed \'s/^.*Version: //; s/Contact:.*\\$//\')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed \'s/^.*samtools //; s/Using.*\\$//\')\n    END_VERSIONS\n    """\n\n    stub:\n    def args2 = task.ext.args2 ?: \'\'\n    def prefix = task.ext.prefix ?: "${meta.id}"\n    def extension = args2.contains("--output-fmt sam")   ? "sam" :\n                    args2.contains("--output-fmt cram")  ? "cram":\n                    sort_bam && args2.contains("-O cram")? "cram":\n                    !sort_bam && args2.contains("-C")    ? "cram":\n                    "bam"\n    """\n    touch ${prefix}.${extension}\n    touch ${prefix}.csi\n    touch ${prefix}.crai\n\n    cat <<-END_VERSIONS > versions.yml\n    "${task.process}":\n        bwa: \\$(echo \\$(bwa 2>&1) | sed \'s/^.*Version: //; s/Contact:.*\\$//\')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed \'s/^.*samtools //; s/Using.*\\$//\')\n    END_VERSIONS\n    """\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"understanding-the-pipe-in-bwa_mem",children:"Understanding the Pipe in BWA_MEM"}),"\n",(0,r.jsx)(n.p,{children:"The key line in this process is the pipe connecting bwa and samtools:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"bwa mem $args -t $task.cpus $INDEX $reads | samtools $samtools_command $args2 ${reference} --threads $task.cpus -o ${prefix}.${extension} -\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Here's what happens:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"bwa mem"})," aligns reads to reference genome"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Outputs SAM format to stdout"}),"\n",(0,r.jsx)(n.li,{children:"This is hundreds of GB for large genomes"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Without the pipe,"})," this would be written to disk as an intermediate ",(0,r.jsx)(n.code,{children:".sam"})," file"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Pipe (|)"})," connects output directly to samtools"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Data streams from bwa to samtools in memory"}),"\n",(0,r.jsx)(n.li,{children:"No intermediate SAM file created"}),"\n",(0,r.jsx)(n.li,{children:"Constant memory usage regardless of dataset size"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"samtools sort/view"})," processes the SAM stream"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Either sorts (if ",(0,r.jsx)(n.code,{children:"sort_bam=true"}),") or just converts format"]}),"\n",(0,r.jsxs)(n.li,{children:["Final output directly to ",(0,r.jsx)(n.code,{children:".bam"}),", ",(0,r.jsx)(n.code,{children:".cram"}),", or ",(0,r.jsx)(n.code,{children:".sam"})]}),"\n",(0,r.jsxs)(n.li,{children:["Uses ",(0,r.jsx)(n.code,{children:"--threads $task.cpus"})," to parallelize efficiently"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"benefits-in-this-real-workflow",children:"Benefits in This Real Workflow"}),"\n",(0,r.jsxs)(n.p,{children:["For a ",(0,r.jsx)(n.strong,{children:"human whole-genome sequencing run"})," (~100GB raw reads):"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Without pipes (hypothetical):"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"bwa mem $INDEX $reads > aligned.sam      # 500+ GB intermediate\nsamtools sort aligned.sam > sorted.bam   # 100+ GB final\nrm aligned.sam\n"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Storage needed: 600+ GB"}),"\n",(0,r.jsx)(n.li,{children:"Disk I/O: Read 100GB (bwa) + Write 500GB (SAM) + Read 500GB (samtools) = 1.1TB of I/O"}),"\n",(0,r.jsx)(n.li,{children:"Time: Significantly slower due to I/O contention"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"With pipes (Sarek approach):"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"bwa mem $INDEX $reads | samtools sort -o sorted.bam -\n"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Storage needed: 100 GB (final output only)"}),"\n",(0,r.jsx)(n.li,{children:"Disk I/O: Read 100GB (bwa input) + Write 100GB (final BAM) = 200GB of I/O"}),"\n",(0,r.jsx)(n.li,{children:"Time: 2-5x faster due to eliminated intermediate I/O"}),"\n",(0,r.jsx)(n.li,{children:"Memory: Constant (~2-3GB) regardless of genome size"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"flexibility-through-arguments",children:"Flexibility Through Arguments"}),"\n",(0,r.jsx)(n.p,{children:"The Sarek process demonstrates production-grade flexibility:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-groovy",children:"def samtools_command = sort_bam ? 'sort' : 'view'\n"})}),"\n",(0,r.jsx)(n.p,{children:"This single line allows switching between:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sorting in the pipeline"})," (slower but sorted output)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"No sorting"})," (faster, relies on downstream tools)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"And the extension can be dynamically chosen:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-groovy",children:'def extension = args2.contains("--output-fmt cram") ? "cram" : "bam"\n'})}),"\n",(0,r.jsxs)(n.p,{children:["This means the ",(0,r.jsx)(n.strong,{children:"same process"})," can output BAM, CRAM, or SAM depending on configuration\u2014all using the same efficient pipe pattern."]}),"\n",(0,r.jsx)(n.h3,{id:"error-handling-in-nextflow-processes",children:"Error Handling in Nextflow Processes"}),"\n",(0,r.jsx)(n.p,{children:"Nextflow automatically handles pipe failures. If any command in the chain fails:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"bwa mem ... | samtools sort ...  # If either fails, Nextflow catches it\n"})}),"\n",(0,r.jsx)(n.p,{children:"The entire task is marked as failed, and error logs are captured. This is superior to manual bash scripts where errors can be silent."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"To ensure pipeline stops on any error,"})," include at the top of your Nextflow script:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-groovy",children:"process {\n    shell = ['/bin/bash', '-euo', 'pipefail']  // -o pipefail catches pipe errors\n}\n"})}),"\n",(0,r.jsx)(n.p,{children:"This ensures that if bwa fails and outputs nothing, samtools will detect it as an error rather than silently succeeding on empty input."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"pipes-for-download-upload-and-cloud-storage",children:"Pipes for Download, Upload, and Cloud Storage"}),"\n",(0,r.jsx)(n.p,{children:"One of the most practical uses of pipes in bioinformatics is combining download/upload tools with data processing, eliminating intermediate files entirely. This is especially valuable when working with cloud storage like AWS S3."}),"\n",(0,r.jsx)(n.h3,{id:"example-1-download--decompress--process-no-temp-files",children:"Example 1: Download \u2192 Decompress \u2192 Process (No Temp Files)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Scenario:"})," You need to download a compressed reference genome from a public server, decompress it, and index it\u2014all without storing the compressed file."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Traditional approach (\u274c Wasteful)\nwget https://example.com/reference.fasta.gz  # Downloads 5GB\ngunzip reference.fasta.gz                     # Decompresses to 15GB\nsamtools faidx reference.fasta                # Index the reference\nrm reference.fasta                            # Cleanup takes time\n# Total disk space needed: 20GB\n\n# Pipe approach (\u2713 Efficient)\nwget -q -O - https://example.com/reference.fasta.gz | \\\n  gunzip | \\\n  samtools faidx /dev/stdin\n# Total disk space needed: 15GB (final reference only)\n# No temporary files, no cleanup needed\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"What happens:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"wget -O -"})," downloads to stdout (not to disk)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"gunzip"})," decompresses the stream on-the-fly"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"samtools faidx /dev/stdin"})," creates the index directly from the stream"]}),"\n",(0,r.jsx)(n.li,{children:"Final reference file and index stored without intermediate compressed file"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"example-2-process--compress--upload-to-s3-single-pipeline",children:"Example 2: Process \u2192 Compress \u2192 Upload to S3 (Single Pipeline)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Scenario:"})," Process sequencing data and upload the compressed results directly to AWS S3 without creating local compressed files."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Traditional approach (\u274c Wasteful)\nbwa mem reference.fa reads.fastq | samtools sort -o aligned.bam -\ngzip aligned.bam                           # Creates aligned.bam.gz (100+ GB)\naws s3 cp aligned.bam.gz s3://my-bucket/  # Upload\nrm aligned.bam.gz                          # Cleanup\n# Total disk space: 200+ GB (original BAM + compressed file)\n\n# Pipe approach (\u2713 Efficient)\nbwa mem reference.fa reads.fastq | \\\n  samtools sort -b - | \\\n  gzip | \\\n  aws s3 cp - s3://my-bucket/aligned.bam.gz\n# Total disk space: 0 GB temporary files\n# Compressed data uploaded directly to S3\n# Only stores the final index or metadata files locally\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"What happens:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"bwa mem"})," and ",(0,r.jsx)(n.code,{children:"samtools sort"})," chain together (as before)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"gzip"})," compresses the BAM stream on-the-fly"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"aws s3 cp -"})," uploads directly from stdin to S3"]}),"\n",(0,r.jsx)(n.li,{children:"Data never exists uncompressed on disk"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"example-3-download-from-s3--decompress--analyze-no-local-copy",children:"Example 3: Download from S3 \u2192 Decompress \u2192 Analyze (No Local Copy)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Scenario:"})," Analyze BAM files stored in S3 without downloading the entire uncompressed file locally."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Traditional approach (\u274c Wasteful)\naws s3 cp s3://my-bucket/sample.bam.gz .   # Download 50GB\ngunzip sample.bam.gz                        # Decompress to 150GB\nsamtools flagstat sample.bam                # Analyze\n# Local disk: 200GB, slow downloads\n\n# Pipe approach (\u2713 Efficient)\naws s3 cp s3://my-bucket/sample.bam.gz - | \\\n  gunzip | \\\n  samtools flagstat /dev/stdin\n# Local disk: 0 GB temporary files\n# Stream analysis, no storage overhead\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"What happens:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"aws s3 cp - s3://..."})," downloads from S3 to stdout"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"gunzip"})," decompresses the stream"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"samtools flagstat"})," reads and analyzes directly from the pipe"]}),"\n",(0,r.jsx)(n.li,{children:"Only metadata (flagstat results) stored locally"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"example-4-download-tarball--extract--index-no-intermediate-files",children:"Example 4: Download Tarball \u2192 Extract \u2192 Index (No Intermediate Files)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Scenario:"})," Download a compressed archive of reference sequences, extract, and index\u2014all in one pipeline."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Traditional approach (\u274c Wasteful)\nwget https://example.com/genomes.tar.gz    # 10GB download\ntar -xzf genomes.tar.gz                     # Extracts to 50GB\nfor ref in genomes/*.fasta; do\n  samtools faidx "$ref"\ndone\nrm -rf genomes.tar.gz genomes/              # Cleanup\n# Disk space: 60GB temporary files\n\n# Pipe approach (\u2713 Efficient)\nwget -q -O - https://example.com/genomes.tar.gz | \\\n  tar -xz --to-stdout | \\\n  while read -r line; do\n    echo "$line" >> current_genome.fasta\n    if [[ "$line" =~ ^> ]] && [ -s current_genome.fasta ]; then\n      samtools faidx current_genome.fasta\n    fi\n  done\n# Or more elegantly with GNU tar:\nwget -q -O - https://example.com/genomes.tar.gz | \\\n  tar -xz -C /dev/shm  # Extract to RAM disk (if available)\n# Disk space: Only final index files\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Better approach with GNU tar's streaming:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Extract specific files from tarball without decompressing entire archive\nwget -q -O - https://example.com/genomes.tar.gz | \\\n  tar -xzf - genomes/reference.fasta --to-stdout | \\\n  samtools faidx /dev/stdin\n"})}),"\n",(0,r.jsx)(n.h3,{id:"example-5-process-multiple-files-from-s3-with-gnu-parallel",children:"Example 5: Process Multiple Files from S3 with GNU Parallel"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Scenario:"})," Process many files in S3 in parallel using pipes and parallel processing."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# List all BAM files in S3 and process in parallel\naws s3 ls s3://my-bucket/bams/ --recursive | awk '{print $4}' | \\\n  parallel --pipe --block 10M \\\n  'aws s3 cp \"s3://my-bucket/{}\" - | \\\n   samtools view -b -F 4 | \\\n   samtools sort -o {/.}.sorted.bam -'\n\n# What happens:\n# 1. List all S3 objects\n# 2. Process up to N files in parallel\n# 3. Each file is downloaded and piped directly to samtools\n# 4. Results written back to disk (or piped to another command)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"example-6-bidirectional-piping-upload-results-as-theyre-generated",children:"Example 6: Bidirectional Piping: Upload Results as They're Generated"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Scenario:"})," Generate analysis results and upload to S3 incrementally (useful for long-running processes)."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Real-time upload of VCF variants as they\'re discovered\nbcftools mpileup -f reference.fa sample.bam | \\\n  bcftools call -m | \\\n  tee >(gzip | aws s3 cp - s3://my-bucket/variants.vcf.gz) | \\\n  grep -v "^#" | \\\n  wc -l\n  \n# What happens:\n# 1. bcftools generates variants\n# 2. tee splits the stream into two paths:\n#    - First path: gzip and upload to S3 in real-time\n#    - Second path: count total variants\n# 3. Both operations happen simultaneously from a single bcftools stream\n# 4. Results available in S3 even while analysis continues\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Real-world use case:"})," Monitoring long-running analyses without waiting for completion:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Start a 24-hour variant calling run with real-time uploading\nsamtools mpileup -f ref.fa *.bam | \\\n  bcftools call -m -v 2>/tmp/variants.log | \\\n  tee >(gzip | aws s3 cp - s3://bucket/live-variants.vcf.gz) | \\\n  tail -100 | grep "PASS" > /tmp/latest_variants.txt\n\n# In another terminal, monitor progress:\nwatch -n 10 "wc -l /tmp/latest_variants.txt && aws s3 ls s3://bucket/live-variants.vcf.gz"\n'})}),"\n",(0,r.jsx)(n.h3,{id:"tips-for-reliable-downloadupload-pipes",children:"Tips for Reliable Download/Upload Pipes"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Scenario"}),(0,r.jsx)(n.th,{children:"Pipe Command"}),(0,r.jsx)(n.th,{children:"Notes"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Download + decompress"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"wget -O - | gunzip | process"})}),(0,r.jsxs)(n.td,{children:["Use ",(0,r.jsx)(n.code,{children:"-O -"})," to output to stdout"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Upload + compress"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"process | gzip | aws s3 cp -"})}),(0,r.jsxs)(n.td,{children:["Pipe ",(0,r.jsx)(n.code,{children:"-"})," for stdin/stdout"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"S3 download + analyze"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"aws s3 cp s3://... - | process"})}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"-"})," reads from stdin to stdout"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Tar extract streaming"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"wget -O - tar.gz | tar -xz -O file"})}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"-O"})," or ",(0,r.jsx)(n.code,{children:"--to-stdout"})," extracts to stdout"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Multiple S3 files"})}),(0,r.jsxs)(n.td,{children:["Use ",(0,r.jsx)(n.code,{children:"aws s3 ls"})," + ",(0,r.jsx)(n.code,{children:"parallel"})]}),(0,r.jsxs)(n.td,{children:["Chain with ",(0,r.jsx)(n.code,{children:"aws s3 cp s3://... -"})]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Real-time monitoring"})}),(0,r.jsxs)(n.td,{children:["Use ",(0,r.jsx)(n.code,{children:"tee"})," for branching"]}),(0,r.jsx)(n.td,{children:"Simultaneous upload and local processing"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Error handling"})}),(0,r.jsxs)(n.td,{children:["Check exit codes with ",(0,r.jsx)(n.code,{children:"set -o pipefail"})]}),(0,r.jsx)(n.td,{children:"S3 errors need to fail the pipeline"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"critical-considerations-for-cloud-pipes",children:"Critical Considerations for Cloud Pipes"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"1. Network vs. Disk Bottleneck"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# If network is slower than local processing:\n# Download once, process multiple times\nwget -O - file.tar.gz | tee local.tar.gz | tar -xz | process1\ntar -xz < local.tar.gz | process2\ntar -xz < local.tar.gz | process3\n# Tradeoff: Local storage vs. network bandwidth\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"2. Retry Logic for Failed S3 Operations"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Simple retry with exponential backoff\nretry_count=0\nwhile [ $retry_count -lt 3 ]; do\n  aws s3 cp s3://bucket/file - 2>/dev/null && break\n  retry_count=$((retry_count + 1))\n  sleep $((2 ** retry_count))\ndone | gunzip | process\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"3. Monitoring Upload Progress"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Use pv to monitor upload speed\nprocess_data | \\\n  pv -br | \\\n  gzip | \\\n  aws s3 cp - s3://bucket/file.gz\n\n# Output: [1.2GB/s] or similar\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"4. Handling Large Files with Multipart Upload"})}),"\n",(0,r.jsx)(n.p,{children:"For files larger than a few GB, AWS S3 multipart uploads are more reliable:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# aws s3 cp already handles multipart, but for custom control:\nprocess_data | \\\n  gzip | \\\n  aws s3 cp - s3://bucket/large-file.gz \\\n    --sse AES256 \\\n    --storage-class GLACIER  # Optional: cheaper storage class\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"real-world-bioinformatics-pipes",children:"Real-World Bioinformatics Pipes"}),"\n",(0,r.jsx)(n.h3,{id:"example-1-rna-seq-quality-control-pipeline",children:"Example 1: RNA-seq Quality Control Pipeline"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Process RNA-seq reads:\n# 1. Filter low quality\n# 2. Count valid reads\n# 3. Extract length distribution\n# 4. Detect adapters\n\nfastq_quality_filter -i reads.fastq -q 20 -p 80 | \\\n  tee >(wc -l | awk '{print \"Valid reads: \"$1/4}' > read_count.txt) | \\\n  tee >(awk 'NR%4==2 {print length}' | \\\n        sort -n | uniq -c > length_dist.txt) | \\\n  fastx_collapser -o collapsed.fastq | \\\n  fastx_clipper -a AGATCGGAAGAGC -o trimmed.fastq\n"})}),"\n",(0,r.jsx)(n.h3,{id:"example-2-variant-calling-pipeline",children:"Example 2: Variant Calling Pipeline"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Align reads, mark duplicates, and call variants in one pass\nbwa mem -t 8 reference.fa reads.fastq | \\\n  samtools view -b -S - | \\\n  samtools sort -o sorted.bam - && \\\n  samtools markdup sorted.bam marked.bam && \\\n  samtools index marked.bam && \\\n  bcftools mpileup -f reference.fa marked.bam | \\\n  bcftools call -mv -o variants.vcf\n\n# Memory: Constant (~1-2 GB for buffers)\n# Disk: Only intermediate files marked.bam (necessary for bcftools indexing)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"example-3-fasta-processing-with-decompression",children:"Example 3: FASTA Processing with Decompression"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Decompress, process, and recompress in a single pipeline\nzcat genome.fasta.gz | \\\n  awk '/^>/{if(NR>1)print prev_seq; seq=\"\"; prev_seq=$0; next} {seq=seq $0} END{print prev_seq; print seq}' | \\\n  gzip > processed.fasta.gz\n\n# No intermediate uncompressed files\n# Entire genome processed with minimal disk space\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"summary-why-pipes-matter-in-bioinformatics",children:"Summary: Why Pipes Matter in Bioinformatics"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Problem"}),(0,r.jsx)(n.th,{children:"Pipe Solution"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Intermediate files"})}),(0,r.jsx)(n.td,{children:"Stream directly between tools"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Disk space"})}),(0,r.jsx)(n.td,{children:"No temporary storage needed"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Memory usage"})}),(0,r.jsx)(n.td,{children:"Constant, independent of data size"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Processing speed"})}),(0,r.jsx)(n.td,{children:"Single sequential read of data"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Failure recovery"})}),(0,r.jsx)(n.td,{children:"Failures caught immediately"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Code readability"})}),(0,r.jsx)(n.td,{children:"Clear, linear data flow"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"1. Pipes Enable Streaming Data Processing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Data flows through memory buffers, not disk"}),"\n",(0,r.jsx)(n.li,{children:"Only ~64KB per pipe sits in RAM at any time"}),"\n",(0,r.jsx)(n.li,{children:"Processing 1GB or 1TB uses the same constant memory"}),"\n",(0,r.jsx)(n.li,{children:"This is the fundamental advantage over traditional file-based workflows"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"2. Kernel-Level Synchronization (Automatic)"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"No manual management needed"}),"\n",(0,r.jsx)(n.li,{children:"Backpressure prevents one slow command from overwhelming others"}),"\n",(0,r.jsx)(n.li,{children:"If a command fails, the entire pipeline fails cleanly"}),"\n",(0,r.jsxs)(n.li,{children:["Use ",(0,r.jsx)(n.code,{children:"set -o pipefail"})," in bash to ensure this behavior"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"3. I/O Reduction is Massive in Bioinformatics"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Traditional alignment: 100GB reads \u2192 500GB SAM \u2192 100GB BAM (1.1TB I/O)"}),"\n",(0,r.jsx)(n.li,{children:"Pipe alignment: 100GB reads \u2192 100GB BAM (200GB I/O total)"}),"\n",(0,r.jsx)(n.li,{children:"Savings: 5.5x reduction in disk I/O"}),"\n",(0,r.jsx)(n.li,{children:"Practical benefit: 2-5x faster execution on disk-bound operations"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"4. Pipes Work Seamlessly with Cloud Storage"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Download with ",(0,r.jsx)(n.code,{children:"wget -O -"})," or ",(0,r.jsx)(n.code,{children:"aws s3 cp - s3://..."})]}),"\n",(0,r.jsx)(n.li,{children:"Stream directly from S3 without local copies"}),"\n",(0,r.jsx)(n.li,{children:"Upload results on-the-fly during processing"}),"\n",(0,r.jsxs)(n.li,{children:["Combine with ",(0,r.jsx)(n.code,{children:"tee"})," for simultaneous processing and uploading"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"5. Production Workflows (Nextflow, Sarek) Use Pipes Extensively"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Real example: Sarek's ",(0,r.jsx)(n.code,{children:"BWA_MEM"})," process pipes ",(0,r.jsx)(n.code,{children:"bwa mem | samtools sort"})]}),"\n",(0,r.jsx)(n.li,{children:"This is the standard pattern for large-scale bioinformatics"}),"\n",(0,r.jsx)(n.li,{children:"Nextflow adds reliability and error handling on top"}),"\n",(0,r.jsx)(n.li,{children:"No need to choose between pipes and Nextflow\u2014they work together"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"6. Practical Patterns You'll Use"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Pattern"}),(0,r.jsx)(n.th,{children:"Use Case"}),(0,r.jsx)(n.th,{children:"Command"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Simple chain"})}),(0,r.jsx)(n.td,{children:"Sequential processing"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"cmd1 | cmd2 | cmd3"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Branching"})}),(0,r.jsx)(n.td,{children:"Multiple outputs from one input"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"cmd1 | tee >(cmd2) | cmd3"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Download + process"})}),(0,r.jsx)(n.td,{children:"Remote files without storage"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"wget -O - url | gunzip | process"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Upload + compress"})}),(0,r.jsx)(n.td,{children:"Direct to cloud storage"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"process | gzip | aws s3 cp -"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Error safety"})}),(0,r.jsx)(n.td,{children:"Catch failures in pipes"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"set -o pipefail"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Parallel processing"})}),(0,r.jsx)(n.td,{children:"Scale across cores/machines"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"input | parallel 'process'"})})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"7. When Pipes Are Most Valuable"})}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Use pipes for:"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Massive datasets (100GB+) where I/O is the bottleneck"}),"\n",(0,r.jsx)(n.li,{children:"Cloud storage workflows where local disk is expensive"}),"\n",(0,r.jsx)(n.li,{children:"Real-time monitoring of long-running analyses"}),"\n",(0,r.jsx)(n.li,{children:"Linear processing chains (one output \u2192 next input)"}),"\n",(0,r.jsx)(n.li,{children:"Development and rapid prototyping"}),"\n",(0,r.jsx)(n.li,{children:"Nextflow process scripts (internal tool chaining)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["\u274c ",(0,r.jsx)(n.strong,{children:"Avoid pipes for:"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Highly branching workflows (many different paths)"}),"\n",(0,r.jsx)(n.li,{children:"Complex error recovery (need to restart from middle)"}),"\n",(0,r.jsx)(n.li,{children:"Data that needs multiple passes through the same file"}),"\n",(0,r.jsx)(n.li,{children:"When you need to monitor intermediate results extensively"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"8. Master These Tools for Production Workflows"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Essential for pipe mastery:\nset -o pipefail              # Error handling\ntee                          # Branching\npv                           # Progress monitoring\nprocess substitution: >(cmd) # Streaming to files\nAWS S3 pipes: aws s3 cp - s3://... # Cloud integration\nGNU parallel                 # Scale across cores\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"final-thoughts",children:"Final Thoughts"}),"\n",(0,r.jsx)(n.p,{children:"Unix pipes are a cornerstone of efficient bioinformatics. They're not just a convenient syntax\u2014they're a fundamental architecture for processing data that would otherwise overwhelm available disk space and I/O capacity."}),"\n",(0,r.jsx)(n.p,{children:"The real power becomes apparent at scale:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Small datasets:"})," Pipes save time and code complexity"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Large datasets:"})," Pipes are often the only practical solution"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cloud workflows:"})," Pipes enable streaming to/from S3 without local copies"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Production pipelines:"})," Pipes are embedded in every major workflow (Nextflow, Snakemake, etc.)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"By understanding how pipes work under the hood\u2014how the kernel manages file descriptors, buffers data, and synchronizes backpressure\u2014you can write bioinformatics workflows that are not just faster, but fundamentally more efficient."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Start small:"})," Use pipes in your next single-command analysis. Progress to chaining tools. Eventually, you'll write entire bioinformatics workflows as elegant streaming pipelines\u2014just like Sarek does."]}),"\n",(0,r.jsx)(n.p,{children:"Your future self (and your disk quota administrator) will thank you."})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},29831(e,n,s){s.d(n,{A:()=>i});const i=s.p+"assets/images/intro-7aaa912f8e48c1bb524eacaf56d8a3cd.png"},28453(e,n,s){s.d(n,{R:()=>a,x:()=>l});var i=s(96540);const r={},t=i.createContext(r);function a(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(t.Provider,{value:n},e.children)}},45308(e){e.exports=JSON.parse('{"permalink":"/river-docs/blog/unix-pipes-bioinformatics-streaming-data","source":"@site/blog/2026-02/2026-02-10.md","title":"Unix Pipes in Bioinformatics: How Streaming Data Reduces Memory and Storage","description":"Unix pipes (|) are one of the most powerful yet underutilized features in bioinformatics. They allow you to chain multiple commands together, processing data in a streaming fashion that dramatically reduces memory usage and disk I/O. This post explores why pipes are essential for bioinformatics work and shows how they work under the hood.","date":"2026-02-10T00:00:00.000Z","tags":[{"inline":true,"label":"unix","permalink":"/river-docs/blog/tags/unix"},{"inline":true,"label":"pipes","permalink":"/river-docs/blog/tags/pipes"},{"inline":true,"label":"bioinformatics","permalink":"/river-docs/blog/tags/bioinformatics"},{"inline":true,"label":"streaming","permalink":"/river-docs/blog/tags/streaming"},{"inline":true,"label":"data-processing","permalink":"/river-docs/blog/tags/data-processing"},{"inline":true,"label":"performance","permalink":"/river-docs/blog/tags/performance"}],"readingTime":21.23,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"unix-pipes-bioinformatics-streaming-data","title":"Unix Pipes in Bioinformatics: How Streaming Data Reduces Memory and Storage","authors":["river"],"tags":["unix","pipes","bioinformatics","streaming","data-processing","performance"],"image":"./imgs/intro.png"},"unlisted":false,"prevItem":{"title":"How to Migrate from In-House Pipelines to Enterprise-Level Workflows: A Proven 3-Step Validation Framework","permalink":"/river-docs/blog/migrate-bash-nextflow-enterprise-pipeline"},"nextItem":{"title":"Containers in Bioinformatics: Community Tooling and Efficient Docker Building","permalink":"/river-docs/blog/bioinformatics-containers-build-efficient-docker"}}')}}]);