"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[6959],{4924(n){n.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"how-to-build-slurm-hpc-part-3","metadata":{"permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-3","source":"@site/blog/2026-01/2026-01-14.md","title":"Building a Slurm HPC Cluster (Part 3) - Administration and Best Practices","description":"In Part 1 and Part 2, we built a complete Slurm HPC cluster from a single node to a production-ready multi-node system. Now let\'s learn how to manage, maintain, and secure it effectively.","date":"2026-01-14T00:00:00.000Z","tags":[{"inline":true,"label":"slurm","permalink":"/river-docs/blog/tags/slurm"},{"inline":true,"label":"hpc","permalink":"/river-docs/blog/tags/hpc"},{"inline":true,"label":"administration","permalink":"/river-docs/blog/tags/administration"},{"inline":true,"label":"security","permalink":"/river-docs/blog/tags/security"},{"inline":true,"label":"best-practices","permalink":"/river-docs/blog/tags/best-practices"}],"readingTime":12.14,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"how-to-build-slurm-hpc-part-3","title":"Building a Slurm HPC Cluster (Part 3) - Administration and Best Practices","authors":["river"],"tags":["slurm","hpc","administration","security","best-practices"],"image":"./imgs/hpc_3.svg"},"unlisted":false,"nextItem":{"title":"Building a Slurm HPC Cluster (Part 2) - Scaling to Production with Ansible","permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-2"}},"content":"In [Part 1](/blog/how-to-build-slurm-hpc-part-1) and [Part 2](/blog/how-to-build-slurm-hpc-part-2), we built a complete Slurm HPC cluster from a single node to a production-ready multi-node system. Now let\'s learn how to manage, maintain, and secure it effectively.\\n\\nThis final post covers daily administration tasks, troubleshooting, security hardening, and integration with data processing frameworks.\\n\\n\x3c!--truncate--\x3e\\n\\n## Series Overview\\n\\n- **[Part 1](/blog/how-to-build-slurm-hpc-part-1)**: Introduction, Architecture, and Single Node Setup\\n- **[Part 2](/blog/how-to-build-slurm-hpc-part-2)**: Scaling to Production with Ansible\\n- **Part 3 (This Post)**: Administration and Best Practices\\n\\n## Administration Overview\\n\\nManaging a Slurm cluster involves several key areas:\\n\\n- **Cluster Management**: Build, maintain, and update the cluster via Ansible\\n- **User Management**: Synchronize users across nodes with proper permissions\\n- **Login Security**: Implement SSH hardening with 2FA or key pairs\\n- **Resource Management**: Enforce limits and fair-share policies\\n- **Monitoring**: Track performance and resource utilization\\n- **Troubleshooting**: Diagnose and resolve issues\\n\\n## User and Resource Management\\n\\n### Adding Users and Groups\\n\\nSlurm uses accounts (groups) to organize users and apply resource policies:\\n\\n```bash\\n# Add a new account/group\\nsacctmgr add account research_team Description=\\"Research Team\\"\\n\\n# Add a user to an account\\nsacctmgr add user john account=research_team\\n\\n# Add user with multiple accounts\\nsacctmgr add user alice account=research_team,dev_team DefaultAccount=research_team\\n\\n# View accounts\\nsacctmgr show account\\n\\n# View users\\nsacctmgr show user\\n```\\n\\n### Setting Resource Limits\\n\\n#### Account-Level Limits\\n\\nControl resources for entire groups:\\n\\n```bash\\n# Limit CPU minutes (prevents monopolizing cluster)\\nsacctmgr modify account research_team set GrpCPUMins=100000\\n\\n# Limit memory (in MB)\\nsacctmgr modify account research_team set GrpMem=500000\\n\\n# Limit concurrent jobs\\nsacctmgr modify account research_team set GrpJobs=50\\n\\n# Limit concurrent running jobs\\nsacctmgr modify account research_team set GrpJobsRun=20\\n\\n# Limit number of nodes\\nsacctmgr modify account research_team set GrpNodes=10\\n```\\n\\n#### User-Level Limits\\n\\nControl individual user behavior:\\n\\n```bash\\n# Limit jobs in queue\\nsacctmgr modify user john set MaxJobs=10\\n\\n# Limit running jobs\\nsacctmgr modify user john set MaxJobsRun=5\\n\\n# Limit wall time (in minutes)\\nsacctmgr modify user john set MaxWall=1440  # 24 hours\\n\\n# Limit CPUs per job\\nsacctmgr modify user john set MaxCPUs=32\\n\\n# View user limits\\nsacctmgr show user john withassoc format=user,account,maxjobs,maxsubmit,maxwall\\n```\\n\\n### Quality of Service (QoS)\\n\\nQoS allows you to create service tiers with different priorities:\\n\\n```bash\\n# Create QoS levels\\nsacctmgr add qos normal priority=100\\nsacctmgr add qos high priority=500 MaxWall=2-00:00:00 MaxJobs=5\\nsacctmgr add qos low priority=50\\n\\n# Assign QoS to account\\nsacctmgr modify account research_team set qos=normal,high\\n\\n# Users can specify QoS when submitting\\nsbatch --qos=high job_script.sh\\n```\\n\\n### Fair-Share Scheduling\\n\\nEnsure equitable resource distribution:\\n\\n```bash\\n# Set fair-share values (higher = more priority)\\nsacctmgr modify account research_team set fairshare=100\\nsacctmgr modify account dev_team set fairshare=50\\n\\n# View fair-share tree\\nsshare -a\\n\\n# View detailed fair-share info\\nsshare -A research_team --all\\n```\\n\\n## Node Management\\n\\n### Checking Node Status\\n\\n```bash\\n# View all nodes\\nsinfo\\n\\n# Detailed node information\\nsinfo -Nel\\n\\n# Show node states\\nsinfo -N -o \\"%N %T %C %m %e %f\\"\\n\\n# View specific node details\\nscontrol show node worker-01\\n```\\n\\n<figure markdown=\\"span\\">\\n    ![Job sinfo](./imgs/job_sinfo.png)\\n</figure>\\n\\nNode states you\'ll encounter:\\n- **IDLE**: Available for jobs\\n- **ALLOCATED**: Running jobs\\n- **MIXED**: Partially allocated\\n- **DRAIN**: Won\'t accept new jobs (draining)\\n- **DRAINED**: Fully drained\\n- **DOWN**: Not responding\\n\\n### Node Maintenance\\n\\n#### Draining a Node\\n\\nWhen you need to perform maintenance:\\n\\n```bash\\n# Drain node (won\'t accept new jobs, allows running jobs to finish)\\nscontrol update NodeName=worker-01 State=drain Reason=\\"Hardware upgrade\\"\\n\\n# Force drain (terminate running jobs)\\nscontrol update NodeName=worker-01 State=drain Reason=\\"Emergency maintenance\\"\\n\\n# Check drain reason\\nsinfo -R\\n```\\n\\n#### Resuming a Node\\n\\nAfter maintenance:\\n\\n```bash\\n# Resume node\\nscontrol update NodeName=worker-01 State=resume\\n\\n# Verify it\'s back\\nsinfo -n worker-01\\n```\\n\\n#### Forcing Node Down\\n\\nIf a node is misbehaving:\\n\\n```bash\\n# Mark node as down\\nscontrol update NodeName=worker-01 State=down Reason=\\"Hardware failure\\"\\n\\n# When fixed, resume\\nscontrol update NodeName=worker-01 State=resume\\n```\\n\\n### Adding New Compute Nodes\\n\\n1. Update Ansible inventory (`inventories/hosts`):\\n\\n```ini\\n[slurm_worker]\\nworker-01 ansible_host=192.168.58.11\\nworker-02 ansible_host=192.168.58.12\\nworker-03 ansible_host=192.168.58.13  # NEW\\n```\\n\\n2. Run Ansible playbook:\\n\\n```bash\\nansible-playbook -i inventories/hosts river_cluster.yml --limit worker-03\\n```\\n\\n3. Update `slurm.conf` on controller and all nodes (Ansible handles this)\\n\\n4. Restart slurmctld:\\n\\n```bash\\nsudo systemctl restart slurmctld\\n```\\n\\n5. Verify the new node:\\n\\n```bash\\nsinfo\\nscontrol show node worker-03\\n```\\n\\n## Monitoring and Troubleshooting\\n\\n### Checking Slurm Logs\\n\\nLogs are essential for diagnosing issues:\\n\\n```bash\\n# Controller logs\\nsudo tail -f /var/log/slurm/slurmctld.log\\n\\n# Worker node logs (on compute nodes)\\nsudo tail -f /var/log/slurm/slurmd.log\\n\\n# Database logs\\nsudo tail -f /var/log/slurm/slurmdbd.log\\n\\n# Filter for errors\\nsudo grep \\"error\\" /var/log/slurm/*.log\\n\\n# Filter for specific node\\nsudo grep \\"worker-01\\" /var/log/slurm/slurmctld.log\\n\\n# Last 100 lines with context\\nsudo tail -100 /var/log/slurm/slurmctld.log\\n```\\n\\n### Common Issues and Solutions\\n\\n#### Issue: Node Shows as DOWN\\n\\n**Diagnosis:**\\n```bash\\nsinfo\\n# OUTPUT: worker-01    down   ...\\n\\nscontrol show node worker-01\\n# Check \\"Reason\\" field\\n```\\n\\n**Solutions:**\\n```bash\\n# 1. Check if slurmd is running\\nssh worker-01 \\"sudo systemctl status slurmd\\"\\n\\n# 2. Restart slurmd\\nssh worker-01 \\"sudo systemctl restart slurmd\\"\\n\\n# 3. Check network connectivity\\nping worker-01\\n\\n# 4. Check logs\\nssh worker-01 \\"sudo tail -50 /var/log/slurm/slurmd.log\\"\\n\\n# 5. Resume the node\\nscontrol update NodeName=worker-01 State=resume\\n```\\n\\n#### Issue: Jobs Stuck in Pending\\n\\n**Diagnosis:**\\n```bash\\nsqueue\\n# See jobs in PD (pending) state\\n\\n# Check why job is pending\\nsqueue --start -j JOB_ID\\n\\n# View detailed job info\\nscontrol show job JOB_ID\\n```\\n\\n**Common reasons:**\\n- `Resources`: Not enough resources available\\n- `Priority`: Lower priority than other jobs\\n- `Dependency`: Waiting for another job to complete\\n- `QOSMaxJobsPerUser`: User has too many jobs running\\n\\n**Solutions:**\\n```bash\\n# 1. Check available resources\\nsinfo -o \\"%P %a %l %D %N %C\\"\\n\\n# 2. View job requirements\\nscontrol show job JOB_ID | grep -E \\"Partition|NumNodes|MinMemory\\"\\n\\n# 3. Cancel job if needed\\nscancel JOB_ID\\n\\n# 4. Modify pending job\\nscontrol update JobId=JOB_ID NumNodes=1\\n```\\n\\n#### Issue: Jobs Failing Immediately\\n\\n**Diagnosis:**\\n```bash\\n# Check job status\\nsacct -j JOB_ID\\n\\n# View job output files\\ncat slurm-JOB_ID.out\\ncat slurm-JOB_ID.err\\n```\\n\\n**Common causes:**\\n- Script errors (check shebang line)\\n- Missing executables\\n- Resource limits exceeded\\n- Permission issues\\n\\n#### Issue: Accounting Database Not Working\\n\\n**Diagnosis:**\\n```bash\\n# Check slurmdbd status\\nsudo systemctl status slurmdbd\\n\\n# Test database connection\\nsudo mysql -u slurm -p slurm_acct_db -e \\"SHOW TABLES;\\"\\n\\n# Check slurmdbd logs\\nsudo tail -50 /var/log/slurm/slurmdbd.log\\n```\\n\\n**Solutions:**\\n```bash\\n# 1. Restart slurmdbd\\nsudo systemctl restart slurmdbd\\n\\n# 2. Verify database credentials in slurmdbd.conf\\nsudo cat /etc/slurm-llnl/slurmdbd.conf\\n\\n# 3. Check database permissions\\nsudo mysql -e \\"SHOW GRANTS FOR \'slurm\'@\'localhost\';\\"\\n\\n# 4. Restart slurmctld to reconnect\\nsudo systemctl restart slurmctld\\n```\\n\\n### System Logs with rsyslog\\n\\nOur Ansible setup configures centralized logging:\\n\\n```bash\\n# On controller (rsyslog server)\\nsudo tail -f /var/log/syslog\\n\\n# Filter by hostname\\nsudo grep \\"worker-01\\" /var/log/syslog\\n\\n# Filter by service\\nsudo grep \\"slurmd\\" /var/log/syslog\\n\\n# Check authentication logs\\nsudo tail -f /var/log/auth.log\\n```\\n\\n## Security Best Practices\\n\\n### SSH Hardening\\n\\n:::warning\\nSecure your login nodes! HPC clusters are attractive targets for attackers.\\n:::\\n\\nFor detailed SSH security setup, see our [SSH Remote Server documentation](/docs/resources/administration/ssh-remote-server).\\n\\nKey recommendations:\\n\\n1. **Disable Password Authentication**:\\n```bash\\n# /etc/ssh/sshd_config\\nPasswordAuthentication no\\nPubkeyAuthentication yes\\n```\\n\\n2. **Implement 2FA** with Google Authenticator or similar\\n\\n3. **Use SSH Key Pairs**:\\n```bash\\n# Generate key on your machine\\nssh-keygen -t ed25519 -C \\"your_email@example.com\\"\\n\\n# Copy to cluster\\nssh-copy-id user@controller-node\\n```\\n\\n4. **Limit SSH Access**:\\n```bash\\n# /etc/ssh/sshd_config\\nAllowUsers alice bob charlie\\nAllowGroups cluster_users\\n\\n# Or deny specific users\\nDenyUsers baduser\\n```\\n\\n5. **Change Default Port** (security through obscurity):\\n```bash\\n# /etc/ssh/sshd_config\\nPort 2222\\n```\\n\\n### Munge Authentication\\n\\nMunge provides authentication between Slurm components:\\n\\n```bash\\n# Verify munge is running\\nsudo systemctl status munge\\n\\n# Test munge\\nmunge -n | unmunge\\n\\n# Generate new key (do this on controller, then distribute)\\nsudo /usr/sbin/create-munge-key\\n\\n# Copy key to all nodes (Ansible does this automatically)\\nsudo scp /etc/munge/munge.key worker-01:/etc/munge/\\n\\n# Restart munge on all nodes\\nsudo systemctl restart munge\\n```\\n\\n:::warning\\n**Critical**: The munge key must be identical on all nodes and have proper permissions (0400, owned by munge:munge).\\n:::\\n\\n### Docker Security\\n\\n:::danger\\n**Critical Security Issue**: Users in the `docker` group can gain root privileges!\\n\\n```bash\\n# DON\'T DO THIS (unless they\'re admins)\\nsudo usermod -aG docker regular_user\\n```\\n\\nWhy? Because they can run:\\n```bash\\ndocker run -v /:/hostfs --privileged -it ubuntu bash\\n# Now they have root access to the host filesystem!\\n```\\n:::\\n\\n**Solutions**:\\n\\n1. **Use Docker Rootless Mode**:\\n```bash\\n# Install rootless docker\\ncurl -fsSL https://get.docker.com/rootless | sh\\n```\\n\\n2. **Use Apptainer/Singularity** (designed for HPC):\\n```bash\\n# Install Apptainer\\nsudo apt-get install apptainer\\n\\n# Run containers without root\\napptainer run docker://ubuntu:latest\\n```\\n\\n3. **Restrict Docker Group**: Only add administrators to docker group\\n\\n### Firewall Configuration\\n\\nRestrict access to Slurm ports:\\n\\n```bash\\n# Allow Slurm ports only from cluster network\\nsudo ufw allow from 192.168.58.0/24 to any port 6817  # slurmctld\\nsudo ufw allow from 192.168.58.0/24 to any port 6818  # slurmd\\nsudo ufw allow from 192.168.58.0/24 to any port 6819  # slurmdbd\\n\\n# Allow SSH from anywhere\\nsudo ufw allow 22/tcp\\n\\n# Enable firewall\\nsudo ufw enable\\n```\\n\\n## Shared Storage Best Practices\\n\\n### NFS Performance Tuning\\n\\nOptimize NFS for your workload:\\n\\n```bash\\n# /etc/fstab on compute nodes\\ncontroller-01:/home /home nfs4 rw,soft,rsize=262144,wsize=262144,timeo=14,intr 0 0\\n```\\n\\nParameters explained:\\n- `soft`: Timeout after retry (vs `hard` which waits forever)\\n- `rsize/wsize`: Read/write buffer size (larger = better performance)\\n- `timeo`: Timeout value\\n- `intr`: Allow interrupts\\n\\n### Storage Layout\\n\\nRecommended directory structure:\\n\\n```\\n/home/          # User home directories (SSD/NVMe)\\n  \u251c\u2500 alice/\\n  \u251c\u2500 bob/\\n  \u2514\u2500 charlie/\\n\\n/mnt/data/      # Large datasets (HDD or object storage)\\n  \u251c\u2500 shared/    # Common datasets\\n  \u251c\u2500 projects/  # Project-specific data\\n  \u2514\u2500 scratch/   # Temporary data (auto-cleanup)\\n\\n/opt/           # Shared software/modules\\n  \u251c\u2500 anaconda/\\n  \u251c\u2500 modules/\\n  \u2514\u2500 apps/\\n```\\n\\n### Quotas\\n\\nPrevent users from filling up shared storage:\\n\\n```bash\\n# Set user quotas\\nsudo setquota -u alice 50G 60G 0 0 /home\\nsudo setquota -u alice 500G 550G 0 0 /mnt/data\\n\\n# Check quotas\\nquota -u alice\\n\\n# View all quotas\\nsudo repquota -a\\n```\\n\\n## Integration with Data Processing Frameworks\\n\\nOne of Slurm\'s greatest strengths is integration with modern computing frameworks:\\n\\n### Apache Spark\\n\\nSubmit Spark jobs to Slurm:\\n\\n```bash\\n#!/bin/bash\\n#SBATCH --job-name=spark-job\\n#SBATCH --nodes=4\\n#SBATCH --ntasks-per-node=1\\n#SBATCH --cpus-per-task=8\\n#SBATCH --mem=32G\\n\\n# Load Spark module\\nmodule load spark/3.5.0\\n\\n# Run Spark application\\nspark-submit \\\\\\n  --master yarn \\\\\\n  --num-executors 4 \\\\\\n  --executor-cores 8 \\\\\\n  --executor-memory 28G \\\\\\n  my_spark_app.py\\n```\\n\\n### Ray (Distributed ML)\\n\\n```bash\\n#!/bin/bash\\n#SBATCH --job-name=ray-job\\n#SBATCH --nodes=2\\n#SBATCH --ntasks-per-node=1\\n#SBATCH --cpus-per-task=16\\n#SBATCH --gpus-per-node=2\\n\\n# Start Ray cluster\\nray start --head --port=6379\\nsrun --nodes=1 --ntasks=1 ray start --address=$HEAD_NODE:6379\\n\\n# Run Ray application\\npython ray_train.py\\n```\\n\\n### Dask\\n\\n```python\\nfrom dask_jobqueue import SLURMCluster\\nfrom dask.distributed import Client\\n\\ncluster = SLURMCluster(\\n    cores=8,\\n    memory=\\"16GB\\",\\n    processes=2,\\n    walltime=\\"02:00:00\\",\\n    queue=\\"compute\\"\\n)\\n\\ncluster.scale(jobs=10)  # Request 10 jobs\\nclient = Client(cluster)\\n\\n# Your Dask code here\\n```\\n\\n### Nextflow (Bioinformatics)\\n\\n```groovy\\n// nextflow.config\\nprocess {\\n    executor = \'slurm\'\\n    queue = \'compute\'\\n    memory = \'8 GB\'\\n    time = \'2h\'\\n}\\n```\\n\\nRun with:\\n```bash\\nnextflow run nf-core/rnaseq -profile slurm\\n```\\n\\n## Maintenance Tasks\\n\\n### Regular Updates\\n\\n```bash\\n# Update cluster via Ansible\\nansible-playbook -i inventories/hosts river_cluster.yml --tags update\\n\\n# Update specific nodes\\nansible-playbook -i inventories/hosts river_cluster.yml --limit worker-01,worker-02\\n```\\n\\n### Backup Critical Data\\n\\n```bash\\n# Backup Slurm configuration\\nsudo cp /etc/slurm-llnl/slurm.conf /backup/slurm.conf.$(date +%Y%m%d)\\n\\n# Backup accounting database\\nsudo mysqldump -u slurm -p slurm_acct_db > slurm_acct_backup_$(date +%Y%m%d).sql\\n\\n# Backup user data (use rsync for efficiency)\\nsudo rsync -av /home/ /backup/home/\\n```\\n\\n### Monitoring Disk Space\\n\\n```bash\\n# Check disk usage on all nodes\\nansible all -i inventories/hosts -m shell -a \\"df -h\\"\\n\\n# Check specific directory\\nansible all -i inventories/hosts -m shell -a \\"du -sh /var/log/slurm\\"\\n\\n# Find large files\\nfind /home -type f -size +1G -exec ls -lh {} \\\\;\\n```\\n\\n## Performance Optimization Tips\\n\\n### 1. Tune Scheduler Parameters\\n\\n```bash\\n# /etc/slurm-llnl/slurm.conf\\n\\n# Increase scheduling frequency\\nSchedulerTimeSlice=30\\n\\n# Prioritize recent submitters less\\nPriorityWeightAge=1000\\nPriorityWeightFairshare=10000\\n\\n# Enable backfill scheduling with larger window\\nSchedulerType=sched/backfill\\nbf_window=1440  # 24 hours\\n```\\n\\n### 2. Optimize Job Packing\\n\\n```bash\\n# Use CR_CPU for CPU-bound jobs\\nSelectType=select/cons_tres\\nSelectTypeParameters=CR_CPU\\n\\n# Or CR_Memory for memory-bound jobs\\nSelectTypeParameters=CR_Memory\\n\\n# Or CR_Core for mixed workloads\\nSelectTypeParameters=CR_Core\\n```\\n\\n### 3. Create Multiple Partitions\\n\\n```bash\\n# /etc/slurm-llnl/slurm.conf\\n\\n# Fast partition for short jobs\\nPartitionName=quick Nodes=worker-[01-02] Default=NO MaxTime=01:00:00 State=UP Priority=100\\n\\n# Standard partition\\nPartitionName=standard Nodes=worker-[01-04] Default=YES MaxTime=2-00:00:00 State=UP Priority=50\\n\\n# Long partition for extended jobs\\nPartitionName=long Nodes=worker-[03-04] Default=NO MaxTime=7-00:00:00 State=UP Priority=25\\n\\n# GPU partition\\nPartitionName=gpu Nodes=gpu-[01-02] Default=NO MaxTime=1-00:00:00 State=UP Priority=75\\n```\\n\\n### 4. Enable Job Arrays for Batch Processing\\n\\n```bash\\n#!/bin/bash\\n#SBATCH --array=1-100%10  # 100 tasks, max 10 concurrent\\n\\n# Process task based on array index\\npython process.py --input data_${SLURM_ARRAY_TASK_ID}.txt\\n```\\n\\n## Conclusion\\n\\nCongratulations! You now have the knowledge to build, deploy, and manage a production Slurm HPC cluster. Let\'s recap the journey:\\n\\n### Part 1: Foundations\\n- Understanding Slurm architecture\\n- Single-node setup for learning\\n- Critical cgroup configuration\\n- Job accounting basics\\n\\n### Part 2: Production Deployment\\n- Ansible automation\\n- Multi-node cluster setup\\n- Monitoring with Grafana\\n- Slack alerting\\n\\n### Part 3: Administration (This Post)\\n- User and resource management\\n- Node maintenance and troubleshooting\\n- Security hardening\\n- Performance optimization\\n- Framework integration\\n\\n## Key Takeaways\\n\\n1. **Start Simple, Scale Smart**: Master single-node before going multi-node\\n2. **Automate Everything**: Use Ansible for reproducible deployments\\n3. **Monitor Proactively**: Set up alerting before problems occur\\n4. **Security First**: SSH hardening, proper permissions, Docker caution\\n5. **Regular Maintenance**: Backups, updates, and log monitoring\\n6. **Documentation**: Document your cluster configuration and procedures\\n\\n## What\'s Next?\\n\\nConsider these advanced topics:\\n\\n- **High Availability**: Redundant controllers with failover\\n- **LDAP Integration**: Centralized authentication for large organizations\\n- **GPU Scheduling**: Optimize for machine learning workloads\\n- **Cloud Bursting**: Expand to cloud resources during peak demand\\n- **Custom Plugins**: Extend Slurm with custom scheduling policies\\n\\n## Resources\\n\\n- **Part 1**: [Single Node Setup](/blog/how-to-build-slurm-hpc-part-1)\\n- **Part 2**: [Production Deployment](/blog/how-to-build-slurm-hpc-part-2)\\n- **Documentation**: [Administration Guide](/docs/resources/high-performance-computing/how-to-build-slurm-scalable-using-ansible/administration)\\n- **GitHub**: [RiverXData Slurm Ansible](https://github.com/riverxdata/river-slurm)\\n- **Official Docs**: [SchedMD Slurm Documentation](https://slurm.schedmd.com/)\\n\\n## Contact\\n\\nHave questions or need help with your cluster? Reach out at: nttg8100@gmail.com\\n\\n---\\n\\n*This concludes the RiverXData series on building Slurm HPC clusters. Thank you for following along! We hope this guide helps you build and manage effective HPC infrastructure.*"},{"id":"how-to-build-slurm-hpc-part-2","metadata":{"permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-2","source":"@site/blog/2026-01/2026-01-12.md","title":"Building a Slurm HPC Cluster (Part 2) - Scaling to Production with Ansible","description":"In Part 1, we learned the fundamentals by building a single-node Slurm cluster. Now it\'s time to scale up to a production-ready, multi-node cluster with automated deployment, monitoring, and alerting.","date":"2026-01-12T00:00:00.000Z","tags":[{"inline":true,"label":"slurm","permalink":"/river-docs/blog/tags/slurm"},{"inline":true,"label":"hpc","permalink":"/river-docs/blog/tags/hpc"},{"inline":true,"label":"ansible","permalink":"/river-docs/blog/tags/ansible"},{"inline":true,"label":"automation","permalink":"/river-docs/blog/tags/automation"},{"inline":true,"label":"devops","permalink":"/river-docs/blog/tags/devops"}],"readingTime":8.87,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"how-to-build-slurm-hpc-part-2","title":"Building a Slurm HPC Cluster (Part 2) - Scaling to Production with Ansible","authors":["river"],"tags":["slurm","hpc","ansible","automation","devops"],"image":"./imgs/hpc_2.svg"},"unlisted":false,"prevItem":{"title":"Building a Slurm HPC Cluster (Part 3) - Administration and Best Practices","permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-3"},"nextItem":{"title":"Building a Slurm HPC Cluster (Part 1) - Single Node Setup and Fundamentals","permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-1"}},"content":"In [Part 1](/blog/how-to-build-slurm-hpc-part-1), we learned the fundamentals by building a single-node Slurm cluster. Now it\'s time to scale up to a production-ready, multi-node cluster with automated deployment, monitoring, and alerting.\\n\\nIn this post, we\'ll use Ansible to automate the entire deployment process, making it reproducible and maintainable.\\n\\n\x3c!--truncate--\x3e\\n\\n## Series Overview\\n\\n- **[Part 1](/blog/how-to-build-slurm-hpc-part-1)**: Introduction, Architecture, and Single Node Setup\\n- **Part 2 (This Post)**: Scaling to Production with Ansible\\n- **[Part 3](/blog/how-to-build-slurm-hpc-part-3)**: Administration and Best Practices\\n\\n## Why Ansible for HPC Clusters?\\n\\nMoving from a single-node setup to a multi-node production cluster involves:\\n\\n- Configuring multiple machines identically\\n- Managing dependencies and installation order\\n- Keeping configurations synchronized\\n- Handling user management across nodes\\n- Setting up monitoring and logging infrastructure\\n\\nDoing this manually is error-prone and time-consuming. Infrastructure automation tools like Ansible, Puppet, or Terraform solve this problem. We chose **Ansible** because:\\n\\n- **Agentless**: No software to install on managed nodes\\n- **Declarative**: Describe the desired state, not the steps\\n- **Idempotent**: Safe to run multiple times\\n- **YAML-based**: Easy to read and version control\\n- **Large ecosystem**: Many pre-built roles available\\n\\n### What is Ansible? (Quick Primer)\\n\\nIf you\'re new to Ansible, watch this excellent 100-second introduction:\\n\\n<div style={{ position: \\"relative\\", paddingBottom: \\"56.25%\\", height: 0, overflow: \\"hidden\\", maxWidth: \\"100%\\", background: \\"#000\\" }}>\\n  <iframe \\n    src=\\"https://www.youtube.com/embed/xRMPKQweySE\\" \\n    frameBorder=\\"0\\" \\n    allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" \\n    allowFullScreen\\n    style={{ position: \\"absolute\\", top: 0, left: 0, width: \\"100%\\", height: \\"100%\\" }}\\n  />\\n</div>\\n\\n## Production Cluster Architecture\\n\\nOur production setup includes these components:\\n\\n<figure markdown=\\"span\\">\\n    ![Small HPC Cluster](./imgs/small_HPC.jpg)\\n</figure>\\nSource: https://cs.phenikaa-uni.edu.vn/vi/post/gioi-thieu/co-so-vat-chat/he-thong-tinh-toan-hieu-nang-cao-phenikaa-hpc\\n\\n### Head Node (Controller + Login)\\n\\nThe head node manages the cluster and provides user access:\\n\\n- **slurmctld**: Job scheduling and resource management\\n- **slurmdbd**: Accounting database\\n- **NFS Server**: Shares directories to compute nodes\\n- **Prometheus**: Metrics collection\\n- **Grafana**: Monitoring dashboards\\n- **Alertmanager**: Slack notifications\\n\\n### Compute Nodes\\n\\nWorker nodes that execute jobs:\\n\\n- **slurmd**: Job execution daemon\\n- **NFS Client**: Mounts shared storage\\n- **Node Exporter**: Exposes system metrics\\n- **Slurm Exporter**: Exposes Slurm-specific metrics\\n\\n### Shared Storage (NFS)\\n\\nNFS provides unified file system access across all nodes:\\n\\n<figure markdown=\\"span\\">\\n    ![NFS Architecture](./imgs/NFS.png)\\n</figure>\\nSource: https://thuanbui.me/cai-dat-nfs-server-va-nfs-client-tren-ubuntu-22-04/\\n\\n- `/home`: User home directories (fast SSD/NVMe storage)\\n- `/mnt/data`: Large datasets (high-capacity HDD)\\n\\n### Monitoring Stack\\n\\nComplete observability for your cluster:\\n\\n<figure markdown=\\"span\\">\\n    ![Grafana Dashboard](./imgs/grafana.png)\\n</figure>\\nSource: https://swsmith.cc/posts/grafana-slurm.html\\n\\n- **Prometheus**: Time-series metrics database\\n- **Grafana**: Beautiful dashboards for visualization\\n- **Alertmanager**: Sends alerts to Slack when issues occur\\n- **Node Exporter**: System-level metrics (CPU, memory, disk)\\n- **Slurm Exporter**: Slurm-specific metrics (jobs, partitions, nodes)\\n\\n## Setting Up the RiverXData Slurm Cluster\\n\\nWe\'ve created a comprehensive Ansible playbook that automates everything. Let\'s get started!\\n\\n<figure markdown=\\"span\\">\\n    ![Slurm Deployment Architecture](./imgs/slurm.svg)\\n</figure>\\n\\n### Prerequisites\\n\\n- Multiple Ubuntu 20.04 or 24.04 machines (or VMs)\\n- SSH access to all nodes\\n- Sudo privileges on all nodes\\n- A Slack workspace (for alerts)\\n\\n### Step 1: Clone the Repository\\n\\n```bash\\ngit clone https://github.com/riverxdata/river-slurm.git -b 1.0.0\\ncd river-slurm\\n```\\n\\n### Step 2: Install Ansible and Dependencies\\n\\n```bash\\n# Ubuntu 24.04, without Vagrant\\nbash scripts/setup.sh 24.04 false\\n\\n# For Ubuntu 20.04\\nbash scripts/setup.sh 20.04 false\\n\\n# For developers: Install with Vagrant support\\nbash scripts/setup.sh 24.04 true\\n```\\n\\nThis script installs:\\n- Ansible and required Python packages\\n- Community Ansible collections\\n- Galaxy roles (geerlingguy.docker, etc.)\\n\\n### Step 3: Set Up Slack Alerts\\n\\n:::info\\nIn production environments, even small teams benefit from proactive monitoring. Slack is perfect for this - you\'ll get notifications when nodes go down, jobs fail, or resources run low.\\n:::\\n\\n#### Create a Slack App\\n\\n1. Go to [Slack API](https://api.slack.com/apps) and create a new app\\n2. Choose \\"From scratch\\"\\n3. Name it (e.g., \\"Slurm Cluster Monitor\\")\\n4. Select your workspace\\n\\n<figure markdown=\\"span\\">\\n    ![Create Slack App](./imgs/create_app.png)\\n</figure>\\n\\n#### Enable Incoming Webhooks\\n\\n1. Navigate to \\"Incoming Webhooks\\" in your app settings\\n2. Activate incoming webhooks\\n3. Click \\"Add New Webhook to Workspace\\"\\n4. Select the channel for notifications (e.g., `#cluster-alerts`)\\n5. Copy the webhook URL\\n\\n<figure markdown=\\"span\\">\\n    ![Slack Config Step 1](./imgs/config_1.png)\\n</figure>\\n\\n<figure markdown=\\"span\\">\\n    ![Slack Config Step 2](./imgs/config_2.png)\\n</figure>\\n\\n<figure markdown=\\"span\\">\\n    ![Slack Config Step 3](./imgs/config_3.png)\\n</figure>\\n\\n#### Test Your Webhook\\n\\n```bash\\ncurl -X POST -H \'Content-type: application/json\' \\\\\\n  --data \'{\\"text\\":\\"Hello from Slurm cluster!\\"}\' \\\\\\n  https://hooks.slack.com/services/YOUR/WEBHOOK/URL\\n```\\n\\nYou should see the message appear in your Slack channel!\\n\\n### Step 4: Configure Your Inventory\\n\\nCreate `inventories/hosts` (or copy from `inventories/hosts.example`):\\n\\n```ini\\n[slurm_master]\\ncontroller-01 ansible_host=192.168.58.10\\n\\n[slurm_worker]\\nworker-01 ansible_host=192.168.58.11\\nworker-02 ansible_host=192.168.58.12\\n\\n[slurm:children]\\nslurm_master\\nslurm_worker\\n\\n[all:vars]\\nansible_user=your_username\\nslurm_password=secure_munge_password\\nslurm_account_db_pass=secure_db_password\\nslack_api_url=https://hooks.slack.com/services/YOUR/WEBHOOK/URL\\nslack_channel=#cluster-alerts\\nadmin_user=admin\\nadmin_password=secure_grafana_password\\n```\\n\\n:::warning\\n**Security Best Practice**: Use [Ansible Vault](https://docs.ansible.com/ansible/latest/user_guide/vault.html) to encrypt sensitive variables like passwords and API keys.\\n\\n```bash\\n# Create encrypted vault\\nansible-vault create inventories/vault.yml\\n\\n# Or encrypt existing file\\nansible-vault encrypt inventories/hosts\\n```\\n:::\\n\\n#### Optional Parameters\\n\\n```ini\\ndefault_password=temporary_user_password  # Forces change on first login\\nusers=alice,bob,charlie                   # Comma-separated list\\n```\\n\\n### Step 5: Deploy the Cluster\\n\\nNow for the magic moment - deploy your entire cluster with one command!\\n\\n```bash\\n# If you have passwordless sudo configured\\nansible-playbook -i inventories/hosts river_cluster.yml\\n\\n# If you need to enter sudo password\\nansible-playbook -i inventories/hosts river_cluster.yml --ask-become-pass\\n```\\n\\nWhat this playbook does:\\n\\n1. **Prepares all nodes**:\\n   - Updates packages\\n   - Installs dependencies\\n   - Configures firewalls\\n\\n2. **Sets up the controller**:\\n   - Installs slurmctld and slurmdbd\\n   - Configures MariaDB for accounting\\n   - Sets up NFS server\\n   - Installs monitoring stack\\n\\n3. **Configures compute nodes**:\\n   - Installs slurmd\\n   - Mounts NFS shares\\n   - Configures metrics exporters\\n\\n4. **Deploys monitoring**:\\n   - Prometheus for metrics collection\\n   - Grafana with pre-configured dashboards\\n   - Alertmanager with Slack integration\\n\\n5. **Synchronizes configurations**:\\n   - Copies slurm.conf to all nodes\\n   - Sets up Munge authentication\\n   - Configures log aggregation\\n\\n### Step 6: Add Users\\n\\n```bash\\nansible-playbook -i inventories/hosts river_users.yml\\n```\\n\\nThis creates Linux users on all nodes with:\\n- Synchronized UID/GID across nodes\\n- Home directories on shared NFS\\n- Slurm accounting associations\\n\\n:::info\\n**Note on User Management**: For production, consider integrating with LDAP or Active Directory. However, NIS and LDAP setup can be complex on Ubuntu. Our Ansible approach provides a simpler alternative that works well for small to medium clusters.\\n:::\\n\\n### Step 7: Verify the Setup\\n\\nSSH into the controller node and run:\\n\\n```bash\\n# Check cluster status\\nsinfo\\n\\n# Expected output:\\n# PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\\n# compute*     up   infinite      2   idle worker-01,worker-02\\n\\n# View job queue\\nsqueue\\n\\n# Submit a test job\\nsrun --nodes=1 --ntasks=1 hostname\\n\\n# Check accounting\\nsacct\\n\\n# View cluster configuration\\nscontrol show config | head -20\\n```\\n\\n### Step 8: Access Grafana Dashboards\\n\\nGrafana runs on the controller node at port 3000. To access it securely from your local machine:\\n\\n```bash\\n# Create SSH tunnel\\nssh -N -L 3001:localhost:3000 your_user@controller_ip\\n\\n# Now open in browser: http://localhost:3001\\n# Login: admin / your_grafana_password\\n```\\n\\nYou\'ll see pre-configured dashboards showing:\\n\\n#### Node Metrics Dashboard\\n- CPU usage per node\\n- Memory utilization\\n- Disk I/O\\n- Network traffic\\n- System load\\n\\n<figure markdown=\\"span\\">\\n    ![Node Grafana Dashboard](./imgs/node_grafana.png)\\n</figure>\\n\\n#### Slurm Metrics Dashboard\\n- Active jobs\\n- Job queue length\\n- Node states (idle, allocated, down)\\n- CPU allocation\\n- Memory usage\\n- Job completion rates\\n\\n<figure markdown=\\"span\\">\\n    ![Slurm Grafana Dashboard](./imgs/slurm_grafana.png)\\n</figure>\\n\\n### What About Alerts?\\n\\nAlertmanager is configured to send Slack notifications for:\\n\\n- **Node down**: When a compute node becomes unresponsive\\n- **Node resumed**: When a node comes back online\\n- **High CPU usage**: Sustained high CPU across cluster\\n- **High memory usage**: Memory pressure warnings\\n- **Disk space low**: Storage running out\\n\\nExample alert in Slack when a node goes down:\\n\\n<figure markdown=\\"span\\">\\n    ![Node Resume Alert](./imgs/resume_node.png)\\n</figure>\\n\\nFor detailed information, check the Grafana dashboard:\\n\\n<figure markdown=\\"span\\">\\n    ![Grafana Node Down](./imgs/grafana_down.png)\\n</figure>\\n\\n## Testing Your Cluster\\n\\nLet\'s run some tests to ensure everything works:\\n\\n### Test 1: Simple Job\\n\\n```bash\\nsrun hostname\\n```\\n\\n### Test 2: Multi-node Job\\n\\n```bash\\nsrun --nodes=2 --ntasks=2 hostname\\n```\\n\\n### Test 3: Interactive Session\\n\\n```bash\\nsrun --nodes=1 --cpus-per-task=2 --mem=2G --pty bash\\n\\n# Inside the session\\nhostname\\nnproc\\nfree -h\\nexit\\n```\\n\\n### Test 4: Batch Job\\n\\nCreate `test_job.sh`:\\n\\n```bash\\n#!/bin/bash\\n#SBATCH --job-name=test\\n#SBATCH --output=test_%j.out\\n#SBATCH --error=test_%j.err\\n#SBATCH --nodes=1\\n#SBATCH --ntasks=1\\n#SBATCH --cpus-per-task=2\\n#SBATCH --mem=1G\\n#SBATCH --time=00:05:00\\n\\necho \\"Job started at $(date)\\"\\necho \\"Running on node: $(hostname)\\"\\necho \\"CPUs allocated: $SLURM_CPUS_PER_TASK\\"\\necho \\"Memory allocated: $SLURM_MEM_PER_NODE MB\\"\\n\\n# Do some work\\nsleep 60\\n\\necho \\"Job finished at $(date)\\"\\n```\\n\\nSubmit it:\\n\\n```bash\\nsbatch test_job.sh\\n\\n# Check status\\nsqueue\\n\\n# When done, view output\\ncat test_*.out\\n```\\n\\n### Test 5: Resource Limits\\n\\n```bash\\n# Submit job requesting more resources than available\\nsrun --mem=999999 --pty bash\\n\\n# Should fail with:\\n# srun: error: Unable to allocate resources: Requested node configuration is not available\\n```\\n\\n### Test 6: Accounting\\n\\n```bash\\n# View your jobs\\nsacct\\n\\n# Detailed accounting info\\nsacct --format=JobID,JobName,User,State,Start,End,Elapsed,CPUTime,MaxRSS\\n\\n# Cluster usage summary\\nsreport cluster utilization\\n```\\n\\n## Architecture Diagram\\n\\nHere\'s what you\'ve built:\\n\\n```\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502           Users SSH to Controller           \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n                  \u2502\\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n    \u2502   Controller Node (Head)     \u2502\\n    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\\n    \u2502  \u2502 slurmctld              \u2502  \u2502  Job Scheduling\\n    \u2502  \u2502 slurmdbd + MariaDB     \u2502  \u2502  Accounting\\n    \u2502  \u2502 NFS Server             \u2502  \u2502  Shared Storage\\n    \u2502  \u2502 Prometheus + Grafana   \u2502  \u2502  Monitoring\\n    \u2502  \u2502 Alertmanager           \u2502  \u2502  Alerts\\n    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\\n    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n         \u2502                  \u2502\\n    \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n    \u2502 worker-01\u2502       \u2502 worker-02\u2502\\n    \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502       \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\\n    \u2502 \u2502slurmd\u2502 \u2502       \u2502 \u2502slurmd\u2502 \u2502\\n    \u2502 \u2502NFS \u2191 \u2502 \u2502       \u2502 \u2502NFS \u2191 \u2502 \u2502\\n    \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502       \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\n## For Developers: Local Testing with Vagrant\\n\\nIf you want to test the deployment locally using VMs:\\n\\n```bash\\n# Install Vagrant with libvirt provider\\nbash scripts/setup.sh 24.04 true\\n\\n# Create local VMs and deploy cluster\\nvagrant up\\n\\n# SSH to controller\\nvagrant ssh controller-01\\n\\n# Destroy VMs when done\\nvagrant destroy -f\\n```\\n\\n## Key Takeaways\\n\\nIn this post, we\'ve covered:\\n\\n1. **Why Automation**: The benefits of using Ansible for cluster management\\n2. **Production Architecture**: Multi-node setup with monitoring and alerting\\n3. **Slack Integration**: Proactive monitoring with notifications\\n4. **Automated Deployment**: One command to deploy the entire cluster\\n5. **Verification**: Testing your cluster thoroughly\\n\\n:::info\\n**What\'s Next?**\\n\\nIn [Part 3](/blog/how-to-build-slurm-hpc-part-3), we\'ll cover daily administration tasks, troubleshooting, security best practices, and advanced resource management.\\n:::\\n\\n## Resources\\n\\n- **GitHub Repository**: [RiverXData Slurm Ansible](https://github.com/riverxdata/river-slurm)\\n- **Deployment Docs**: [Scalable Slurm Deployment](/docs/resources/high-performance-computing/how-to-build-slurm-scalable-using-ansible/deployment)\\n- **Architecture Overview**: [Slurm Architecture](/docs/resources/high-performance-computing/how-to-build-slurm-scalable-using-ansible/overview)\\n- **Ansible Documentation**: [docs.ansible.com](https://docs.ansible.com/)\\n\\n## Contact\\n\\nQuestions about the deployment? Reach out at: nttg8100@gmail.com\\n\\n---\\n\\n*This is Part 2 of the RiverXData series on building Slurm HPC clusters. Continue to [Part 3](/blog/how-to-build-slurm-hpc-part-3) for administration and best practices.*"},{"id":"how-to-build-slurm-hpc-part-1","metadata":{"permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-1","source":"@site/blog/2026-01/2026-01-09.md","title":"Building a Slurm HPC Cluster (Part 1) - Single Node Setup and Fundamentals","description":"Building a High-Performance Computing (HPC) cluster can seem daunting, but with the right approach, you can create a robust system for managing computational workloads. This is Part 1 of a 3-part series where we\'ll build a complete Slurm cluster from scratch.","date":"2026-01-09T00:00:00.000Z","tags":[{"inline":true,"label":"slurm","permalink":"/river-docs/blog/tags/slurm"},{"inline":true,"label":"hpc","permalink":"/river-docs/blog/tags/hpc"},{"inline":true,"label":"tutorial","permalink":"/river-docs/blog/tags/tutorial"},{"inline":true,"label":"getting-started","permalink":"/river-docs/blog/tags/getting-started"}],"readingTime":7.65,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"how-to-build-slurm-hpc-part-1","title":"Building a Slurm HPC Cluster (Part 1) - Single Node Setup and Fundamentals","authors":["river"],"tags":["slurm","hpc","tutorial","getting-started"],"image":"./imgs/hpc_1.svg"},"unlisted":false,"prevItem":{"title":"Building a Slurm HPC Cluster (Part 2) - Scaling to Production with Ansible","permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-2"},"nextItem":{"title":"RIVER- A Web Application to Run Nf-Core","permalink":"/river-docs/blog/river-platform-and-nextflow"}},"content":"Building a High-Performance Computing (HPC) cluster can seem daunting, but with the right approach, you can create a robust system for managing computational workloads. This is **Part 1** of a 3-part series where we\'ll build a complete Slurm cluster from scratch.\\n\\nIn this first post, we\'ll cover the fundamentals by setting up a single-node Slurm cluster and understanding the core concepts.\\n\\n\x3c!--truncate--\x3e\\n\\n## Series Overview\\n\\n- **Part 1 (This Post)**: Introduction, Architecture, and Single Node Setup\\n- **[Part 2](/blog/how-to-build-slurm-hpc-part-2)**: Scaling to Production with Ansible\\n- **[Part 3](/blog/how-to-build-slurm-hpc-part-3)**: Administration and Best Practices\\n\\n## Why Slurm?\\n\\nWhen it comes to job scheduling in HPC environments, several options exist including PBS, Grid Engine, and IBM\'s LSF. However, Slurm (Simple Linux Utility for Resource Management) stands out for several compelling reasons:\\n\\n- **Open Source**: Free to use with a large, active community\\n- **Scalability**: Designed to scale from small clusters to the world\'s largest supercomputers\\n- **Flexibility**: Fine-grained control over job scheduling, resource allocation, and priority settings\\n- **Integration**: Works seamlessly with MPI, distributed computing frameworks (Spark, Ray, Dask), and monitoring tools\\n- **Performance**: Optimized for high throughput with minimal overhead\\n\\n## Understanding Slurm Architecture\\n\\nBefore diving into the implementation, it\'s crucial to understand the key components of a Slurm cluster:\\n\\n<figure markdown=\\"span\\">\\n    ![HPC Architecture](./imgs/HPC_architecture.png)\\n</figure>\\nsource: https://www.marquette.edu/high-performance-computing/architecture.php\\n\\n### Core Components\\n\\n1. **slurmctld (Controller Daemon)**: The brain of the cluster, running on the controller node. It handles job scheduling, resource tracking, and communicates with compute nodes.\\n\\n2. **slurmd (Node Daemon)**: Runs on compute nodes to execute jobs and report status back to the controller.\\n\\n3. **slurmdbd (Database Daemon)**: Optional but recommended for storing job accounting data, resource usage tracking, and fair-share scheduling.\\n\\n### Node Types\\n\\n| Node Type  | Services                | Purpose                              |\\n| ---------- | ----------------------- | ------------------------------------ |\\n| Controller | slurmctld               | Manages job scheduling and resources |\\n| Compute    | slurmd                  | Executes submitted jobs              |\\n| Login      | Slurm clients           | User access point for job submission |\\n| Database   | slurmdbd, MySQL/MariaDB | Stores accounting data               |\\n\\n<figure markdown=\\"span\\">\\n    ![Slurm Architecture](./imgs/slurm_arch.gif)\\n</figure>\\nsource: https://www.schedmd.com/\\n\\nFor a deeper understanding of Slurm architecture, check our [Slurm Architecture documentation](/docs/resources/high-performance-computing/slurm-architecture).\\n\\n## Single Node Setup - Understanding the Fundamentals\\n\\nStarting with a single-node setup helps you understand how Slurm works before scaling up. This approach is perfect for learning and local development.\\n\\n<div align=\\"center\\">\\n    <figure markdown=\\"span\\">\\n        ![Slurm Logo](./imgs/Slurm_logo.svg)\\n    </figure>\\n</div>\\n\\n:::info\\nThis setup runs on Ubuntu 20.04 and includes all standard Slurm features. Note that this configuration is for learning purposes - for production environments, you\'ll want the multi-node setup covered in Part 2.\\n:::\\n\\n### Basic Installation\\n\\nFirst, install the required Slurm components:\\n\\n```bash\\nsudo apt-get update -y && sudo apt-get install -y slurmd slurmctld\\n```\\n\\nVerify the installation:\\n\\n```bash\\n# Locate slurmd and slurmctld\\nwhich slurmd\\n# Output: /usr/sbin/slurmd\\n\\nwhich slurmctld\\n# Output: /usr/sbin/slurmctld\\n```\\n\\n### Configuring slurm.conf\\n\\nThe `slurm.conf` file is the heart of your Slurm configuration. This file must be identical across all nodes in a cluster (but for now, we just have one node).\\n\\nCreate your `slurm.conf`:\\n\\n```bash\\ncat <<EOF > slurm.conf\\n# slurm.conf for a single-node Slurm cluster\\nClusterName=localcluster\\nSlurmctldHost=localhost\\nMpiDefault=none\\nProctrackType=proctrack/linuxproc\\nReturnToService=2\\nSlurmctldPidFile=/run/slurmctld.pid\\nSlurmctldPort=6817\\nSlurmdPidFile=/run/slurmd.pid\\nSlurmdPort=6818\\nSlurmdSpoolDir=/var/lib/slurm-llnl/slurmd\\nSlurmUser=slurm\\nStateSaveLocation=/var/lib/slurm-llnl/slurmctld\\nSwitchType=switch/none\\nTaskPlugin=task/none\\n\\n# TIMERS\\nInactiveLimit=0\\nKillWait=30\\nMinJobAge=300\\nSlurmctldTimeout=120\\nSlurmdTimeout=300\\nWaittime=0\\n\\n# SCHEDULING\\nSchedulerType=sched/backfill\\nSelectType=select/cons_tres\\nSelectTypeParameters=CR_Core\\n\\n# ACCOUNTING (not enabled yet)\\nAccountingStorageType=accounting_storage/none\\nJobAcctGatherType=jobacct_gather/none\\nJobAcctGatherFrequency=30\\n\\n# LOGGING\\nSlurmctldDebug=info\\nSlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log\\nSlurmdDebug=info\\nSlurmdLogFile=/var/log/slurm-llnl/slurmd.log\\n\\n# COMPUTE NODES (adjust CPUs and RealMemory to match your system)\\nNodeName=localhost CPUs=2 Sockets=1 CoresPerSocket=2 ThreadsPerCore=1 RealMemory=1024 State=UNKNOWN\\n\\n# PARTITION CONFIGURATION\\nPartitionName=LocalQ Nodes=ALL Default=YES MaxTime=INFINITE State=UP\\nEOF\\n\\nsudo mv slurm.conf /etc/slurm-llnl/slurm.conf\\n```\\n\\n### Starting Services\\n\\nStart the Slurm daemons:\\n\\n```bash\\n# Start slurmd (compute daemon)\\nsudo service slurmd start\\nsudo service slurmd status\\n\\n# Start slurmctld (controller daemon)\\nsudo service slurmctld start\\nsudo service slurmctld status\\n```\\n\\n<figure markdown=\\"span\\">\\n    ![slurmd status](./imgs/slurmd_status.png)\\n</figure>\\n\\n<figure markdown=\\"span\\">\\n    ![slurmctld status](./imgs/slurmctld_status.png)\\n</figure>\\n\\nTest your setup by submitting a simple interactive job:\\n\\n```bash\\nsrun --mem 500MB -c 1 --pty bash\\n\\n# Check job details\\nsqueue -o \\"%i %P %u %T %M %l %D %C %m %R %Z %N\\" | column -t\\n```\\n\\n### Critical: Resource Limiting with cgroups\\n\\n:::warning\\n**This is a critical step that\'s often overlooked!**\\n\\nWithout proper cgroup configuration, jobs can exceed their allocated resources, potentially causing system instability or crashes. The job scheduler will accept resource limits, but won\'t actually enforce them.\\n:::\\n\\nLet\'s test this problem first. Submit a job requesting 500MB and try to allocate much more:\\n\\n```bash\\nsrun --mem 500MB -c 1 --pty bash\\n\\n# Try to allocate 1GB of memory (exceeding the 500MB limit)\\ndeclare -a mem\\ni=0\\nwhile :; do\\n    mem[$i]=$(head -c 100M </dev/zero | tr \'\\\\000\' \'x\') \\n    ((i++))\\n    echo \\"Allocated: $((i * 100)) MB\\"\\ndone\\n```\\n\\nBefore submitting the job, memory usage is less than 200MB:\\n\\n<figure markdown=\\"span\\">\\n    ![Memory before stress](./imgs/memory_before_stress.png)\\n</figure>\\n\\nAfter allocating 1GB, the job is not killed due to missing control group (cgroup) configuration:\\n\\n<figure markdown=\\"span\\">\\n    ![Over resource limit](./imgs/overresource_limit.png)\\n</figure>\\n\\nYou\'ll notice the job continues running even after exceeding 500MB - that\'s the problem!\\n\\nNow let\'s fix it with cgroups:\\n\\n```bash\\ncat <<EOF > cgroup.conf\\nCgroupAutomount=yes\\nCgroupMountpoint=/sys/fs/cgroup\\nConstrainCores=yes\\nConstrainRAMSpace=yes\\nConstrainDevices=yes\\nConstrainSwapSpace=yes\\nMaxSwapPercent=5\\nMemorySwappiness=0\\nEOF\\n\\nsudo mv cgroup.conf /etc/slurm-llnl/cgroup.conf\\n```\\n\\nUpdate `slurm.conf` to use cgroup plugins:\\n\\n```bash\\nsudo sed -i -e \\"s|ProctrackType=proctrack/linuxproc|ProctrackType=proctrack/cgroup|\\" \\\\\\n            -e \\"s|TaskPlugin=task/none|TaskPlugin=task/cgroup|\\" /etc/slurm-llnl/slurm.conf\\n```\\n\\nEnable cgroup in GRUB and reboot:\\n\\n```bash\\nsudo sed -i \'s/^GRUB_CMDLINE_LINUX=\\"/GRUB_CMDLINE_LINUX=\\"cgroup_enable=memory swapaccount=1 /\' /etc/default/grub\\nsudo update-grub\\nsudo reboot\\n```\\n\\nAfter reboot, restart Slurm services:\\n\\n```bash\\nsudo service slurmctld restart\\nsudo service slurmd restart\\n```\\n\\nNow test again with the same memory allocation script - this time, the job will be killed when it exceeds the limit!\\n\\n<figure markdown=\\"span\\">\\n    ![Out of Memory](./imgs/oom.png)\\n</figure>\\n\\n### Enabling Accounting\\n\\nJob accounting is essential for:\\n- Tracking who is using resources\\n- Monitoring job completion and failures\\n- Enforcing resource limits per user/group\\n- Fair-share scheduling\\n\\n<figure markdown=\\"span\\">\\n    ![Accounting disabled](./imgs/sacct_disable.png)\\n</figure>\\n\\nInstall the required packages:\\n\\n```bash\\nsudo apt-get install slurmdbd mariadb-server -y\\n```\\n\\nCreate the database and user:\\n\\n```bash\\nsudo service mysql start\\n\\nsudo mysql -e \\"CREATE DATABASE slurm_acct_db;\\"\\nsudo mysql -e \\"CREATE USER \'slurm\'@\'localhost\' IDENTIFIED BY \'slurm\';\\"\\nsudo mysql -e \\"GRANT ALL PRIVILEGES ON slurm_acct_db.* TO \'slurm\'@\'localhost\';\\"\\nsudo mysql -e \\"FLUSH PRIVILEGES;\\"\\n```\\n\\nVerify the database was created:\\n\\n```bash\\nsudo mysql -e \\"SHOW DATABASES;\\" \\nsudo mysql -e \\"SELECT User, Host FROM mysql.user;\\"\\n```\\n\\n<figure markdown=\\"span\\">\\n    ![Add database](./imgs/add_db.png)\\n</figure>\\n\\nConfigure `slurmdbd`:\\n\\n```bash\\ncat <<EOF > slurmdbd.conf\\nPidFile=/run/slurmdbd.pid\\nLogFile=/var/log/slurm/slurmdbd.log\\nDebugLevel=error\\nDbdHost=localhost\\nDbdPort=6819\\n\\n# DB connection data\\nStorageType=accounting_storage/mysql\\nStorageHost=localhost\\nStoragePort=3306\\nStorageUser=slurm\\nStoragePass=slurm\\nStorageLoc=slurm_acct_db\\nSlurmUser=slurm\\nEOF\\n\\nsudo mv slurmdbd.conf /etc/slurm-llnl/slurmdbd.conf\\nsudo service slurmdbd start\\n```\\n\\nUpdate `slurm.conf` to enable accounting:\\n\\n```bash\\nsudo sed -i -e \\"s|AccountingStorageType=accounting_storage/none|AccountingStorageType=accounting_storage/slurmdbd\\\\nAccountingStorageEnforce=associations,limits,qos\\\\nAccountingStorageHost=localhost\\\\nAccountingStoragePort=6819|\\" /etc/slurm-llnl/slurm.conf \\n\\nsudo sed -i -e \\"s|JobAcctGatherType=jobacct_gather/none|JobAcctGatherType=jobacct_gather/cgroup|\\" /etc/slurm-llnl/slurm.conf\\n\\nsudo systemctl restart slurmctld slurmd\\n```\\n\\nAdd your cluster and user to accounting:\\n\\n```bash\\n# Add cluster\\nsudo sacctmgr -i add cluster localcluster\\n\\n# Add account for your user\\nsudo sacctmgr -i add account $USER Cluster=localcluster\\n\\n# Add your user to the account\\nsudo sacctmgr -i add user $USER account=$USER DefaultAccount=$USER\\n\\nsudo systemctl restart slurmctld slurmd\\n```\\n\\n<figure markdown=\\"span\\">\\n    ![Add account](./imgs/add_account.png)\\n</figure>\\n\\nNow test accounting by submitting a job and viewing its details:\\n\\n```bash\\n# Submit a test job\\nsrun --mem 500MB -c 1 hostname\\n\\n# View accounting information\\nsacct\\n```\\n\\n<figure markdown=\\"span\\">\\n    ![Account usage](./imgs/account_usage.png)\\n</figure>\\n\\n## Key Takeaways\\n\\nIn this first part of our series, we\'ve covered:\\n\\n1. **Why Slurm**: Understanding the advantages of Slurm over alternatives\\n2. **Architecture**: Core components (slurmctld, slurmd, slurmdbd) and their roles\\n3. **Basic Setup**: Installing and configuring a single-node cluster\\n4. **Critical cgroups**: Why resource limiting is essential (and how to enable it)\\n5. **Accounting**: Setting up job tracking and resource monitoring\\n\\n:::info\\n**What\'s Next?**\\n\\nIn [Part 2](/blog/how-to-build-slurm-hpc-part-2), we\'ll take this knowledge and scale to a multi-node production cluster using Ansible automation. We\'ll add monitoring with Grafana, alerting via Slack, and shared storage with NFS.\\n:::\\n\\n## Resources\\n\\n- **Full Documentation**: [Single Node Slurm Setup](/docs/resources/high-performance-computing/how-to-build-slurm-single-node-with-full-functions)\\n- **Architecture Details**: [Slurm Architecture](/docs/resources/high-performance-computing/slurm-architecture)\\n- **HPC Overview**: [High-Performance Computing Overview](/docs/resources/high-performance-computing/high-performance-computing-overview)\\n- **Official Docs**: [SchedMD Slurm Documentation](https://slurm.schedmd.com/)\\n\\n---\\n\\n*This is Part 1 of the RiverXData series on building Slurm HPC clusters. Continue to [Part 2](/blog/how-to-build-slurm-hpc-part-2) to learn about production deployment with Ansible.*"},{"id":"river-platform-and-nextflow","metadata":{"permalink":"/river-docs/blog/river-platform-and-nextflow","source":"@site/blog/2026-01/2026-01-08.md","title":"RIVER- A Web Application to Run Nf-Core","description":"Simply, I just want a web application. This is an all-in-one application similar to Google Drive, but more advanced. It allows me to select files and put them into a workflow obtained directly from GitHub. For example, rnaseq from nf-core. The web application allows me to run and monitor Nextflow jobs. Then, it puts the results back to Google Drive, where I can view the results. It is useful for sharing results with my team or external teams. I can develop pipelines independently. My data can be stored on the cloud for backup. It is a simple and standard procedure for bioinformatics analysis. But I have not found any solution that fits these requirements is EASY, FREE, OPEN-SOURCE","date":"2026-01-08T00:00:00.000Z","tags":[{"inline":true,"label":"s3","permalink":"/river-docs/blog/tags/s-3"},{"inline":true,"label":"data-analysis","permalink":"/river-docs/blog/tags/data-analysis"},{"inline":true,"label":"slurm","permalink":"/river-docs/blog/tags/slurm"},{"inline":true,"label":"hpc","permalink":"/river-docs/blog/tags/hpc"},{"inline":true,"label":"web-platform","permalink":"/river-docs/blog/tags/web-platform"}],"readingTime":2.14,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"river-platform-and-nextflow","title":"RIVER- A Web Application to Run Nf-Core","authors":["river"],"tags":["s3","data-analysis","slurm","hpc","web-platform"],"image":"./imgs/intro.png"},"unlisted":false,"prevItem":{"title":"Building a Slurm HPC Cluster (Part 1) - Single Node Setup and Fundamentals","permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-1"}},"content":"Simply, I just want a web application. This is an all-in-one application similar to Google Drive, but more advanced. It allows me to select files and put them into a workflow obtained directly from GitHub. For example, rnaseq from nf-core. The web application allows me to run and monitor Nextflow jobs. Then, it puts the results back to Google Drive, where I can view the results. It is useful for sharing results with my team or external teams. I can develop pipelines independently. My data can be stored on the cloud for backup. It is a simple and standard procedure for bioinformatics analysis. But I have not found any solution that fits these requirements is `EASY`, `FREE`, `OPEN-SOURCE`\\n\\nIf you feel the same, **RIVER** may be your choice.\\n\\n\x3c!-- truncate --\x3e\\n:::info\\nThis blog post is intended for the general community, show cases only. I\'ll focus on the big picture rather than diving deep into technical details.\\n:::\\n\\nTo use it, you can follow the below tutorial\\n\\n## Quick review\\nFirst, you can take a look at how it work first. In breif, it will allows the users to interact with S3 bucket, select inputs from S3 buckets, run and put back results to s3 that\\nRIVER platform can visualize. After watching this, you are able to configure the RIVER platform as contributor, add your interested workflows and allow you to run tools\\n\\n\\n\\n<div style={{ position: \\"relative\\", paddingBottom: \\"56.25%\\", height: 0, overflow: \\"hidden\\", maxWidth: \\"100%\\", background: \\"#000\\" }}>\\n  <iframe \\n    src=\\"https://www.youtube.com/embed/boabEFNIkNA\\" \\n    frameBorder=\\"0\\" \\n    allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" \\n    allowFullScreen\\n    style={{ position: \\"absolute\\", top: 0, left: 0, width: \\"100%\\", height: \\"100%\\" }}\\n  />\\n</div> \\n\\n## Get started\\nYour raw data should be uploaded to the platform. The fastq files are downloaded from the below url, then upload to the platform\\n\\nThe samplesheet.csv on test profile\\n```bash\\nsample,fastq_1,fastq_2\\nSAMPLE1_PE,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample1_R1.fastq.gz,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample1_R2.fastq.gz\\nSAMPLE2_PE,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample2_R1.fastq.gz,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample2_R2.fastq.gz\\n```\\n\\nThe samplesheet.csv on RIVER\\n```bash\\nsample,fastq_1,fastq_2\\ntest,s3://genomics/demo/resources/fastq_files/sample1_R1.fastq.gz,s3://genomics/demo/resources/fastq_files/sample1_R2.fastq.gz\\n```\\n\\n<div style={{ position: \\"relative\\", paddingBottom: \\"56.25%\\", height: 0, overflow: \\"hidden\\", maxWidth: \\"100%\\", background: \\"#000\\" }}>\\n  <iframe \\n    src=\\"https://www.youtube.com/embed/Fujhi6ZnHQQ\\" \\n    frameBorder=\\"0\\" \\n    allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" \\n    allowFullScreen\\n    style={{ position: \\"absolute\\", top: 0, left: 0, width: \\"100%\\", height: \\"100%\\" }}\\n  />\\n</div>"}]}}')}}]);