"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[3783],{3338(e,n,o){o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"analysis/Development","title":"Development","description":"How Jobs Execute on HPC and Monitoring","source":"@site/docs/4.analysis/1.Development.md","sourceDirName":"4.analysis","slug":"/analysis/Development","permalink":"/docs/analysis/Development","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Upgrade","permalink":"/docs/platform/Upgrade"},"next":{"title":"Testing","permalink":"/docs/analysis/Testing"}}');var i=o(4848),r=o(8453);const s={},a="Development",l={},c=[{value:"How Jobs Execute on HPC and Monitoring",id:"how-jobs-execute-on-hpc-and-monitoring",level:2},{value:"Job Execution Workflow",id:"job-execution-workflow",level:3},{value:"Storage Configuration",id:"storage-configuration",level:3},{value:"Example Job Script",id:"example-job-script",level:3},{value:"Building Tools for the RIVER Platform",id:"building-tools-for-the-river-platform",level:2},{value:"Step 1: Clone Your Repo (created by template)",id:"step-1-clone-your-repo-created-by-template",level:3},{value:"Step 2: Configure the Schema",id:"step-2-configure-the-schema",level:3},{value:"Step 3: Add your job script",id:"step-3-add-your-job-script",level:3}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"development",children:"Development"})}),"\n",(0,i.jsx)(n.h2,{id:"how-jobs-execute-on-hpc-and-monitoring",children:"How Jobs Execute on HPC and Monitoring"}),"\n",(0,i.jsx)(n.p,{children:"The platform leverages SSH protocol to submit jobs and query job states on your HPC cluster. It automates the entire job execution workflow with a streamlined process."}),"\n",(0,i.jsx)(n.h3,{id:"job-execution-workflow",children:"Job Execution Workflow"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Core Process:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Transfer"}),": Job scripts are securely transferred to the remote HPC server"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Environment Setup"}),": Automatically installs micromamba, nextflow, singularity, python and essential tools (skips if already installed)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Tool Deployment"}),": Clones tools from GitHub repositories and injects user-defined parameters into execution scripts"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Web Applications"}),": Creates secure reverse proxy with authentication for accessing web applications that utilize extensive HPC resources"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Pipeline Execution"}),": Executes nf-core pipelines directly through nextflow"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"storage-configuration",children:"Storage Configuration"}),"\n",(0,i.jsxs)(n.p,{children:["It will create the ",(0,i.jsx)(n.code,{children:"params.json"})," file and ingested to the running script"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"For nf-core workflows:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"S3 credential for storage will be created automatically and handle I/O by nextflow (AWS CLI v2)"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"For non nf-core jobs:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["S3 credential for storage will be used by Goofys for mounting as fuse filesystem via ",(0,i.jsx)(n.code,{children:"goofys"})," for efficient input/output operations"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"example-job-script",children:"Example Job Script"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n#SBATCH --job-name="Thanh-Giang Tan Nguyen 11/04/2025 11:34:02 PM"\n#SBATCH --cpus-per-task=1\n#SBATCH --output=/home/river/.river/jobs/6c938f33-7c36-4132-b22d-b7f6287d1ef1/job.log\n#SBATCH --mem=2G\n#SBATCH --time=1:00:00\n\nset -euo pipefail\ncd /home/river/.river/jobs/6c938f33-7c36-4132-b22d-b7f6287d1ef1\n# === Setup paths ===\nRIVER_HOME="/home/river/.river"\nBIN_DIR="$RIVER_HOME/bin"\nGOOFYS_PATH="$BIN_DIR/goofys"\n\nexport PATH=$BIN_DIR:$PATH\nexport RIVER_HOME=$RIVER_HOME\nexport RIVER_HOME_TOOLS=$RIVER_HOME/bin\nexport SINGULARITY_CACHE_DIR=$RIVER_HOME/images/singularities/cache\nexport NXF_SINGULARITY_CACHEDIR=$RIVER_HOME/images/singularities/images\nexport NXF_VER=25.04.2\nexport job_id=6c938f33-7c36-4132-b22d-b7f6287d1ef1\n\nmkdir -p $RIVER_HOME_TOOLS \nmkdir -p $SINGULARITY_CACHE_DIR\nmkdir -p $NXF_SINGULARITY_CACHEDIR\n\n# === Install pixi ===\nwhich pixi || curl -fsSL https://pixi.sh/install.sh | sh\nexport PATH=$PATH:$HOME/.pixi/bin\npixi config append default-channels bioconda --global\npixi config append default-channels conda-forge --global\npixi global install nextflow jq git singularity python=3.14\n\n# === Install Goofys if missing ===\nif [ ! -f "$GOOFYS_PATH" ]; then\n    echo "Installing goofys..."\n    curl -L https://github.com/kahing/goofys/releases/download/v0.24.0/goofys -o "$GOOFYS_PATH"\n    chmod +x "$GOOFYS_PATH"\nelse\n    echo "Goofys already exists at: $GOOFYS_PATH"\nfi\n\n# Setup port for quick tunneling\nexport PORT=$(python -c "import socket; s=socket.socket(); s.bind((\'\',0)); print(s.getsockname()[1]); s.close()")\necho $PORT > $RIVER_HOME/jobs/6c938f33-7c36-4132-b22d-b7f6287d1ef1/job.port\necho $(hostname) > $RIVER_HOME/jobs/6c938f33-7c36-4132-b22d-b7f6287d1ef1/job.host\n\n# === Export user-defined Environment Variables ===\nwhile IFS== read -r key value; do\n   export "$key=$value"\ndone < <(jq -r \'to_entries|map("\\(.key)=\\(.value|tostring)")|.[]\' params.json)\n\n# === Clone Repository ===\nrepo_name=$(basename -s .git "$git")\nowner=$(basename "$(dirname "$git")")\nlocal_dir="$RIVER_HOME/tools/$owner/$repo_name/$tag"\n\nif [ "$owner" = "nf-core" ]; then\n    echo "Detected nf-core tool. Will run via Nextflow."\nelse\n    if [ ! -d "$local_dir" ]; then\n        echo "Cloning $git into $local_dir"\n        git clone --branch "$tag" --single-branch "$git" "$local_dir"\n    else\n        echo "Repository already cloned at $local_dir"\n    fi\n\n    ln -sf "$local_dir" "$RIVER_HOME/jobs/6c938f33-7c36-4132-b22d-b7f6287d1ef1/analysis"\nfi\n\n# === Setup Cloud Storage Mount ===\nmount_point="$RIVER_HOME/jobs/6c938f33-7c36-4132-b22d-b7f6287d1ef1/workspace"\nmkdir -p "$mount_point"\n\ntrap \'{ umount "$mount_point" || echo "Warning: S3 bucket was not mounted." ; }\' EXIT\n\n# === Run Main Job Command ===\nif [ "$owner" = "nf-core" ]; then\n    # AWS config\n    cat <<EOF > aws.config\naws {\n    client {\n        endpoint = "$AWS_ENDPOINT_URL"\n        s3PathStyleAccess = true\n    }\n}\nEOF\n    if [ -n "${profile:-}" ]; then\n        profiles="singularity,$profile"\n    else\n        profiles="singularity"\n    fi\n    nextflow run "$owner/$repo_name" \\\n        -r "$tag" \\\n        -c aws.config \\\n        -c river.config \\\n        -profile "$profiles" \\\n        -process.executor slurm \\\n        -process.shell \'bash\' \\\n        --outdir "s3://$bucket_name/$outdir/6c938f33-7c36-4132-b22d-b7f6287d1ef1" \\\n        -with-report "s3://$bucket_name/$outdir/6c938f33-7c36-4132-b22d-b7f6287d1ef1/report.html" \\\n        -resume\nelse\n    "$GOOFYS_PATH" \\\n    --profile "$bucket_name" \\\n    --file-mode=0700 \\\n    --dir-mode=0700 \\\n    --endpoint=$AWS_ENDPOINT_URL \\\n    "$bucket_name" "$mount_point"\n    bash $local_dir/river/main.sh\nfi\n'})}),"\n",(0,i.jsx)(n.h2,{id:"building-tools-for-the-river-platform",children:"Building Tools for the RIVER Platform"}),"\n",(0,i.jsx)(n.p,{children:"This guide provides an end-to-end process for creating a tool compatible with the RIVER platform. Prerequisites: Git configured on your machine."}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Start with the template repository ",(0,i.jsx)(n.a,{href:"https://github.com/riverxdata/batch-template-analysis",children:"https://github.com/riverxdata/batch-template-analysis"})]}),"\n",(0,i.jsxs)(n.li,{children:["For instructions on creating repositories from templates, see the ",(0,i.jsx)(n.a,{href:"https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-repository-from-a-template",children:"GitHub documentation"}),"."]}),"\n"]})}),"\n",(0,i.jsx)(n.h3,{id:"step-1-clone-your-repo-created-by-template",children:"Step 1: Clone Your Repo (created by template)"}),"\n",(0,i.jsxs)(n.p,{children:["This repo ",(0,i.jsx)(n.code,{children:"nttg8100/demo-river-non-ui-tool.git"})," is generated from the template ",(0,i.jsx)(n.code,{children:"https://github.com/riverxdata/batch-template-analysis"})," for non-interactive analysis"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"git clone git@github.com:nttg8100/demo-river-non-ui-tool.git\ncd demo-river-non-ui-tool\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Required directory structure:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 river\n\u2502   \u2514\u2500\u2500 main.sh\n\u2514\u2500\u2500 schema.json\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-2-configure-the-schema",children:"Step 2: Configure the Schema"}),"\n",(0,i.jsxs)(n.p,{children:["The schema defines tool parameters and adapts from nf-core standards. Use either ",(0,i.jsx)(n.code,{children:"schema.json"})," or ",(0,i.jsx)(n.code,{children:"nextflow_schema.json"})," (for nf-core pipelines)."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key features:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Groups parameters for easier configuration"}),"\n",(0,i.jsx)(n.li,{children:"Optional for web-based tools"}),"\n",(0,i.jsxs)(n.li,{children:["Follows ",(0,i.jsx)(n.a,{href:"https://docs.seqera.io/platform-cloud/pipeline-schema/overview",children:"nextflow JSON schema"})," standards"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example schema.json:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n"$defs": {\n    "input_output_options": {\n        "title": "Input output option",\n        "type": "object",\n        "description": "",\n        "default": "",\n        "required": ["file"],\n        "properties": \n            {\n                "file": {\n                                "type": "string",\n                                "default": "workspace/test.txt",\n                                "format": "file-path",\n                                "fa_icon": "fas fa-dna",\n                                "description": "File path"\n                },\n                "dir": {\n                                "type": "string",\n                                "default": "workspace",\n                                "format": "directory-path",\n                                "fa_icon": "fas fa-dna",\n                                "description": "Directory path"\n                },\n                "integer": {\n                                "type": "integer",\n                                "default": 1,\n                                "description": "Integer",\n                                "fa_icon": "fas fa-greater-than-equal"\n                },\n                "number": {\n                                "type": "number",\n                                "default": 0.3,\n                                "description": "Float",\n                                "fa_icon": "fas fa-greater-than-equal"\n                },\n                "string": {\n                                "type": "string",\n                                "default": "string1",\n                                "description": "String",\n                                "fa_icon": "fas fa-greater-than-equal",\n                                "enum": ["string1", "string2", "string3"]\n                },\n                "boolean": {\n                                "type": "boolean",\n                                "default": true,\n                                "description": "Boolean",\n                                "fa_icon": "fas fa-greater-than-equal"\n                }\n            }\n        }\n    }\n}\n'})}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:["This schema is compatible with the nf-core schema, you can copy this json file content and add to the web-based editor provided by nf-core. After the adjustment, you can copy and put it back\nUse the ",(0,i.jsx)(n.a,{href:"https://oldsite.nf-co.re/pipeline_schema_builder",children:"nf-core pipeline schema builder"})," to create and modify your schema visually."]})}),"\n",(0,i.jsx)(n.h3,{id:"step-3-add-your-job-script",children:"Step 3: Add your job script"}),"\n",(0,i.jsxs)(n.p,{children:["It will ingest the parameters values to the script on the ",(0,i.jsx)(n.code,{children:"river/main.sh"})]}),"\n",(0,i.jsx)(n.p,{children:"Here, we add the example just to print the out."}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsx)(n.p,{children:"You should not expose your credential like S3 keys to the console. It can be viewed via log file."})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\necho ""\necho "============================================="\necho "          RIVER ANALYSIS TEMPLATE"\necho "============================================="\necho ""\n\n# Default Environment Variables\necho ">> Default Environment Variables:"\necho "---------------------------------------------"\necho "UUID Job ID  : $job_id"\necho "CPU          : $cpu"\necho "Memory       : $memory"\necho "Time         : $time"\necho "RIVER_HOME   : $RIVER_HOME"\necho "---------------------------------------------"\necho ""\n\n# Parameters\necho ">> Parameters:"\necho "---------------------------------------------"\necho "File Path    : $file"\necho "Folder Path  : $dir"\necho "Integer      : $integer"\necho "Number(float): $number"\necho "String       : $string"\necho "Boolean      : $boolean"\necho "---------------------------------------------"\necho ""\n\n# Bootstrap (for Custom Reverse Proxy)\necho ">> Bootstrap: Uncomment if you want to use custom reverse proxy"\necho "---------------------------------------------"\n# Custom reverse location.\n# Example: RStudio Server automatically redirects to `/` and restricts iframe usage.\necho "$job_id/" > "$RIVER_HOME/jobs/$job_id/job.proxy_location"\n\n# Example: VS Code Server or JupyterLab requires reverse proxy.\necho "$job_id" > "$RIVER_HOME/jobs/$job_id/job.url"\n\n# If the job is only exported via port (basic web app), still create this file but leave it empty.\ntouch "$RIVER_HOME/jobs/$job_id/job.url"\necho "---------------------------------------------"\necho ""\n\n# Placeholder for Main Script Execution\necho "Put your main script here, using arguments from environment variables."\n\necho ""\necho "\u2714 Analysis Completed!"\necho "============================================="\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Now, you can run the tools on the HPC. However, to ensure the integration process is smooth, we encourage you to add your self testing on your local environment. Check ",(0,i.jsx)(n.a,{href:"/docs/analysis/Testing",children:"Testing"})]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,o){o.d(n,{R:()=>s,x:()=>a});var t=o(6540);const i={},r=t.createContext(i);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);