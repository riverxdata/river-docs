"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[6959],{4924(n){n.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"bioinformatics-containers-build-efficient-docker","metadata":{"permalink":"/river-docs/blog/bioinformatics-containers-build-efficient-docker","source":"@site/blog/2026-02/2026-02-08.md","title":"Containers in Bioinformatics: Community Tooling and Efficient Docker Building","description":"Docker containers are revolutionizing bioinformatics by automating reproducibility and portability across platforms. But what problems can they actually solve? This post shows real-world applications of containers in bioinformatics workflows, then guides you through the simplest possible ways to use, build and debug them.","date":"2026-02-08T00:00:00.000Z","tags":[{"inline":true,"label":"docker","permalink":"/river-docs/blog/tags/docker"},{"inline":true,"label":"bioinformatics","permalink":"/river-docs/blog/tags/bioinformatics"},{"inline":true,"label":"bioconda","permalink":"/river-docs/blog/tags/bioconda"},{"inline":true,"label":"nextflow","permalink":"/river-docs/blog/tags/nextflow"},{"inline":true,"label":"best-practices","permalink":"/river-docs/blog/tags/best-practices"}],"readingTime":20.69,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"bioinformatics-containers-build-efficient-docker","title":"Containers in Bioinformatics: Community Tooling and Efficient Docker Building","authors":["river"],"tags":["docker","bioinformatics","bioconda","nextflow","best-practices"],"image":"./imgs/intro.png"},"unlisted":false,"nextItem":{"title":"Bioinformatics Workflow Template: Standardizing Python Pipelines with Modular Design","permalink":"/river-docs/blog/bioinformatics-workflow-template-ci-cd-best-practices"}},"content":"Docker containers are revolutionizing bioinformatics by automating reproducibility and portability across platforms. But what problems can they actually solve? This post shows real-world applications of containers in bioinformatics workflows, then guides you through the simplest possible ways to use, build and debug them. \\n\\n\x3c!-- truncate --\x3e\\n\\n## Prerequisites: Get Docker Installed and Running\\n\\nBefore reading this blog, **you need Docker installed on your system**. This blog assumes you already have Docker up and running and covers how to effectively use containers for bioinformatics work.\\n\\n### Start Here First\\n\\nIf Docker is not yet installed on your machine, visit the official Docker documentation:\\n\\n**\ud83d\udce6 https://docs.docker.com/get-started/get-docker/**\\n\\nThis guide will walk you through:\\n- **Installing Docker Desktop** (Mac, Windows, Linux)\\n- **Verifying your installation** with `docker --version`\\n- **Running your first container** with `docker run hello-world`\\n- **Understanding Docker basics** (images, containers, registries)\\n\\n### What You Need to Know\\n\\nAfter installing Docker, verify it\'s working:\\n\\n```bash\\n# Check Docker version\\ndocker --version\\n# Output: Docker version 24.0.0, build 12345abc\\n\\n# Run a test container\\ndocker run hello-world\\n# Output: Hello from Docker! This message shows that your installation appears to be working correctly.\\n```\\n\\n### About This Blog\\n\\nOnce Docker is installed and working, this blog will teach you:\\n\\n1. **Why containers matter** for reproducible bioinformatics\\n2. **How to use pre-built containers** from BioContainers and Seqera\\n3. **How to run containers** with your own data\\n4. **How to build custom containers** with Conda and Micromamba\\n5. **How to debug containers** in Nextflow pipelines\\n6. **Best practices** for efficient, reproducible workflows\\n\\n### Prerequisites Checklist\\n\\nBefore continuing, verify you have:\\n\\n- \u2705 Docker installed (`docker --version` works)\\n- \u2705 Docker daemon running (or Docker Desktop active)\\n- \u2705 Can pull images (`docker pull hello-world` works)\\n- \u2705 Basic familiarity with command line/terminal\\n- \u2705 (Optional but helpful) Nextflow installed for pipeline examples\\n\\n**Not ready?** Go to https://docs.docker.com/get-started/get-docker/ first, then come back here!\\n\\n---\\n\\n## 1. Why Manual Installation Fails: The Old Way vs. Modern Containers\\n\\nTraditionally, bioinformatics labs installed software on shared servers or laptops using commands like:\\n\\n```bash\\nsudo apt-get install samtools bwa python3\\n```\\n\\nWhile this seems easy, it creates major reproducibility, portability, and maintenance headaches:\\n\\n- **Manual Steps:** Each researcher (or lab) performs installs slightly differently. Written instructions, scripts, and troubleshooting steps are often lost, mixed, or outdated.\\n- **Not Reproducible:** No record of what was installed, which versions, or the exact order. Over time, software updates, conflicts, and broken dependencies accumulate, leading to the \\"works for me\\" problem.\\n- **Hard to Debug:** Version mismatches, library conflicts, and subtle errors are almost impossible to diagnose or fix once the environment drifts.\\n- **Not Portable:** Scripts break on macOS, Windows, or other Linux variants. HPC systems often restrict sudo/root access, making traditional installs impossible.\\n\\n**Result:** Collaboration across labs, institutions, or countries becomes fragile. Reproducing results\u2014even on your own machine\u2014can be a challenge!\\n\\n### The Modern Solution: Containers and Community Tooling\\n\\nToday, bioinformatics has largely replaced manual installs with containers and environment managers:\\n\\n- **BioConda:** Over 8000 bioinformatics packages can be installed reproducibly via Conda.\\n- **BioContainers:** Automatically generates Docker containers for each tool, available at [quay.io/biocontainers](https://quay.io/organization/biocontainers).\\n    ```bash\\n    docker pull quay.io/biocontainers/samtools:1.15.1--h1170115_0\\n    ```\\n- **Seqera/Nextflow/nf-core:** Pipelines with environment definitions (Conda, Docker, Singularity) are shared and run identically everywhere.\\n\\n*Bottom line:* Containers and Conda have solved the nightmare of manual installs, making reproducible science and collaboration routine in bioinformatics.\\n\\n---\\n\\n## 2. Using Docker Containers for Bioinformatics Data Analysis\\n\\nOne of the biggest advantages of Docker is the ability to run bioinformatics tools directly\u2014without any installation\u2014on your own datasets. Here\u2019s a practical example using FastQC, a widely used quality control tool for sequencing data.\\n\\n### Example: Run FastQC from a Container\\n\\n1. **Pull the FastQC container (only needed once):**\\n   ```bash\\n   docker pull quay.io/biocontainers/fastqc:<version>\\n   # Replace <version> with the specific tag, e.g. 0.11.9--hdfd78af_1\\n   ```\\n\\n2. **Run FastQC, mounting your data folder:**\\n   ```bash\\n   docker run --rm -v \\"$PWD:/data\\" quay.io/biocontainers/fastqc:<version> fastqc /data/my_reads.fastq\\n   ```\\n   - `--rm`: Automatically remove the container after it exits.\\n   - `-v \\"$PWD:/data\\"`: Mount your current directory (with your FASTQ file) into the container at `/data`.\\n   - The container runs the FastQC executable, analyzing the file and writing results back to your folder.\\n\\n**Key Benefits:**\\n- No software installation or dependency headaches.\\n- Exact version and environment guaranteed.\\n- Can share or rerun analysis anywhere Docker is available (Linux, Mac, Windows, cloud, HPC).\\n\\nThis approach works for thousands of bioinformatics containers\u2014swap FastQC for Samtools, BWA, STAR, or other tools and run your analysis fully reproducibly and portable.\\n\\n### Running Containers Interactively\\n\\nSometimes you want to run tools or explore interactively inside the container, just as if you were launching a shell. Docker makes this easy with the `-it` flags:\\n\\n- `-i` keeps STDIN open (interactive input)\\n- `-t` allocates a pseudo-terminal (TTY)\\n\\n**Example: Start an interactive shell inside the FastQC container:**\\n\\n```bash\\ndocker run --rm -it -v \\"$PWD:/data\\" quay.io/biocontainers/fastqc:<version> /bin/bash\\n```\\n\\nNow you\u2019re inside the container shell:\\n\\n```bash\\nfastqc /data/my_reads.fastq\\nls /data\\n```\\n\\n**Benefits:**\\n- Inspect files, experiment with commands, or troubleshoot in real-time\\n- Explore the container environment (dependencies, installed tools)\\n- Still fully reproducible every time you launch\\n\\n### Debugging and Inspecting a Running Docker Container\\n\\nIf your container is already running (perhaps as part of a pipeline or a background process), Docker lets you connect and debug interactively using `docker exec`:\\n\\n1. List running containers:\\n   ```bash\\n   docker ps\\n   ```\\n   Note the CONTAINER ID in the output.\\n\\n2. Start a bash shell inside the running container:\\n   ```bash\\n   docker exec -it <CONTAINER_ID> /bin/bash\\n   ```\\n   Now you can inspect files, check logs, and run additional commands inside the live container.\\n\\n---\\n\\n### ENTRYPOINT and Overriding Commands\\n\\nMany containers run a specific tool as their default ENTRYPOINT\u2014for example, FastQC containers launch FastQC automatically. However, you can temporarily override ENTRYPOINT and start a shell using:\\n\\n```bash\\ndocker run --rm -it --entrypoint /bin/bash quay.io/biocontainers/fastqc:<version>\\n```\\n\\nThis starts an interactive shell even if the container\u2019s Dockerfile specifies a different default command. Useful for debugging, troubleshooting, or exploring the container environment.\\n\\n**Summary:**\\n- Use `docker exec` to debug running containers\\n- Override ENTRYPOINT for interactive shells\\n- Enables real-time troubleshooting and inspection, making containers even more flexible for science\\n\\n---\\n\\n### Running Containers with Nextflow: Inspecting the Actual Docker Commands\\n\\nWhen you run a Nextflow pipeline with Docker containers, Nextflow automatically generates and executes Docker commands for each task. Understanding these commands is useful for debugging, profiling, and understanding exactly what your pipeline is doing.\\n\\n#### Where Nextflow Stores Docker Commands\\n\\nNextflow creates a work directory structure where each task has its own folder containing:\\n- `.command.sh` - The shell script to execute\\n- `.command.run` - The wrapper script that Nextflow uses to run the task\\n- `.command.trace` - Task execution trace\\n- `.exitcode` - Exit code of the task\\n- Output files from the task\\n\\n**Directory structure:**\\n```\\nwork/\\n\u251c\u2500\u2500 ab/\\n\u2502   \u251c\u2500\u2500 abc123def456.../\\n\u2502   \u2502   \u251c\u2500\u2500 .command.sh\\n\u2502   \u2502   \u251c\u2500\u2500 .command.run\\n\u2502   \u2502   \u251c\u2500\u2500 .command.trace\\n\u2502   \u2502   \u2514\u2500\u2500 ... (output files)\\n\u2502   \u2514\u2500\u2500 def789ghi012.../\\n\u2502       \u2514\u2500\u2500 ...\\n\u2514\u2500\u2500 fe/\\n    \u2514\u2500\u2500 949a78b06c5fd8284aeb2f87d14f7d/\\n        \u251c\u2500\u2500 .command.sh\\n        \u251c\u2500\u2500 .command.run\\n        \u2514\u2500\u2500 ... (task outputs)\\n```\\n\\n#### Viewing the Actual Docker Command\\n\\nTo see the exact Docker command that Nextflow executed for a specific task:\\n\\n```bash\\n# 1. Find your task\'s work directory\\n# For example, in the pipeline output you might see:\\n#   vep (ensembl-vep:115.2) [fe/949a78b...]\\n\\n# 2. Navigate to that directory\\ncd work/fe/949a78b06c5fd8284aeb2f87d14f7d/\\n\\n# 3. View the command that was executed\\ncat .command.run\\n```\\n\\n#### Example: Nextflow Docker Command for VEP (Variant Effect Predictor)\\n\\nHere\'s a complete example of how Nextflow runs a Docker container:\\n\\n```bash\\ndocker rm $NXF_BOXID &>/dev/null || true\\ndocker stop $NXF_BOXID\\n\\ndocker run -i \\\\\\n  --cpu-shares 8192 \\\\\\n  --memory 14336m \\\\\\n  -e \\"NXF_TASK_WORKDIR\\" \\\\\\n  -e \\"NXF_DEBUG=${NXF_DEBUG:=0}\\" \\\\\\n  -v <your project working directory>:<your project working directory> \\\\\\n  -w \\"$NXF_TASK_WORKDIR\\" \\\\\\n  -u $(id -u):$(id -g) \\\\\\n  --name $NXF_BOXID \\\\\\n  community.wave.seqera.io/library/ensembl-vep:115.2--90ec797ecb088e9a \\\\\\n  /bin/bash <your project working directory>/work/fe/949a78b06c5fd8284aeb2f87d14f7d/.command.run nxf_trace\\n```\\n\\n#### Breaking Down the Docker Command\\n\\nLet\'s understand each component:\\n\\n| Component                      | Purpose                                                      |\\n| ------------------------------ | ------------------------------------------------------------ |\\n| `docker run`                   | Start a new container                                        |\\n| `-i`                           | Keep STDIN open (interactive)                                |\\n| `--cpu-shares 8192`            | Allocate CPU resources (set by Nextflow from cpus directive) |\\n| `--memory 14336m`              | Limit memory to 14GB (set by Nextflow from memory directive) |\\n| `-e NXF_TASK_WORKDIR`          | Pass Nextflow task work directory env var                    |\\n| `-e NXF_DEBUG=${NXF_DEBUG:=0}` | Pass debug flag                                              |\\n| `-v /project:/project`         | Mount project directory for input/output                     |\\n| `-w \\"$NXF_TASK_WORKDIR\\"`       | Set working directory inside container                       |\\n| `-u $(id -u):$(id -g)`         | Run as current user (file permissions)                       |\\n| `--name $NXF_BOXID`            | Name the container (cleanup reference)                       |\\n| `image:tag`                    | Container image to use                                       |\\n| `/bin/bash .command.run`       | Execute the task script                                      |\\n| `nxf_trace`                    | Nextflow tracing flag                                        |\\n\\n#### Re-Running a Failed Task Manually\\n\\nIf a task fails in Nextflow, you can manually re-run it for debugging:\\n\\n```bash\\n# 1. Navigate to the task work directory\\ncd work/fe/949a78b06c5fd8284aeb2f87d14f7d/\\n\\n# 2. View the task script\\ncat .command.sh\\n\\n# 3. View Nextflow\'s Docker wrapper\\ncat .command.run\\n\\n# 4. Manually run the Docker command (copy from .command.run output)\\n# and inspect what went wrong\\ndocker run -i \\\\\\n  --cpu-shares 8192 \\\\\\n  --memory 14336m \\\\\\n  -e \\"NXF_TASK_WORKDIR\\" \\\\\\n  -v <your project working directory>:<your project working directory> \\\\\\n  -w \\"$NXF_TASK_WORKDIR\\" \\\\\\n  -u $(id -u):$(id -g) \\\\\\n  community.wave.seqera.io/library/ensembl-vep:115.2--90ec797ecb088e9a \\\\\\n  /bin/bash <your project working directory>/work/fe/949a78b06c5fd8284aeb2f87d14f7d/.command.run\\n\\n# 5. Check the exit code\\ncat .exitcode\\n\\n# 6. View any error logs\\ncat .command.err  # (if exists)\\ncat .command.out  # (if exists)\\n\\n# 7. If you run on HPC, (e.g SLURM ), run submit job\\nsbatch .command.run\\n\\n# 8. You can debug manually by running container interactively\\ndocker run -it -v $PWD:$PWD -w $PWD <your container> \\n\\n# 9. Modify .command.sh then inside your container, start to debug by running command directly\\nbash .command.sh\\n```\\n\\n#### Key Nextflow Docker Features\\n\\n| Feature                   | How Nextflow Uses It                                                     |\\n| ------------------------- | ------------------------------------------------------------------------ |\\n| **Resource limits**       | `--cpus` and `--memory` directives \u2192 `--cpu-shares` and `--memory` flags |\\n| **Volume mounts**         | Pipeline inputs/outputs \u2192 `-v` flags                                     |\\n| **User permissions**      | Current user \u2192 `-u $(id -u):$(id -g)`                                    |\\n| **Work directory**        | Task-specific directory \u2192 `-w \\"$NXF_TASK_WORKDIR\\"`                       |\\n| **Container cleanup**     | Automatic removal \u2192 `docker rm` commands                                 |\\n| **Environment variables** | Pipeline configuration \u2192 `-e` flags                                      |\\n\\n#### Profiling and Optimization\\n\\nBy examining the actual Docker commands, you can:\\n\\n1. **Check resource allocation:**\\n   ```bash\\n   grep \\"cpu-shares\\\\|memory\\" work/*/*)/.command.run\\n   ```\\n\\n2. **Verify mounted volumes:**\\n   ```bash\\n   grep \\"\\\\-v\\" work/ab/abc123*/.command.run\\n   ```\\n\\n3. **See container images being used:**\\n   ```bash\\n   grep \\"docker run\\" work/*/*)/.command.run | grep -oE \\"image:[^ ]+\\"\\n   ```\\n\\n4. **Profile task execution:**\\n   ```bash\\n   # Check how long each task took\\n   for dir in work/*/*/*/.command.trace; do\\n       echo \\"Task: $(dirname $dir)\\"\\n       cat \\"$dir\\"\\n   done\\n   ```\\n\\n### Summary\\n\\n- Nextflow automatically generates Docker commands for each task\\n- Task commands are stored in `work/<task-id>/.command.run`\\n- You can inspect and manually re-run failed tasks\\n- Understanding these commands helps with debugging and optimization\\n- Resource limits from Nextflow directives are converted to Docker flags\\n- Each task runs in isolation with its own work directory\\n\\n---\\n\\n## 3. Efficient Docker Container Building for Bioinformatics\\n\\nBuilding efficient Docker containers keeps your images fast to download, runs, and easy to share.\\n\\n### Best Practices\\n\\n- **Start with a lightweight, official base image**\\n  - Alpine, Debian-slim, Ubuntu LTS\\n\\n- **Install only required tools**\\n  - Don\'t add extra utilities or unnecessary packages\\n\\n- **Clean cache and remove temp files**\\n  - In the same RUN step: `rm -rf /var/lib/apt/lists/*`\\n\\n- **Use multi-stage builds for large or compiled tools**\\n  - Build from source in a builder stage, copy binaries into a slim final image\\n\\n### Minimal Example: Install a Tool\\n\\n```dockerfile\\nFROM ubuntu:24.04\\nRUN apt-get update && \\\\\\n    apt-get install -y --no-install-recommends samtools && \\\\\\n    apt-get clean && rm -rf /var/lib/apt/lists/*\\nWORKDIR /data\\nCMD [\\"/bin/bash\\"]\\n```\\n\\n### Multi-Stage Build Example: Build from Source\\n\\n```dockerfile\\nFROM ubuntu:24.04 AS builder\\nRUN apt-get update && apt-get install -y build-essential wget\\nRUN wget https://github.com/samtools/samtools/releases/download/1.18/samtools-1.18.tar.bz2 && \\\\\\n    tar -xjf samtools-1.18.tar.bz2 && \\\\\\n    cd samtools-1.18 && \\\\\\n    ./configure --prefix=/usr/local && make && make install\\n\\nFROM ubuntu:24.04\\nRUN apt-get update && apt-get install -y --no-install-recommends libbz2-1.0 liblzma5 zlib1g\\nCOPY --from=builder /usr/local/bin/samtools /usr/local/bin/\\nWORKDIR /data\\nCMD [\\"/bin/bash\\"]\\n```\\n\\n### Further Optimizations\\n- Sort package lists alphabetically\\n- Use `.dockerignore` to reduce build context size\\n- Pin exact tool/package versions for reproducibility\\n\\n---\\n\\n## 4. Building Docker Images with Conda and Micromamba\\n\\nBuilding Docker containers from Conda packages is one of the most efficient ways to create reproducible bioinformatics environments. Micromamba is a lightweight, faster alternative to Conda that\'s perfect for containers.\\n\\n### Why Micromamba for Docker Images?\\n\\nMicromamba offers significant advantages over traditional Conda:\\n- **Fast:** 10-100x faster dependency resolution than Conda\\n- **Lightweight:** Minimal footprint, perfect for containerized deployments\\n- **Reproducible:** Environment lock files guarantee exact versions\\n- **Production-ready:** Used extensively in bioinformatics pipelines\\n- **Small base image:** Official `mambaorg/micromamba` image is only ~150MB\\n\\n### The Modern Approach: Conda.yml + Micromamba\\n\\nInstead of writing complex RUN commands with inline package specifications, use a `conda.yml` environment file:\\n\\n**conda.yml:**\\n```yaml\\nname: base\\nchannels:\\n  - conda-forge\\n  - bioconda\\ndependencies:\\n  - bioconda::fastqc=0.12.1\\n  - bioconda::samtools=1.18\\n  - bioconda::bwa=0.7.17\\n  - conda-forge::procps-ng\\n```\\n\\n**Dockerfile:**\\n```dockerfile\\nFROM mambaorg/micromamba:1.5.10-noble\\n\\n# Copy environment file\\nCOPY --chown=$MAMBA_USER:$MAMBA_USER conda.yml /tmp/conda.yml\\n\\n# Install packages from environment file\\nRUN micromamba install -y -n base -f /tmp/conda.yml \\\\\\n    && micromamba clean -a -y\\n\\n# Export explicit lock file for reproducibility\\nRUN micromamba env export --name base --explicit > environment.lock \\\\\\n    && echo \\">> CONDA_LOCK_START\\" \\\\\\n    && cat environment.lock \\\\\\n    && echo \\"<< CONDA_LOCK_END\\"\\n\\n# Set PATH\\nUSER root\\nENV PATH=\\"$MAMBA_ROOT_PREFIX/bin:$PATH\\"\\n\\nWORKDIR /data\\nCMD [\\"/bin/bash\\"]\\n```\\n\\n### Understanding the Dockerfile\\n\\nLet\'s break down each component:\\n\\n**1. Base Image:**\\n```dockerfile\\nFROM mambaorg/micromamba:1.5.10-noble\\n```\\n- `mambaorg/micromamba` includes Micromamba pre-installed\\n- `1.5.10` is the version (pin for reproducibility)\\n- `noble` is Ubuntu 24.04 LTS variant\\n- Much lighter than installing Conda from scratch\\n\\n**2. Copy Environment File:**\\n```dockerfile\\nCOPY --chown=$MAMBA_USER:$MAMBA_USER conda.yml /tmp/conda.yml\\n```\\n- Copies your conda.yml to the container\\n- `$MAMBA_USER` is the non-root user (set by base image)\\n- Preserves proper file ownership\\n\\n**3. Install with Micromamba:**\\n```dockerfile\\nRUN micromamba install -y -n base -f /tmp/conda.yml \\\\\\n    && micromamba clean -a -y\\n```\\n- `-y`: Auto-confirm installation\\n- `-n base`: Install into the base environment\\n- `-f /tmp/conda.yml`: Use environment file\\n- `micromamba clean -a -y`: Remove cached packages (reduces image size by ~50%)\\n\\n**4. Generate Lock File (for reproducibility):**\\n```dockerfile\\nRUN micromamba env export --name base --explicit > environment.lock \\\\\\n    && echo \\">> CONDA_LOCK_START\\" \\\\\\n    && cat environment.lock \\\\\\n    && echo \\"<< CONDA_LOCK_END\\"\\n```\\n- Exports explicit lock file with exact pinned versions and URLs\\n- Echo statements make lock file visible in build logs\\n- This file can be saved and used for exact reproducibility later\\n\\n**5. Set PATH and User:**\\n```dockerfile\\nUSER root\\nENV PATH=\\"$MAMBA_ROOT_PREFIX/bin:$PATH\\"\\n```\\n- Ensures Micromamba binaries are in PATH\\n- `USER root` required for some system monitoring (see procps-ng below)\\n\\n### Why procps-ng is Important for Nextflow\\n\\nNotice `conda-forge::procps-ng` in the dependencies. Here\'s why:\\n\\n```yaml\\ndependencies:\\n  - conda-forge::procps-ng\\n```\\n\\nNextflow uses `procps-ng` tools (like `ps`, `top`) to monitor:\\n- CPU usage\\n- Memory consumption\\n- Process status\\n\\nWithout it, Nextflow cannot properly track task resource usage. Always include it in bioinformatics containers.\\n\\n### Complete Multi-Tool Example\\n\\nHere\'s a full example with multiple bioinformatics tools:\\n\\n**conda.yml:**\\n```yaml\\nname: base\\nchannels:\\n  - conda-forge\\n  - bioconda\\ndependencies:\\n  - bioconda::fastqc=0.12.1\\n  - bioconda::samtools=1.18\\n  - bioconda::bwa=0.7.17\\n  - bioconda::bcftools=1.18\\n  - bioconda::picard=3.0.0\\n  - conda-forge::procps-ng\\n  - conda-forge::parallel\\n```\\n\\n**Dockerfile:**\\n```dockerfile\\nFROM mambaorg/micromamba:1.5.10-noble\\n\\nCOPY --chown=$MAMBA_USER:$MAMBA_USER conda.yml /tmp/conda.yml\\n\\nRUN micromamba install -y -n base -f /tmp/conda.yml \\\\\\n    && micromamba install -y -n base conda-forge::procps-ng \\\\\\n    && micromamba env export --name base --explicit > environment.lock \\\\\\n    && echo \\">> CONDA_LOCK_START\\" \\\\\\n    && cat environment.lock \\\\\\n    && echo \\"<< CONDA_LOCK_END\\" \\\\\\n    && micromamba clean -a -y\\n\\nUSER root\\nENV PATH=\\"$MAMBA_ROOT_PREFIX/bin:$PATH\\"\\n\\nWORKDIR /data\\nCMD [\\"/bin/bash\\"]\\n```\\n\\n### Building and Using Your Container\\n\\n**1. Build the image:**\\n```bash\\ndocker build -t myregistry.com/bioinformatics/biotools:1.0 .\\n```\\n\\n**2. Test it locally:**\\n```bash\\ndocker run --rm -v \\"$PWD:/data\\" myregistry.com/bioinformatics/biotools:1.0 \\\\\\n    fastqc /data/sample.fastq\\n```\\n\\n**3. Push to registry:**\\n```bash\\ndocker push myregistry.com/bioinformatics/biotools:1.0\\n```\\n\\n**4. Use in Nextflow:**\\n```groovy\\nprocess FASTQC {\\n    tag \\"${meta.id}\\"\\n    label \'process_medium\'\\n\\n    conda \\"${moduleDir}/environment.yml\\"\\n    container \\"${ workflow.containerEngine == \'singularity\' && !task.ext.singularity_pull_docker_container ?\\n        \'https://depot.galaxyproject.org/singularity/fastqc:0.12.1--hdfd78af_0\' :\\n        \'biocontainers/fastqc:0.12.1--hdfd78af_0\' }\\"\\n\\n    input:\\n    tuple val(meta), path(reads)\\n\\n    output:\\n    tuple val(meta), path(\\"*.html\\"), emit: html\\n    tuple val(meta), path(\\"*.zip\\") , emit: zip\\n    path  \\"versions.yml\\"           , emit: versions\\n\\n    when:\\n    task.ext.when == null || task.ext.when\\n\\n    script:\\n    def args          = task.ext.args ?: \'\'\\n    def prefix        = task.ext.prefix ?: \\"${meta.id}\\"\\n    // Make list of old name and new name pairs to use for renaming in the bash while loop\\n    def old_new_pairs = reads instanceof Path || reads.size() == 1 ? [[ reads, \\"${prefix}.${reads.extension}\\" ]] : reads.withIndex().collect { entry, index -> [ entry, \\"${prefix}_${index + 1}.${entry.extension}\\" ] }\\n    def rename_to     = old_new_pairs*.join(\' \').join(\' \')\\n    def renamed_files = old_new_pairs.collect{ _old_name, new_name -> new_name }.join(\' \')\\n\\n    // The total amount of allocated RAM by FastQC is equal to the number of threads defined (--threads) time the amount of RAM defined (--memory)\\n    // https://github.com/s-andrews/FastQC/blob/1faeea0412093224d7f6a07f777fad60a5650795/fastqc#L211-L222\\n    // Dividing the task.memory by task.cpu allows to stick to requested amount of RAM in the label\\n    def memory_in_mb = task.memory ? task.memory.toUnit(\'MB\') / task.cpus : null\\n    // FastQC memory value allowed range (100 - 10000)\\n    def fastqc_memory = memory_in_mb > 10000 ? 10000 : (memory_in_mb < 100 ? 100 : memory_in_mb)\\n\\n    \\"\\"\\"\\n    printf \\"%s %s\\\\\\\\n\\" ${rename_to} | while read old_name new_name; do\\n        [ -f \\"\\\\${new_name}\\" ] || ln -s \\\\$old_name \\\\$new_name\\n    done\\n\\n    fastqc \\\\\\\\\\n        ${args} \\\\\\\\\\n        --threads ${task.cpus} \\\\\\\\\\n        --memory ${fastqc_memory} \\\\\\\\\\n        ${renamed_files}\\n\\n    cat <<-END_VERSIONS > versions.yml\\n    \\"${task.process}\\":\\n        fastqc: \\\\$( fastqc --version | sed \'/FastQC v/!d; s/.*v//\' )\\n    END_VERSIONS\\n    \\"\\"\\"\\n\\n    stub:\\n    def prefix = task.ext.prefix ?: \\"${meta.id}\\"\\n    \\"\\"\\"\\n    touch ${prefix}.html\\n    touch ${prefix}.zip\\n\\n    cat <<-END_VERSIONS > versions.yml\\n    \\"${task.process}\\":\\n        fastqc: \\\\$( fastqc --version | sed \'/FastQC v/!d; s/.*v//\' )\\n    END_VERSIONS\\n    \\"\\"\\"\\n}\\n```\\n\\n### Advantages of This Approach\\n\\n| Feature             | conda.yml + Micromamba  | Traditional Conda Dockerfile |\\n| ------------------- | ----------------------- | ---------------------------- |\\n| **Build speed**     | Very fast (10-100x)     | Slow                         |\\n| **Image size**      | ~200MB for tools        | ~1-2GB                       |\\n| **Reproducibility** | Explicit lock file      | Dependency drift possible    |\\n| **Version pinning** | Easy in YAML            | Inline in Dockerfile         |\\n| **Readability**     | Clear, readable YAML    | Complex RUN statements       |\\n| **Channel support** | Multiple channels       | Limited                      |\\n| **Maintenance**     | Update conda.yml easily | Rebuild entire Dockerfile    |\\n\\n### Saving and Sharing Explicit Lock Files\\n\\nThe generated `environment.lock` file is extremely valuable. Save it in your project:\\n\\n```bash\\n# Extract from build output and save\\ndocker build . 2>&1 | grep -A 1000 \\"CONDA_LOCK_START\\" | grep -B 1000 \\"CONDA_LOCK_END\\" > environment.lock\\n```\\n\\nLater, reproduce the exact environment:\\n```bash\\n# Rebuild from locked versions\\nmicromamba install -y -n base --file environment.lock\\n```\\n\\n### Pixi: The Next Generation Tool for Conda-Based Containers\\n\\nPixi is an emerging tool that combines the best aspects of Conda, Mamba, and lock file management into a single, ultra-fast solution. It\'s gaining traction in bioinformatics for building reproducible containers.\\n\\n**Why Pixi?**\\n- **Fastest:** Even faster than Micromamba for dependency resolution\\n- **Automatic lock files:** Lock files are created automatically, no manual export needed\\n- **Single-step reproducibility:** One tool handles everything from installation to reproducibility\\n- **Cross-platform:** Works seamlessly across different architectures and systems\\n- **Modern approach:** Designed from scratch with containerization in mind\\n\\n**Example Dockerfile with Pixi:**\\n\\n```dockerfile\\nARG PIXI_VERSION=0.63.2\\nARG BASE_IMAGE=debian:bookworm-slim\\n\\nFROM --platform=linux/amd64 ubuntu:24.04 AS builder\\nARG PIXI_VERSION\\nRUN apt-get update && apt-get install -y curl\\nRUN curl -Ls \\\\\\n    \\"https://github.com/prefix-dev/pixi/releases/download/v${PIXI_VERSION}/pixi-$(uname -m)-unknown-linux-musl\\" \\\\\\n    -o /pixi && chmod +x /pixi\\nRUN /pixi --version\\n\\nFROM --platform=linux/amd64 $BASE_IMAGE\\n\\n# Add appuser with UID 1000\\nRUN useradd -u 1000 -m -s /bin/bash appuser\\nUSER appuser\\nWORKDIR /home/appuser\\n\\n# Copy pixi from builder\\nCOPY --from=builder --chown=appuser:appuser --chmod=0555 /pixi /usr/local/bin/pixi\\n\\n# Set up pixi with conda channels\\nENV PATH=\\"/home/appuser/.pixi/bin:${PATH}\\"\\nRUN pixi config set default-channels \'[\\"conda-forge\\", \\"bioconda\\"]\' --global\\n\\n# Install tools using pixi\\nRUN pixi global install fastqc=0.12.1 samtools=1.18 bwa=0.7.17\\n\\n# Clean cache\\nRUN pixi clean cache -y\\n\\nWORKDIR /data\\nCMD [\\"/bin/bash\\"]\\n```\\n\\n**Key Pixi Features in the Dockerfile:**\\n- **Multi-stage build:** Builder stage compiles Pixi, final stage is lean\\n- **User management:** Non-root appuser for security\\n- **Global channel setup:** Pre-configures conda-forge and bioconda channels\\n- **Direct package installation:** `pixi global install` installs and locks versions automatically\\n- **Automatic lock file:** Lock file is generated implicitly, ensuring reproducibility\\n- **Cache cleaning:** `pixi clean cache -y` removes temporary files\\n\\n**Comparing Container Tools:**\\n\\n| Feature              | Conda           | Micromamba     | Pixi         |\\n| -------------------- | --------------- | -------------- | ------------ |\\n| **Speed**            | Slow (baseline) | 10-100x faster | 100x+ faster |\\n| **Lock file**        | Manual export   | Manual export  | Automatic    |\\n| **Image size**       | Large           | Small          | Small        |\\n| **Reproducibility**  | Good            | Excellent      | Excellent    |\\n| **Complexity**       | High            | Medium         | Low          |\\n| **Learning curve**   | Steep           | Medium         | Low          |\\n| **Production-ready** | Yes             | Yes            | Emerging     |\\n\\n**Summary:**\\n- Use `conda.yml` with Micromamba for established, proven workflows\\n- Use Pixi for cutting-edge projects requiring maximum speed and simplicity\\n- Always include `procps-ng` for Nextflow compatibility (regardless of tool)\\n- Generate/maintain explicit lock files for reproducibility\\n- Pin exact versions to prevent dependency drift\\n- Multi-channel support (conda-forge + bioconda) handles complex dependencies\\n\\n---\\n\\n## 5. Finding Pre-Built Containers: Before Building from Scratch\\n\\nBefore you spend time building Docker images from scratch, check if your tool already exists in a public container registry. Most bioinformatics tools have pre-built, maintained containers ready to use. This is the **fastest and easiest approach** when available.\\n\\n### The Container Search Strategy\\n\\nAlways follow this order:\\n1. **Check BioContainers** (https://biocontainers.pro/registry) - Most comprehensive bioinformatics registry\\n2. **Check Seqera Containers** (https://seqera.io/containers/) - Multi-tool combinations, modern builds\\n3. **Build yourself** - Only if tool doesn\'t exist\\n\\n### 1. BioContainers (Primary Source)\\n\\n**BioContainers** automatically builds and maintains Docker containers for every bioconda package. This is the most comprehensive source for bioinformatics tools.\\n\\n**URL:** https://biocontainers.pro/registry\\n\\n**How to find and use a tool:**\\n\\n```bash\\n# Search for FastQC\\ndocker search quay.io/biocontainers/fastqc\\n\\n# Pull a specific version\\ndocker pull quay.io/biocontainers/fastqc:0.12.1--hdfd78af_1\\n\\n# Run the tool\\ndocker run --rm -v \\"$PWD:/data\\" quay.io/biocontainers/fastqc:0.12.1--hdfd78af_1 \\\\\\n    fastqc /data/sample.fastq\\n```\\n\\n![biocontainer](./imgs/biocontainer_container_registry.png)\\n\\n**Advantages:**\\n- \u2705 Automatically updated when bioconda packages update\\n- \u2705 Single-tool containers (clean, minimal)\\n- \u2705 Stable and widely tested\\n- \u2705 Available for most bioinformatics tools (~10,000+)\\n- \u2705 Free and publicly accessible\\n\\n**Limitations:**\\n- One tool per image (no multi-tool combinations)\\n- Must manually orchestrate multiple containers for pipelines\\n\\n**Example with multiple tools in a workflow:**\\n\\n```bash\\n# Align reads with BWA\\ndocker run --rm -v \\"$PWD:/data\\" quay.io/biocontainers/bwa:0.7.17--h5bf99c6_8 \\\\\\n    bwa mem /data/ref.fa /data/reads.fq > /data/aligned.sam\\n\\n# Sort BAM with Samtools\\ndocker run --rm -v \\"$PWD:/data\\" quay.io/biocontainers/samtools:1.18--hd87286a_0 \\\\\\n    samtools sort /data/aligned.sam > /data/aligned.bam\\n```\\n\\n### 2. Seqera Containers (Also support Multi-Tool Option)\\n\\n**Seqera Containers** provides a registry of pre-built containers that often combine multiple tools, perfect for complex workflows.\\n\\n**URL:** https://seqera.io/containers/\\n\\n**How to find and use containers:**\\n\\n1. Visit https://seqera.io/containers/\\n2. Search for your tool(s)\\n3. Add multiple tools if needed (e.g., `fastqc` + `samtools` together)\\n4. Select your architecture (linux/amd64 or linux/arm64)\\n5. Get the container reference\\n\\n**Example:**\\n```bash\\n# Pull a multi-tool container\\ndocker pull wave.seqera.io/library/fastqc:0.12.1-samtools_1.18\\n\\n# Or use in Nextflow\\nprocess analyzeReads {\\n    container \'wave.seqera.io/library/fastqc:0.12.1-samtools_1.18\'\\n    script:\\n    \'\'\'\\n    fastqc input.fastq\\n    samtools view input.bam\\n    \'\'\'\\n}\\n```\\n\\n![seqera_container](./imgs/seqera_container_registry.png)\\n**Advantages:**\\n- \u2705 Combines multiple tools in one image\\n- \u2705 Modern infrastructure\\n- \u2705 Web interface for easy discovery\\n- \u2705 Automatic lock file generation\\n- \u2705 Multi-architecture support\\n\\n**Limitations:**\\n- Fewer pre-built combinations than BioContainers\\n- Relies on Seqera infrastructure\\n\\n### 3. When to Build from Scratch\\n\\nOnly build your own container if:\\n\\n1. **Tool doesn\'t exist** in any public registry\\n2. **Version not available** and you need an older/newer version\\n3. **Custom modifications** required (additional dependencies, scripts)\\n4. **Performance optimizations** needed for specific hardware\\n\\n**Example scenario:**\\n```bash\\n# Check if tool exists\\ndocker pull quay.io/biocontainers/myraretool:latest  # \u274c Not found\\n\\n# Check Seqera Containers\\n# Visit https://seqera.io/containers/ - (tool not found)\\n\\n# \u2192 NOW you build it yourself using Section 4 approach\\ndocker build -t myregistry.com/myraretool:1.0 .\\ndocker push myregistry.com/myraretool:1.0\\n```\\n\\n### Quick Reference: Where to Find Each Tool\\n\\n| Tool Type                                               | Primary Location                         | Alternative                   | Fallback       |\\n| ------------------------------------------------------- | ---------------------------------------- | ----------------------------- | -------------- |\\n| **Common bioinformatics** (FastQC, SAMtools, BWA, etc.) | https://biocontainers.pro/registry       | https://seqera.io/containers/ | Build yourself |\\n| **R/Bioconductor packages**                             | https://biocontainers.pro/registry       | https://seqera.io/containers/ | Build yourself |\\n| **Python bioinformatics**                               | https://biocontainers.pro/registry       | https://seqera.io/containers/ | Build yourself |\\n| **Specialized tools**                                   | https://biocontainers.pro/registry first | GitHub releases               | Build yourself |\\n\\n### Practical Workflow: Search \u2192 Pull \u2192 Use\\n\\n```bash\\n#!/bin/bash\\n# 1. Search for tool in BioContainers\\nTOOL=\\"fastqc\\"\\nVERSION=\\"0.12.1\\"\\necho \\"Searching for $TOOL:$VERSION in BioContainers...\\"\\ndocker pull quay.io/biocontainers/$TOOL:$VERSION--* 2>/dev/null && \\\\\\n    echo \\"\u2713 Found in BioContainers!\\" && \\\\\\n    exit 0\\n\\n# 2. Try Seqera Containers\\necho \\"Trying Seqera Containers...\\"\\n# (Check https://seqera.io/containers/ manually or via API)\\n\\n# 3. Build yourself\\necho \\"\u2717 Not found in public registries. Building from scratch...\\"\\n# Use Section 4 approach with Micromamba or Pixi\\ndocker build -t myregistry.com/$TOOL:$VERSION .\\n```\\n\\n### Best Practices for Using Pre-Built Containers\\n\\n1. **Pin exact versions:**\\n   ```bash\\n   \u2713 Good: docker pull quay.io/biocontainers/samtools:1.18--hd87286a_0\\n   \u2717 Bad:  docker pull quay.io/biocontainers/samtools:latest\\n   ```\\n\\n2. **Check build hash in Nextflow:**\\n   ```groovy\\n   process myTask {\\n       // Include digest for reproducibility\\n       container \'quay.io/biocontainers/samtools:1.18--hd87286a_0@sha256:abc123...\'\\n       script: \'samtools view input.bam\'\\n   }\\n   ```\\n\\n3. **Keep local cache for offline work:**\\n   ```bash\\n   docker pull quay.io/biocontainers/samtools:1.18--hd87286a_0\\n   docker tag quay.io/biocontainers/samtools:1.18--hd87286a_0 \\\\\\n       myregistry.com/samtools:1.18\\n   docker push myregistry.com/samtools:1.18\\n   ```\\n\\n### Summary: Container Registry Decision Tree\\n\\n```\\nDo you need a bioinformatics tool in a container?\\n\u2502\\n\u251c\u2500\u2192 Search https://biocontainers.pro/registry (BioContainers)\\n\u2502   \u251c\u2500\u2192 Found? Use it \u2713\\n\u2502   \u2514\u2500\u2192 Not found?\\n\u2502       \u2502\\n\u2502       \u251c\u2500\u2192 Search https://seqera.io/containers/ (Seqera Containers)\\n\u2502       \u2502   \u251c\u2500\u2192 Found? Use it \u2713\\n\u2502       \u2502   \u2514\u2500\u2192 Not found?\\n\u2502       \u2502       \u2502\\n\u2502       \u2502       \u2514\u2500\u2192 Build using Section 4\\n\u2502       \u2502           (Micromamba or Pixi) \u2713\\n```\\n\\n**Key Takeaway:**\\n- **Check existing registries first** - saves time and uses community-maintained containers\\n- **BioContainers (https://biocontainers.pro/registry) is your first stop** - most comprehensive bioinformatics registry\\n- **Seqera (https://seqera.io/containers/) for multi-tool combinations** - modern approach for complex workflows\\n- **Build yourself only as last resort** - but you now have the tools (Section 4) to do it well"},{"id":"bioinformatics-workflow-template-ci-cd-best-practices","metadata":{"permalink":"/river-docs/blog/bioinformatics-workflow-template-ci-cd-best-practices","source":"@site/blog/2026-02/2026-02-04.md","title":"Bioinformatics Workflow Template: Standardizing Python Pipelines with Modular Design","description":"Building reproducible bioinformatics pipelines is hard. Every project starts from scratch with its own testing, CI/CD, and deployment strategy. What if you could clone a template, add your analysis tools, and be ready to go?","date":"2026-02-04T00:00:00.000Z","tags":[{"inline":true,"label":"ci-cd","permalink":"/river-docs/blog/tags/ci-cd"},{"inline":true,"label":"bioinformatics","permalink":"/river-docs/blog/tags/bioinformatics"},{"inline":true,"label":"python","permalink":"/river-docs/blog/tags/python"},{"inline":true,"label":"docker","permalink":"/river-docs/blog/tags/docker"},{"inline":true,"label":"testing","permalink":"/river-docs/blog/tags/testing"},{"inline":true,"label":"template","permalink":"/river-docs/blog/tags/template"},{"inline":true,"label":"reproducibility","permalink":"/river-docs/blog/tags/reproducibility"}],"readingTime":12.29,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"bioinformatics-workflow-template-ci-cd-best-practices","title":"Bioinformatics Workflow Template: Standardizing Python Pipelines with Modular Design","authors":["river"],"tags":["ci-cd","bioinformatics","python","docker","testing","template","reproducibility"],"image":"./imgs/intro.png"},"unlisted":false,"prevItem":{"title":"Containers in Bioinformatics: Community Tooling and Efficient Docker Building","permalink":"/river-docs/blog/bioinformatics-containers-build-efficient-docker"},"nextItem":{"title":"Running GitHub Actions Locally with act: 5x Faster Development","permalink":"/river-docs/blog/cicd-bioinformatics-act-local-github-action"}},"content":"Building reproducible bioinformatics pipelines is hard. Every project starts from scratch with its own testing, CI/CD, and deployment strategy. **What if you could clone a template, add your analysis tools, and be ready to go?**\\n\\nThis post introduces a **standardized bioinformatics workflow template** featuring consistent testing, CI/CD, and project structure. Developed from real production experience with `bioinfor-wf-template`, this template reduces setup time from days to minutes, ensures research reproducibility, and promotes modular, reusable code. It is Python-based and ideal for proof-of-concept projects. Support for more advanced and widely adopted bioinformatics frameworks (such as Snakemake and Nextflow) is planned, applying the same core principles while leveraging their native testing systems.\\n\\n\x3c!-- truncate --\x3e\\n\\n## The Problem: Bioinformatics Projects Start From Zero\\n\\nMost bioinformatics projects face similar challenges:\\n\\n**No Standard Structure**\\n- Where do I put my scripts? `src/`? `bin/`? `scripts/`?\\n- How do I organize apps vs workflows?\\n- Where do tests go?\\n\\n**Testing Nightmare**\\n- Unit tests for data validation\\n- End-to-end (E2E) tests with real data\\n- Different frameworks need different test approaches\\n- Docker container testing\\n\\n**CI/CD Inconsistency**\\n- Each project has its own GitHub Actions workflow\\n- No standard for running tests on changes\\n- Hard to scale to 50+ apps and workflows\\n\\n**Onboarding Friction**\\n- New team members spend days setting up\\n- \\"How do I run the tests?\\" \u2192 No clear answer\\n- \\"Where\'s the documentation?\\" \u2192 Scattered or missing\\n\\n---\\n\\n## The Solution: Standardized Template\\n\\nIntroducing a **production-ready bioinformatics workflow template** with:\\n- \u2705 Clear project structure for apps, workflows, and tests\\n- \u2705 Unified testing framework (pytest) for all modules\\n- \u2705 Smart CI/CD that only runs tests for changed files\\n- \u2705 Docker integration for reproducible environments\\n- \u2705 One-command local testing with `act`\\n- \u2705 Type hints and validation for data integrity\\n- \u2705 Scales from 5 apps to 50+ production pipelines\\n\\n---\\n\\n## Part 1: Understanding the Template Structure\\n\\n### Repository Layout\\n\\n```\\nbioinfor-wf-template/\\n\u251c\u2500\u2500 apps/                          # Individual bioinformatics tools\\n\u2502   \u251c\u2500\u2500 fastqc/                    # Quality control\\n\u2502   \u2502   \u251c\u2500\u2500 main.py                # Implementation\\n\u2502   \u2502   \u251c\u2500\u2500 tests/                 # Unit + E2E tests\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_fastq_in_pairs.py      # Unit tests\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 test_e2e.py                 # Integration tests\\n\u2502   \u2502   \u2514\u2500\u2500 Makefile               # Local testing commands\\n\u2502   \u251c\u2500\u2500 multiqc/                   # Aggregation\\n\u2502   \u2502   \u251c\u2500\u2500 main.py\\n\u2502   \u2502   \u251c\u2500\u2500 tests/\\n\u2502   \u2502   \u2514\u2500\u2500 Makefile\\n\u2502   \u2514\u2500\u2500 [your-tool]/               # Add more tools\\n\u2502\\n\u251c\u2500\u2500 workflows/                      # Composed pipelines\\n\u2502   \u251c\u2500\u2500 qc/                        # Quality control workflow\\n\u2502   \u2502   \u251c\u2500\u2500 main.py                # Orchestrates apps\\n\u2502   \u2502   \u251c\u2500\u2500 tests/\\n\u2502   \u2502   \u2514\u2500\u2500 Makefile\\n\u2502   \u2514\u2500\u2500 [your-pipeline]/           # Add more workflows\\n\u2502\\n\u251c\u2500\u2500 .github/\\n\u2502   \u2514\u2500\u2500 workflows/\\n\u2502       \u2514\u2500\u2500 tests.yaml             # GitHub Actions CI/CD\\n\u2502\\n\u251c\u2500\u2500 conftest.py                    # Shared pytest configuration\\n\u251c\u2500\u2500 pytest.ini                     # Pytest settings\\n\u251c\u2500\u2500 requirements.txt               # Dependencies\\n\u251c\u2500\u2500 pixi.toml                      # (Optional) Reproducible environment\\n\u2514\u2500\u2500 README.md\\n```\\n\\n**Key insight**: Apps are reusable building blocks, workflows compose them into pipelines.\\n\\n### Example App: FastQC Quality Control\\n\\n**apps/fastqc/main.py**\\n```python\\ndef validate_fastq_files(fastq1: str, fastq2: str):\\n    \\"\\"\\"Validate paired FASTQ files match naming convention.\\"\\"\\"\\n    base1 = os.path.basename(fastq1)\\n    base2 = os.path.basename(fastq2)\\n    expected_base2 = base1.replace(\\"_R1_\\", \\"_R2_\\")\\n    if base2 != expected_base2:\\n        raise ValueError(f\\"Expected \'{expected_base2}\', got \'{base2}\'\\")\\n\\ndef run_fastqc(fastq1: str, fastq2: str, output_dir: str):\\n    \\"\\"\\"Run FastQC on paired FASTQ files using Docker.\\"\\"\\"\\n    validate_fastq_files(fastq1, fastq2)\\n    cmd = [\\n        \\"docker\\", \\"run\\", \\"--rm\\",\\n        \\"-v\\", f\\"{cwd}:{cwd}\\",\\n        \\"biocontainers/fastqc:v0.11.9_cv8\\",\\n        \\"fastqc\\", fastq1, fastq2, \\"--outdir\\", output_dir\\n    ]\\n    subprocess.run(cmd, check=True)\\n```\\n\\n**Key patterns**:\\n- \u2705 Validation before execution\\n- \u2705 Docker isolation (reproducible)\\n- \u2705 Clear function signatures\\n- \u2705 Type hints for IDE support\\n\\n### Testing Strategy: Unit + E2E\\n\\n**apps/fastqc/tests/test_fastq_in_pairs.py** (Unit tests)\\n```python\\ndef test_validate_fastq_files_valid_pair():\\n    \\"\\"\\"Test that valid paired files pass validation.\\"\\"\\"\\n    fastq1 = \\"/path/sample_R1_001.fastq.gz\\"\\n    fastq2 = \\"/path/sample_R2_001.fastq.gz\\"\\n    validate_fastq_files(fastq1, fastq2)  # Should not raise\\n\\ndef test_validate_fastq_files_invalid_pair():\\n    \\"\\"\\"Test that mismatched files raise error.\\"\\"\\"\\n    fastq1 = \\"/path/sample_R1_001.fastq.gz\\"\\n    fastq2 = \\"/path/sample_R3_001.fastq.gz\\"\\n    with pytest.raises(ValueError):\\n        validate_fastq_files(fastq1, fastq2)\\n```\\n\\n**apps/fastqc/tests/test_e2e.py** (End-to-end tests)\\n```python\\n@pytest.fixture\\ndef dummy_fastq_files():\\n    \\"\\"\\"Create minimal FASTQ files for testing.\\"\\"\\"\\n    fastq1.write_text(\\"@SEQ_ID\\\\nGATTT...\\\\n+\\\\nIIIII...\\\\n\\")\\n    fastq2.write_text(\\"@SEQ_ID\\\\nGATTT...\\\\n+\\\\nIIIII...\\\\n\\")\\n    return str(fastq1), str(fastq2)\\n\\n@pytest.mark.skipif(\\n    not subprocess.run([\\"docker\\", \\"--version\\"]).returncode == 0,\\n    reason=\\"Docker is not available\\"\\n)\\ndef test_run_fastqc_e2e(dummy_fastq_files, data_dir):\\n    \\"\\"\\"Test actual FastQC execution with real (dummy) data.\\"\\"\\"\\n    fastq1, fastq2 = dummy_fastq_files\\n    run_fastqc(fastq1, fastq2, str(data_dir / \\"results\\"))\\n    # Assert output files exist\\n```\\n\\n**Test classification**:\\n- **Unit tests** (`test_fastq_in_pairs.py`): Always fast, no Docker needed\\n- **E2E tests** (`test_e2e.py`): Slower, require Docker, test actual tools\\n\\n---\\n\\n## Part 2: Local Testing Workflow\\n\\n### Quick Setup\\n\\n```bash\\n# Clone the template\\ngit clone git@github.com:riverxdata/bioinfor-wf-template.git my-pipeline\\ncd my-pipeline\\n\\n# Install dependencies\\npip install -r requirements.txt\\n\\n# Run all tests locally\\npytest -v\\n\\n# Or with Pixi (recommended)\\npixi global install act\\npixi run tests\\n```\\n\\n### Testing Commands with Make\\n\\nEach app and workflow has a Makefile for consistent commands:\\n\\n**apps/fastqc/Makefile**\\n```makefile\\n.PHONY: unittest e2e\\n\\nunittest:\\n\\tpytest -v -k \\"not e2e\\" tests/\\n\\ne2e:\\n\\tpytest -v -k \\"e2e\\" tests/\\n```\\n\\n**Run tests locally:**\\n```bash\\n# Unit tests only (fast - 10 seconds)\\nmake -C apps/fastqc unittest\\n\\n# End-to-end tests (slower - 2 minutes)\\nmake -C apps/fastqc e2e\\n\\n# All tests\\nmake -C apps/fastqc unittest e2e\\n```\\n\\n**Run tests for entire workflows:**\\n```bash\\n# All unit tests in all apps\\nfor app in apps/*; do make -C $app unittest; done\\n\\n# All E2E tests in all workflows\\nfor workflow in workflows/*; do make -C $workflow e2e; done\\n```\\n\\n---\\n\\n## Part 3: Smart CI/CD with GitHub Actions\\n\\n### GitHub Actions Workflow Analysis\\n\\n**`.github/workflows/tests.yaml`** intelligently runs tests only for changed files:\\n\\n```yaml\\njobs:\\n  unittest_and_e2e:\\n    runs-on: ubuntu-latest\\n    steps:\\n      # 1. Detect changed files\\n      - uses: tj-actions/changed-files@v46.0.5\\n        id: changed-files\\n        with:\\n          files: apps/**\\n          base: ${{ github.event.pull_request.base.ref }}\\n      \\n      # 2. Run unit tests for changed apps\\n      - name: Unit tests\\n        if: steps.changed-files.outputs.all_changed_files != \'\'\\n        run: |\\n          for app in ${{ steps.changed-files.outputs.all_changed_files }}; do\\n            make -C $app unittest\\n          done\\n      \\n      # 3. Run E2E tests for changed apps\\n      - name: E2E tests\\n        if: steps.changed-files.outputs.all_changed_files != \'\'\\n        run: |\\n          for app in ${{ steps.changed-files.outputs.all_changed_files }}; do\\n            make -C $app e2e\\n          done\\n      \\n      # 4. Run all workflow E2E tests\\n      - name: Workflow tests\\n        run: |\\n          for workflow in workflows/*; do\\n            make -C $workflow e2e\\n          done\\n```\\n\\n### How It Works\\n\\n**Scenario 1: Edit `apps/fastqc/main.py`**\\n```\\nGitHub detects change \u2192 Run only fastqc tests \u2192 ~30 seconds\\n```\\n\\n**Scenario 2: Edit `workflows/qc/main.py`**\\n```\\nGitHub detects change \u2192 Run qc workflow E2E \u2192 ~2 minutes\\nIncludes fastqc + multiqc + orchestration\\n```\\n\\n**Scenario 3: Edit `README.md`**\\n```\\nGitHub detects no code changes \u2192 Skip all tests\\n```\\n\\n**Performance benefit**: \\n- Without optimization: All tests run (~10 min) for every PR\\n- With optimization: Only affected tests run (~30 sec - 2 min)\\n- **Result: 5-10x faster feedback** \ud83d\ude80\\n\\n---\\n\\n## Part 4: Example Apps and Patterns\\n\\nThe template works for any bioinformatics tool wrapped in Python. Here are common patterns:\\n\\n### Pattern 1: Docker-Based Tool Wrapper\\n\\nMost bioinformatics tools are containerized. Wrap them in Python:\\n\\n```python\\n# apps/bowtie2/main.py\\ndef run_bowtie2_alignment(reference, fastq1, fastq2, output_bam):\\n    \\"\\"\\"Align reads using Bowtie2 via Docker.\\"\\"\\"\\n    cmd = [\\n        \\"docker\\", \\"run\\", \\"--rm\\",\\n        \\"-v\\", f\\"{Path.cwd()}:{Path.cwd()}\\",\\n        \\"biocontainers/bowtie2:v2.5.1_cv1\\",\\n        \\"bowtie2-build\\", reference, \\"index\\",\\n        \\"-x\\", \\"index\\", \\"-1\\", fastq1, \\"-2\\", fastq2, \\"-S\\", output_bam\\n    ]\\n    subprocess.run(cmd, check=True)\\n\\n# apps/bowtie2/tests/test_e2e.py\\ndef test_bowtie2_alignment_e2e(reference_fasta, fastq_files):\\n    \\"\\"\\"Test alignment with real (dummy) data.\\"\\"\\"\\n    run_bowtie2_alignment(*reference_fasta, *fastq_files, \\"output.bam\\")\\n    assert Path(\\"output.bam\\").exists()\\n```\\n\\n### Pattern 2: Python Library Wrapper\\n\\nSome tools have Python packages. Use them directly:\\n\\n```python\\n# apps/cutadapt/main.py\\nimport cutadapt\\n\\ndef trim_adapters(fastq1, fastq2, output_dir):\\n    \\"\\"\\"Trim adapters from FASTQ files.\\"\\"\\"\\n    Path(output_dir).mkdir(exist_ok=True)\\n    \\n    # Use cutadapt Python API\\n    adapter = \\"AGATCGGAAGAGC\\"  # Common Illumina adapter\\n    results = cutadapt.main([\\n        \\"-a\\", adapter,\\n        \\"-A\\", adapter,\\n        \\"-o\\", f\\"{output_dir}/trimmed_R1.fastq.gz\\",\\n        \\"-p\\", f\\"{output_dir}/trimmed_R2.fastq.gz\\",\\n        fastq1, fastq2\\n    ])\\n    return results\\n\\n# apps/cutadapt/tests/test_e2e.py\\ndef test_trim_adapters_e2e(fastq_files, tmp_path):\\n    \\"\\"\\"Test adapter trimming.\\"\\"\\"\\n    trim_adapters(fastq_files[0], fastq_files[1], str(tmp_path))\\n    assert (tmp_path / \\"trimmed_R1.fastq.gz\\").exists()\\n    assert (tmp_path / \\"trimmed_R2.fastq.gz\\").exists()\\n```\\n\\n### Pattern 3: Custom Analysis\\n\\nWrite your own analysis tools:\\n\\n```python\\n# apps/gene_counter/main.py\\nimport pandas as pd\\n\\ndef count_genes(bam_file, annotation_gtf, output_csv):\\n    \\"\\"\\"Count reads per gene from BAM file.\\"\\"\\"\\n    # Load annotation\\n    gtf = pd.read_csv(annotation_gtf, sep=\\"\\\\t\\", comment=\\"#\\")\\n    \\n    # Count using samtools + Python\\n    import pysam\\n    bam = pysam.AlignmentFile(bam_file)\\n    \\n    counts = {}\\n    for read in bam:\\n        gene = read.get_tag(\\"XG\\") if read.has_tag(\\"XG\\") else \\"unknown\\"\\n        counts[gene] = counts.get(gene, 0) + 1\\n    \\n    # Save results\\n    df = pd.DataFrame(list(counts.items()), columns=[\\"gene\\", \\"count\\"])\\n    df.to_csv(output_csv, index=False)\\n    \\n    return df\\n\\n# apps/gene_counter/tests/test_e2e.py\\ndef test_gene_counting_e2e(bam_file, annotation_gtf, tmp_path):\\n    \\"\\"\\"Test gene counting.\\"\\"\\"\\n    output_csv = tmp_path / \\"counts.csv\\"\\n    count_genes(bam_file, annotation_gtf, str(output_csv))\\n    \\n    results = pd.read_csv(output_csv)\\n    assert len(results) > 0\\n    assert \\"count\\" in results.columns\\n```\\n\\n---\\n\\n## Part 5: Adding New Apps and Workflows\\n\\n### Template for New Python App\\n\\n**Create the structure:**\\n```bash\\nmkdir -p apps/myapp/tests\\ntouch apps/myapp/__init__.py\\ntouch apps/myapp/main.py\\ntouch apps/myapp/tests/test_unit.py\\ntouch apps/myapp/tests/test_e2e.py\\ntouch apps/myapp/Makefile\\n```\\n\\n**apps/myapp/main.py**\\n```python\\nimport subprocess\\nfrom pathlib import Path\\n\\ndef validate_input(input_file: str):\\n    \\"\\"\\"Validate input data format.\\"\\"\\"\\n    if not Path(input_file).exists():\\n        raise FileNotFoundError(f\\"Input file not found: {input_file}\\")\\n\\ndef run_myapp(input_file: str, output_dir: str):\\n    \\"\\"\\"Run my bioinformatics tool.\\"\\"\\"\\n    Path(output_dir).mkdir(parents=True, exist_ok=True)\\n    \\n    cmd = [\\n        \\"docker\\", \\"run\\", \\"--rm\\",\\n        \\"-v\\", f\\"{Path.cwd()}:{Path.cwd()}\\",\\n        \\"myregistry/myapp:latest\\",\\n        \\"myapp\\", input_file, \\"--output\\", output_dir\\n    ]\\n    subprocess.run(cmd, check=True)\\n```\\n\\n**apps/myapp/tests/test_unit.py**\\n```python\\nfrom apps.myapp.main import validate_input\\nimport pytest\\n\\ndef test_validate_input_exists():\\n    \\"\\"\\"Test validation of existing file.\\"\\"\\"\\n    # Create temp file\\n    with open(\\"/tmp/test_input.txt\\", \\"w\\") as f:\\n        f.write(\\"test data\\")\\n    \\n    # Should not raise\\n    validate_input(\\"/tmp/test_input.txt\\")\\n\\ndef test_validate_input_missing():\\n    \\"\\"\\"Test validation of missing file.\\"\\"\\"\\n    with pytest.raises(FileNotFoundError):\\n        validate_input(\\"/nonexistent/file.txt\\")\\n```\\n\\n**apps/myapp/tests/test_e2e.py**\\n```python\\nimport pytest\\nfrom pathlib import Path\\nimport subprocess\\nfrom apps.myapp.main import run_myapp\\n\\n@pytest.mark.skipif(\\n    not subprocess.run([\\"docker\\", \\"--version\\"]).returncode == 0,\\n    reason=\\"Docker not available\\"\\n)\\ndef test_run_myapp_e2e(tmp_path):\\n    \\"\\"\\"Test complete myapp execution.\\"\\"\\"\\n    input_file = tmp_path / \\"input.txt\\"\\n    input_file.write_text(\\"test data\\")\\n    \\n    output_dir = tmp_path / \\"output\\"\\n    run_myapp(str(input_file), str(output_dir))\\n    \\n    # Verify output\\n    assert output_dir.exists()\\n    assert (output_dir / \\"result.txt\\").exists()\\n```\\n\\n**apps/myapp/Makefile**\\n```makefile\\n.PHONY: unittest e2e\\n\\nunittest:\\n\\tpytest -v -k \\"not e2e\\" tests/\\n\\ne2e:\\n\\tpytest -v -k \\"e2e\\" tests/\\n```\\n\\n### Template for New Workflow\\n\\n**Create the structure:**\\n```bash\\nmkdir -p workflows/mypipeline/tests\\ntouch workflows/mypipeline/main.py\\ntouch workflows/mypipeline/tests/test_e2e.py\\ntouch workflows/mypipeline/Makefile\\n```\\n\\n**workflows/mypipeline/main.py**\\n```python\\nimport argparse\\nfrom apps.myapp.main import run_myapp\\nfrom apps.anotherap.main import run_anotherap\\n\\ndef run_mypipeline(input_file, output_dir):\\n    \\"\\"\\"Compose myapp and anotherap into a pipeline.\\"\\"\\"\\n    \\n    # Step 1: Run first app\\n    intermediate_dir = f\\"{output_dir}/intermediate\\"\\n    run_myapp(input_file, intermediate_dir)\\n    \\n    # Step 2: Run second app with output from first\\n    final_dir = f\\"{output_dir}/final\\"\\n    run_anotherap(f\\"{intermediate_dir}/result.txt\\", final_dir)\\n    \\n    print(f\\"Pipeline complete. Results in: {final_dir}\\")\\n\\nif __name__ == \\"__main__\\":\\n    parser = argparse.ArgumentParser(description=\\"My bioinformatics pipeline\\")\\n    parser.add_argument(\\"input_file\\")\\n    parser.add_argument(\\"output_dir\\")\\n    args = parser.parse_args()\\n    \\n    run_mypipeline(args.input_file, args.output_dir)\\n```\\n\\n**workflows/mypipeline/tests/test_e2e.py**\\n```python\\nimport subprocess\\nfrom pathlib import Path\\nfrom workflows.mypipeline.main import run_mypipeline\\n\\ndef test_mypipeline_e2e(tmp_path):\\n    \\"\\"\\"Test complete pipeline execution.\\"\\"\\"\\n    input_file = tmp_path / \\"input.txt\\"\\n    input_file.write_text(\\"test data\\")\\n    \\n    output_dir = tmp_path / \\"output\\"\\n    run_mypipeline(str(input_file), str(output_dir))\\n    \\n    # Verify all outputs exist\\n    assert (output_dir / \\"intermediate\\").exists()\\n    assert (output_dir / \\"final\\").exists()\\n```\\n\\n**workflows/mypipeline/Makefile**\\n```makefile\\n.PHONY: e2e\\n\\ne2e:\\n\\tpytest -v -k \\"e2e\\" tests/\\n```\\n\\n---\\n\\n## Part 6: Reproducible Environment with Pixi\\n\\nFor maximum reproducibility, add Pixi support:\\n\\n**pixi.toml**\\n```toml\\n[project]\\nname = \\"bioinfor-wf\\"\\nversion = \\"0.1.0\\"\\nchannels = [\\"conda-forge\\", \\"bioconda\\"]\\n\\n[dependencies]\\npython = \\"3.12\\"\\npytest = \\">=7.0\\"\\nnumpy = \\">=1.24\\"\\npandas = \\">=1.5\\"\\n\\n[tasks]\\ntest = \\"pytest -v\\"\\nunittest = \\"pytest -v -k \'not e2e\'\\"\\ne2e = \\"pytest -v -k \'e2e\'\\"\\nci = \\"act push\\"\\n\\n[environments]\\ntest = { channels = [\\"conda-forge\\", \\"bioconda\\", \\"nvidia\\"], features = [\\"gpu\\"] }\\ndev = { features = [\\"dev\\"] }\\n```\\n\\n**Usage:**\\n```bash\\n# Install dependencies\\npixi install\\n\\n# Run tests\\npixi run test\\n\\n# Run GitHub Actions locally\\npixi run ci\\n```\\n\\n---\\n\\n## Benefits and Impact\\n\\n| Aspect                    | Before                | After                           |\\n| ------------------------- | --------------------- | ------------------------------- |\\n| **Onboarding time**       | 2-3 days              | 30 minutes                      |\\n| **Test setup**            | Different per project | Standardized pytest             |\\n| **CI/CD setup**           | Write from scratch    | Clone workflow                  |\\n| **Running tests locally** | \\"How do I test this?\\" | `make -C apps/fastqc unittest`  |\\n| **Code consistency**      | All over the place    | Type hints, validation enforced |\\n| **New app creation**      | Copy-paste existing   | Use template structure          |\\n| **App reusability**       | One-off scripts       | Modular, reusable components    |\\n| **Team onboarding**       | Weeks of confusion    | 30 min + documentation          |\\n\\n---\\n\\n## Best Practices for Template Adoption\\n\\n### 1. **One Function Per Purpose**\\nEach function should do one thing well:\\n```python\\n# \u274c Bad: Multiple responsibilities\\ndef process_fastq_and_align(fastq1, fastq2, ref, output):\\n    validate_fastq(fastq1, fastq2)\\n    align(fastq1, fastq2, ref, output)\\n    quality_check(output)\\n\\n# \u2705 Good: Single responsibility\\ndef validate_fastq(fastq1, fastq2): ...\\ndef run_alignment(fastq1, fastq2, ref, output): ...\\ndef quality_check(bam_file): ...\\n```\\n\\n### 2. **Always Validate Input**\\n```python\\ndef run_analysis(input_file: str):\\n    \\"\\"\\"Validate before processing.\\"\\"\\"\\n    input_path = Path(input_file)\\n    if not input_path.exists():\\n        raise FileNotFoundError(f\\"Input file not found: {input_file}\\")\\n    if input_path.stat().st_size == 0:\\n        raise ValueError(\\"Input file is empty\\")\\n```\\n\\n### 3. **Use Type Hints**\\n```python\\nfrom pathlib import Path\\nfrom typing import List, Tuple\\n\\ndef process_samples(samples: List[str], output_dir: str) -> Tuple[Path, Path]:\\n    \\"\\"\\"Type hints help catch errors early.\\"\\"\\"\\n    ...\\n```\\n\\n### 4. **Separate Unit and E2E Tests**\\n```python\\n# Unit: Fast, no Docker needed\\ndef test_validation():\\n    assert validate(valid_input) == True\\n\\n# E2E: Slow, includes Docker\\n@pytest.mark.skipif(no_docker, reason=\\"Docker required\\")\\ndef test_full_workflow_e2e():\\n    ...\\n```\\n\\n### 5. **Document Dependencies**\\nKeep `requirements.txt` updated:\\n```\\npytest>=7.0\\ndocker>=6.0\\nnumpy>=1.24\\npandas>=1.5\\n```\\n\\n---\\n\\n## Common Patterns\\n\\n### Error Handling\\n\\n```python\\nimport subprocess\\nimport sys\\n\\ndef run_command(cmd: List[str]) -> bool:\\n    \\"\\"\\"Run shell command with proper error handling.\\"\\"\\"\\n    try:\\n        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\\n        return True\\n    except subprocess.CalledProcessError as e:\\n        print(f\\"Error: Command failed with exit code {e.returncode}\\")\\n        print(f\\"stderr: {e.stderr}\\")\\n        sys.exit(1)\\n```\\n\\n### Logging and Progress\\n\\n```python\\nimport logging\\n\\nlogger = logging.getLogger(__name__)\\n\\ndef process_pipeline(samples: List[str]):\\n    \\"\\"\\"Process with logging.\\"\\"\\"\\n    for i, sample in enumerate(samples, 1):\\n        logger.info(f\\"Processing sample {i}/{len(samples)}: {sample}\\")\\n        # Process...\\n        logger.info(f\\"\u2713 Completed {sample}\\")\\n```\\n\\n### Temporary File Management\\n\\n```python\\nfrom pathlib import Path\\nimport tempfile\\n\\ndef run_with_temp_files(input_file: str) -> str:\\n    \\"\\"\\"Create and clean temporary files automatically.\\"\\"\\"\\n    with tempfile.TemporaryDirectory() as tmp_dir:\\n        tmp_path = Path(tmp_dir)\\n        \\n        # Work with temp files\\n        intermediate = tmp_path / \\"intermediate.txt\\"\\n        # ... process ...\\n        \\n        # Copy final result to output\\n        final_output = \\"final_result.txt\\"\\n        # ... copy ...\\n        \\n        # Temp dir automatically cleaned up when exiting with block\\n    return final_output\\n```\\n\\n---\\n\\n## Scaling to 50+ Apps and Workflows\\n\\nAs your template grows, follow these patterns:\\n\\n### Directory Organization\\n```\\napps/\\n\u251c\u2500\u2500 alignment/          # Group by domain\\n\u2502   \u251c\u2500\u2500 bowtie2/\\n\u2502   \u251c\u2500\u2500 bwa/\\n\u2502   \u2514\u2500\u2500 hisat2/\\n\u251c\u2500\u2500 quality_control/\\n\u2502   \u251c\u2500\u2500 fastqc/\\n\u2502   \u251c\u2500\u2500 multiqc/\\n\u2502   \u2514\u2500\u2500 qualimap/\\n\u2514\u2500\u2500 variant_calling/\\n    \u251c\u2500\u2500 gatk/\\n    \u2514\u2500\u2500 samtools/\\n\\nworkflows/\\n\u251c\u2500\u2500 rnaseq/\\n\u251c\u2500\u2500 wgs/\\n\u251c\u2500\u2500 amplicon/\\n\u2514\u2500\u2500 metagenomics/\\n```\\n\\n### Automated Testing\\n\\n```bash\\n#!/bin/bash\\n# test_all.sh - Run all tests\\n\\nfailed_tests=0\\ntotal_tests=0\\n\\nfor app in apps/*/*/; do\\n    if [ -f \\"$app/Makefile\\" ]; then\\n        total_tests=$((total_tests + 1))\\n        if ! make -C \\"$app\\" unittest; then\\n            failed_tests=$((failed_tests + 1))\\n            echo \\"\u274c Failed: $app\\"\\n        else\\n            echo \\"\u2713 Passed: $app\\"\\n        fi\\n    fi\\ndone\\n\\necho \\"Tests: $((total_tests - failed_tests))/$total_tests passed\\"\\nexit $failed_tests\\n```\\n\\n### Documentation\\n\\nKeep a central `CONTRIBUTING.md`:\\n```markdown\\n## Adding a New App\\n\\n1. Create directory: `mkdir -p apps/category/myapp/tests`\\n2. Copy template: `cp -r apps/fastqc/* apps/category/myapp/`\\n3. Edit `main.py` with your logic\\n4. Write tests in `tests/`\\n5. Update `README.md`\\n6. Create PR\\n\\n## Adding a New Workflow\\n\\n1. Create directory: `mkdir -p workflows/mypipeline/tests`\\n2. Create `main.py` that composes apps\\n3. Write E2E test in `tests/`\\n4. Add to `.github/workflows/tests.yaml`\\n```\\n\\n---\\n\\n## Key Takeaways\\n\\n1. **Template consistency** \u2192 New projects setup in 30 minutes\\n2. **Modular structure** \u2192 Apps are reusable, workflows compose them\\n3. **Testing discipline** \u2192 Unit + E2E tests, both automated\\n4. **Smart CI/CD** \u2192 Only test changed files (5-10x faster)\\n5. **Multi-framework** \u2192 Python, Nextflow, Snakemake all supported\\n6. **Reproducibility** \u2192 Docker + Pixi guarantees same environment everywhere\\n7. **Scaling** \u2192 Pattern works for 5 apps or 50+ apps\\n\\n---\\n\\n## Getting Started\\n\\n```bash\\n# Clone the template\\ngit clone git@github.com:riverxdata/bioinfor-wf-template.git my-analysis\\n\\n# Install dependencies\\ncd my-analysis\\npip install -r requirements.txt\\n\\n# Run tests\\npytest -v\\n\\n# Or with Pixi\\npixi global install act\\npixi run test\\n\\n# Or locally simulate GitHub Actions\\nact push\\n```\\n\\n---\\n\\n## References\\n\\n- [GitHub Repository](https://github.com/riverxdata/bioinfor-wf-template)\\n- [pytest Documentation](https://docs.pytest.org/)\\n- [Docker for Bioinformatics](https://biocontainers.pro/)\\n- [Nextflow Guide](https://nextflow.io/docs/latest/)\\n- [Snakemake Tutorial](https://snakemake.readthedocs.io/)\\n\\n**Start building standardized bioinformatics pipelines today! \ud83e\uddec\ud83d\ude80**"},{"id":"cicd-bioinformatics-act-local-github-action","metadata":{"permalink":"/river-docs/blog/cicd-bioinformatics-act-local-github-action","source":"@site/blog/2026-02/2026-02-03.md","title":"Running GitHub Actions Locally with act: 5x Faster Development","description":"GitHub Actions are powerful for automating bioinformatics pipelines, but waiting 5-10 minutes for each cloud run is painful during development. act lets you run GitHub Actions workflows locally on your machine in seconds, slashing feedback time by 5x.","date":"2026-02-03T00:00:00.000Z","tags":[{"inline":true,"label":"ci-cd","permalink":"/river-docs/blog/tags/ci-cd"},{"inline":true,"label":"github-actions","permalink":"/river-docs/blog/tags/github-actions"},{"inline":true,"label":"docker","permalink":"/river-docs/blog/tags/docker"},{"inline":true,"label":"bioinformatics","permalink":"/river-docs/blog/tags/bioinformatics"},{"inline":true,"label":"python","permalink":"/river-docs/blog/tags/python"},{"inline":true,"label":"devops","permalink":"/river-docs/blog/tags/devops"}],"readingTime":11.26,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"cicd-bioinformatics-act-local-github-action","title":"Running GitHub Actions Locally with act: 5x Faster Development","authors":["river"],"tags":["ci-cd","github-actions","docker","bioinformatics","python","devops"],"image":"./imgs/intro.png"},"unlisted":false,"prevItem":{"title":"Bioinformatics Workflow Template: Standardizing Python Pipelines with Modular Design","permalink":"/river-docs/blog/bioinformatics-workflow-template-ci-cd-best-practices"},"nextItem":{"title":"Machine Learning in Bioinformatics Part 1: Building KNN from Scratch","permalink":"/river-docs/blog/machine-learning-bioinformatics-part1-knn"}},"content":"GitHub Actions are powerful for automating bioinformatics pipelines, but waiting 5-10 minutes for each cloud run is painful during development. **`act`** lets you run GitHub Actions workflows **locally on your machine** in seconds, slashing feedback time by 5x.\\n\\nIn this post, we\'ll explore `act`, a command-line tool that runs GitHub Actions locally using Docker. Perfect for testing ML pipelines, gene expression analysis, and CI/CD workflows before pushing to GitHub.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Why Test GitHub Actions Locally?\\n\\nTraditional GitHub Actions workflow:\\n1. Write workflow \u2192 Push to GitHub\\n2. Wait 5-10 minutes for cloud runner\\n3. Workflow fails \u2192 Fix locally \u2192 Push again\\n4. Repeat steps 2-3 (multiple times!)\\n\\n**Total feedback cycle: 30+ minutes for a simple fix**\\n\\nWith `act`:\\n1. Write workflow\\n2. Run locally with `act` \u2192 Instant feedback (10-30 seconds)\\n3. Debug and iterate locally\\n4. Push confident code to GitHub\\n\\n**Total feedback cycle: 5 minutes**\\n\\n---\\n\\n## Part 1: Installation and Setup\\n\\n### Step 1: Install `act` with Pixi (Recommended)\\n\\nThe easiest way to install `act` globally is using **Pixi**, a fast package manager for Python and system tools:\\n\\n**Install act globally with Pixi:**\\n```bash\\npixi global install act\\n```\\n\\nThat\'s it! Pixi handles downloading the correct binary for your OS (macOS, Linux, or Windows).\\n\\n**Verify Installation**\\n```bash\\nact --version\\n# Output: act version 0.2.X\\n```\\n\\n**Why use Pixi?**\\n- \u2705 Cross-platform (macOS, Linux, Windows)\\n- \u2705 No system dependencies needed\\n- \u2705 One command: `pixi global install act`\\n- \u2705 Automatic updates: `pixi global upgrade act`\\n- \u2705 Isolated from system Python/packages\\n- \u2705 Same tool you use for project management\\n\\n### Alternative Installation Methods\\n\\nIf you prefer not to use Pixi, here are other options:\\n\\n**macOS (Homebrew)**\\n```bash\\nbrew install act\\n```\\n\\n**Linux**\\n```bash\\n# Debian/Ubuntu\\ncurl https://raw.githubusercontent.com/nektos/act/master/install.sh | bash\\n\\n# Or with pacman (Arch)\\npacman -S act\\n```\\n\\n**Windows (PowerShell)**\\n```powershell\\nchoco install act\\n# Or if using Scoop:\\nscoop install act\\n```\\n\\n**Manual Installation**\\n- Download from [GitHub Releases](https://github.com/nektos/act/releases)\\n- Extract to a directory in your `$PATH`\\n\\n### Step 2: Install Docker\\n\\n`act` requires Docker to run workflows in containers (just like GitHub\'s cloud runners).\\n\\n**Option 1: Docker Desktop (Easiest)**\\n- Download [Docker Desktop](https://www.docker.com/products/docker-desktop) for macOS or Windows\\n- Install and start the application\\n- Docker will automatically be available in your terminal\\n\\n**Option 2: Install Docker with Pixi** (if you prefer package managers)\\n```bash\\n# Install Docker tools in your Pixi environment\\npixi global install docker\\n```\\n\\n**Option 3: Linux (System Package)**\\n```bash\\n# Debian/Ubuntu\\nsudo apt-get install docker.io docker-compose\\nsudo usermod -aG docker $USER  # Add current user to docker group\\nnewgrp docker  # Activate group changes\\n```\\n\\n**Verify Docker Installation**\\n```bash\\ndocker --version\\ndocker run hello-world  # Should complete successfully\\n```\\n\\n**Troubleshooting Docker:**\\n- macOS/Windows: Ensure Docker Desktop is running (check system tray icon)\\n- Linux: Check if docker daemon is running: `sudo systemctl start docker`\\n- Permission denied: Add user to docker group: `sudo usermod -aG docker $USER`\\n\\n### Step 3: Verify Setup\\n\\nTest that `act` can find your GitHub workflows:\\n\\n```bash\\ncd /path/to/your/repo\\nact --list\\n```\\n\\n**Expected output:**\\n```\\nStage  Job ID  Job Name  Workflow Name     Workflow File           Events\\n0      test    test      test-workflow     .github/workflows/test.yml  push\\n```\\n\\nIf no workflows appear, ensure `.github/workflows/` exists with `.yml` files.\\n\\n### Step 4: Pixi Integration (Optional but Recommended)\\n\\nIf your project uses Pixi, add `act` as a task for easy execution:\\n\\n**Add to `pixi.toml`:**\\n```toml\\n[tasks]\\nci = \\"act push\\"\\nci-test = \\"act pull_request\\"\\nci-debug = \\"act -v push\\"\\n```\\n\\n**Then run with Pixi:**\\n```bash\\npixi run ci           # Run GitHub Actions locally\\npixi run ci-debug     # With verbose output\\n```\\n\\n**Check your Pixi setup:**\\n```bash\\npixi info\\n# Should show: act is available globally\\n```\\n\\n---\\n\\n## Part 2: Basic Usage - Running Workflows Locally\\n\\n### Simple Example: Python Test Workflow\\n\\nLet\'s create a minimal GitHub Actions workflow and test it with `act`.\\n\\n**Create `.github/workflows/test.yml`:**\\n```yaml\\nname: Python Tests\\non:\\n  push:\\n    branches: [main, develop]\\n\\njobs:\\n  test:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - uses: actions/checkout@v3\\n      \\n      - name: Set up Python\\n        uses: actions/setup-python@v4\\n        with:\\n          python-version: \'3.12\'\\n      \\n      - name: Install dependencies\\n        run: |\\n          pip install pytest numpy pandas\\n      \\n      - name: Run tests\\n        run: |\\n          pytest tests/ -v\\n```\\n\\n**Run locally with act:**\\n```bash\\nact push\\n```\\n\\n**Expected output:**\\n```\\n[Python Tests/test] \ud83d\ude80 Start image pull...\\n[Python Tests/test] \ud83d\udc33 Docker pull requested ghcr.io/catthehacker/ubuntu:act-latest\\n[Python Tests/test] \u2713 Image pull complete\\n[Python Tests/test] \ud83d\ude80 Start container...\\n[Python Tests/test] \u2b50 Run Main actions/checkout@v3\\n[Python Tests/test] \u2713 Complete job\\n```\\n\\n### Running Specific Workflows\\n\\n**List all available workflows:**\\n```bash\\nact --list\\n```\\n\\n**Run a specific job:**\\n```bash\\nact --job test\\n```\\n\\n**Run a specific workflow:**\\n```bash\\nact --workflow test.yml\\n```\\n\\n**Simulate a different event (e.g., pull_request):**\\n```bash\\nact pull_request\\n```\\n\\n---\\n\\n## Part 3: Environment Variables and Secrets\\n\\n### Passing Environment Variables\\n\\n`act` provides several ways to pass variables:\\n\\n**Method 1: Command-line flag**\\n```bash\\nact -e event.json\\n```\\n\\nWhere `event.json` contains:\\n```json\\n{\\n  \\"repository\\": {\\n    \\"name\\": \\"my-repo\\",\\n    \\"owner\\": {\\n      \\"login\\": \\"myusername\\"\\n    }\\n  }\\n}\\n```\\n\\n**Method 2: `.actrc` file** (in repo root)\\n```bash\\n# .actrc\\n-P ubuntu-latest=ghcr.io/catthehacker/ubuntu:act-latest\\n-l\\n```\\n\\n**Method 3: Shell environment variables**\\n```bash\\nexport MY_VAR=\\"value\\"\\nexport ANOTHER_VAR=\\"another\\"\\nact push\\n```\\n\\n### Working with Secrets\\n\\nGitHub Actions use `secrets` for sensitive data. With `act`, you can pass secrets locally:\\n\\n**Method 1: `.secrets` file** (in repo root)\\n```bash\\n# .secrets\\nMY_SECRET=super_secret_value\\nGITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\\nAPI_KEY=your_api_key_here\\n```\\n\\n**Important:** Add `.secrets` to `.gitignore` to prevent committing secrets!\\n```bash\\necho \\".secrets\\" >> .gitignore\\n```\\n\\n**Method 2: Command-line secret flag**\\n```bash\\nact -s MY_SECRET=value -s GITHUB_TOKEN=token\\n```\\n\\n**Workflow using secrets:**\\n```yaml\\njobs:\\n  deploy:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - uses: actions/checkout@v3\\n      \\n      - name: Use secret\\n        run: |\\n          echo \\"API Key: ${{ secrets.API_KEY }}\\"\\n          echo \\"Token: ${{ secrets.GITHUB_TOKEN }}\\"\\n```\\n\\n**Run with secrets:**\\n```bash\\nact --secret-file .secrets\\n```\\n\\n---\\n\\n## Part 4: Docker and Container Management\\n\\n### Understanding `act` Container Images\\n\\n`act` uses pre-built Docker images that mimic GitHub\'s cloud runners. Default images are large (~15GB) but highly compatible.\\n\\n**Available Docker images:**\\n```bash\\n# Ubuntu (recommended for bioinformatics)\\nghcr.io/catthehacker/ubuntu:act-latest\\n\\n# Debian (smaller, faster)\\nghcr.io/catthehacker/ubuntu:full-latest\\n\\n# Minimal (smallest, fast)\\nubuntu:latest  # Docker Hub\\n```\\n\\n### Specifying Docker Images\\n\\n**Method 1: `-P` flag in command**\\n```bash\\nact -P ubuntu-latest=ghcr.io/catthehacker/ubuntu:full-latest\\n```\\n\\n**Method 2: `.actrc` file**\\n```bash\\n# .actrc\\n-P ubuntu-latest=ghcr.io/catthehacker/ubuntu:full-latest\\n-P windows-latest=ghcr.io/catthehacker/windows:full-latest\\n```\\n\\n**Method 3: Command-line shorthand**\\n```bash\\n# Use minimal image\\nact --container-architecture linux/amd64\\n```\\n\\n### Pre-pulling Docker Images\\n\\nLarge images take time on first run. Pre-pull them:\\n\\n```bash\\n# Pull the image once\\ndocker pull ghcr.io/catthehacker/ubuntu:act-latest\\n\\n# Now act will use cached image (much faster)\\nact push\\n```\\n\\n### Managing Disk Space\\n\\nDocker images consume significant space. Clean up unused images:\\n\\n```bash\\n# Remove unused images\\ndocker image prune -a\\n\\n# Remove all containers\\ndocker container prune -a\\n\\n# Check disk usage\\ndocker system df\\n```\\n\\n---\\n\\n## Part 5: Performance Optimization Tips\\n\\n### 1. Use Smaller Docker Images\\n\\nInstead of full Ubuntu, use minimal images:\\n\\n**Before (slow - ~15GB):**\\n```bash\\nact -P ubuntu-latest=ghcr.io/catthehacker/ubuntu:act-latest\\n```\\n\\n**After (fast - ~2GB):**\\n```bash\\nact -P ubuntu-latest=ghcr.io/catthehacker/ubuntu:full-latest\\n```\\n\\n**Performance impact:** ~2-3x faster\\n\\n### 2. Cache Dependencies\\n\\nGitHub Actions support caching. Leverage it in `act`:\\n\\n**Workflow with caching:**\\n```yaml\\njobs:\\n  test:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - uses: actions/checkout@v3\\n      \\n      - name: Cache pip dependencies\\n        uses: actions/cache@v3\\n        with:\\n          path: ~/.cache/pip\\n          key: ${{ runner.os }}-pip-${{ hashFiles(\'**/requirements.txt\') }}\\n          restore-keys: |\\n            ${{ runner.os }}-pip-\\n      \\n      - name: Install Python dependencies\\n        run: pip install -r requirements.txt\\n      \\n      - name: Run tests\\n        run: pytest tests/\\n```\\n\\n**Performance impact:** First run ~30s, subsequent runs ~5s (cache hit)\\n\\n### 3. Run Jobs in Parallel\\n\\nBy default, `act` runs jobs sequentially. Enable parallel execution:\\n\\n```bash\\n# Run all jobs in parallel\\nact --parallel 4\\n```\\n\\n**Workflow with multiple jobs:**\\n```yaml\\njobs:\\n  unit-tests:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - run: pytest tests/unit/\\n  \\n  integration-tests:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - run: pytest tests/integration/\\n  \\n  lint:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - run: pylint src/\\n```\\n\\n**Sequential time:** 30s + 40s + 10s = 80s\\n**Parallel time (--parallel 3):** ~40s\\n\\n### 4. Use `act -l` (List) Mode\\n\\nFor quick workflow checks without running:\\n\\n```bash\\nact --list\\n```\\n\\nShows all jobs without executing them.\\n\\n### 5. Rebuild Docker Image\\n\\nCache can become stale. Rebuild:\\n\\n```bash\\nact --reuse-containers  # Reuse running containers\\nact --rebuild            # Rebuild image from scratch\\n```\\n\\n---\\n\\n## Real Example: Bioinformatics Gene Expression Pipeline\\n\\nLet\'s build a complete ML pipeline workflow and test it locally with `act`.\\n\\n### Workflow File: `.github/workflows/ml-pipeline.yml`\\n\\n```yaml\\nname: ML Pipeline - Gene Expression Analysis\\non:\\n  push:\\n    branches: [main, develop]\\n  pull_request:\\n    branches: [main]\\n\\njobs:\\n  test:\\n    runs-on: ubuntu-latest\\n    strategy:\\n      matrix:\\n        python-version: [\'3.10\', \'3.11\', \'3.12\']\\n    \\n    steps:\\n      - name: Checkout code\\n        uses: actions/checkout@v3\\n      \\n      - name: Set up Python ${{ matrix.python-version }}\\n        uses: actions/setup-python@v4\\n        with:\\n          python-version: ${{ matrix.python-version }}\\n      \\n      - name: Cache pip packages\\n        uses: actions/cache@v3\\n        with:\\n          path: ~/.cache/pip\\n          key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles(\'**/requirements.txt\') }}\\n      \\n      - name: Install dependencies\\n        run: |\\n          python -m pip install --upgrade pip\\n          pip install -r requirements.txt\\n          pip install pytest pytest-cov pytest-xdist\\n      \\n      - name: Lint with pylint\\n        run: pylint src/ --exit-zero\\n        continue-on-error: true\\n      \\n      - name: Run unit tests\\n        run: pytest tests/unit/ -v --cov=src --cov-report=xml\\n      \\n      - name: Run integration tests\\n        run: pytest tests/integration/ -v\\n      \\n      - name: Upload coverage reports\\n        uses: codecov/codecov-action@v3\\n        if: matrix.python-version == \'3.12\'\\n        with:\\n          files: ./coverage.xml\\n          flags: unittests\\n  \\n  security:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - uses: actions/checkout@v3\\n      \\n      - name: Run Bandit security check\\n        run: |\\n          pip install bandit\\n          bandit -r src/ -v --exit-code 0\\n        continue-on-error: true\\n```\\n\\n### Running Locally with `act`\\n\\n**Run all jobs:**\\n```bash\\nact push\\n```\\n\\n**Expected output:**\\n```\\n[ML Pipeline - Gene Expression Analysis/test] \ud83d\ude80 Start image pull...\\n[ML Pipeline - Gene Expression Analysis/test] \u2b50 Run actions/checkout@v3\\n[ML Pipeline - Gene Expression Analysis/test] \u2713 Step \'Checkout code\' completed\\n[ML Pipeline - Gene Expression Analysis/test] \u2b50 Run Set up Python 3.10\\n[ML Pipeline - Gene Expression Analysis/test] \u2713 Step \'Set up Python 3.10\' completed\\n[ML Pipeline - Gene Expression Analysis/test] \u2b50 Run Install dependencies\\n[ML Pipeline - Gene Expression Analysis/test] \u2713 Step \'Install dependencies\' completed\\n[ML Pipeline - Gene Expression Analysis/test] \u2b50 Run Run unit tests\\n...\\n[ML Pipeline - Gene Expression Analysis/test] \u2713 Complete job\\n[ML Pipeline - Gene Expression Analysis/security] \u2b50 Run Bandit security check\\n[ML Pipeline - Gene Expression Analysis/security] \u2713 Complete job\\n```\\n\\n**Run specific Python version:**\\n```bash\\nact push --job test --matrix python-version=3.12\\n```\\n\\n**Run only security tests:**\\n```bash\\nact push --job security\\n```\\n\\n---\\n\\n## Debugging Workflows with `act`\\n\\n### Enable Verbose Logging\\n\\n```bash\\nact -v push\\n```\\n\\nShows detailed output including Docker commands and environment variables.\\n\\n### Debug with Interactive Shell\\n\\nIf a step fails, enter the container:\\n\\n```bash\\n# Create a failing step\\nsteps:\\n  - run: |\\n      echo \\"Debug info:\\"\\n      env | sort\\n      ls -la\\n```\\n\\nOr use `act` with shell access:\\n\\n```bash\\nact -b  # Use local container (not rebuilding)\\n```\\n\\n### Inspect Container After Failure\\n\\nKeep container running after failure:\\n\\n```bash\\n# View logs\\ndocker ps -a\\n\\n# Enter the container\\ndocker exec -it <container_id> /bin/bash\\n```\\n\\n### Common Issues and Solutions\\n\\n**Issue: \\"Cannot connect to Docker daemon\\"**\\n```bash\\n# Solution: Start Docker\\nsudo systemctl start docker  # Linux\\nopen /Applications/Docker.app  # macOS\\n```\\n\\n**Issue: \\"Not enough space for Docker images\\"**\\n```bash\\n# Solution: Clean up\\ndocker system prune -a --volumes\\n```\\n\\n**Issue: \\"Workflow runs but tests fail locally but pass on GitHub\\"**\\n```bash\\n# Solution: Check Python versions match\\npython --version\\nact --list  # Verify Python version in workflow\\n```\\n\\n---\\n\\n## Performance Comparison: Local vs Cloud\\n\\n| Task | Local (`act`) | GitHub Cloud | Speedup |\\n|------|--|--|--|\\n| First run | 45s | 300s+ | 6-7x |\\n| Subsequent runs (cached) | 8s | 250s+ | 30x |\\n| Development iteration | 2 min (10 runs) | 50 min | **25x** |\\n| Cost | $0 | $0.008/min | N/A |\\n\\n**Key insight:** For a typical development session with 10 iterations, `act` saves 48 minutes of waiting time!\\n\\n---\\n\\n## Best Practices for CI/CD with `act`\\n\\n### 1. **Test Locally Before Pushing**\\n```bash\\n# Before git push\\nact push\\n# If all pass, then:\\ngit push origin main\\n```\\n\\n### 2. **Match GitHub Runner Environment**\\n```bash\\n# Use same image as GitHub\\nact -P ubuntu-latest=ghcr.io/catthehacker/ubuntu:act-latest\\n```\\n\\n### 3. **Commit `.actrc` to Repository**\\n```bash\\n# .actrc (can be committed)\\n-P ubuntu-latest=ghcr.io/catthehacker/ubuntu:full-latest\\n-l\\n```\\n\\nBut keep `.secrets` in `.gitignore`:\\n```bash\\n# .gitignore\\n.secrets\\n```\\n\\n### 4. **Use Matrix Strategy for Multiple Versions**\\n```yaml\\nstrategy:\\n  matrix:\\n    python-version: [\'3.10\', \'3.11\', \'3.12\']\\n    os: [ubuntu-latest, macos-latest]\\n```\\n\\nTest all combinations locally before push.\\n\\n### 5. **Document Dependencies**\\n```bash\\n# requirements.txt\\npytest>=7.0.0\\nnumpy>=1.24.0\\npandas>=1.5.0\\nscipy>=1.10.0\\n```\\n\\nKeep updated so `act` can replicate exact environment.\\n\\n### 6. **Use Pixi for Global Tools** (Bioinformatics Recommended)\\n\\nIf you use Pixi for project management, keep your global tools there too:\\n\\n```bash\\n# Install act globally with Pixi\\npixi global install act\\n\\n# Update act easily\\npixi global upgrade act\\n\\n# See all globally installed tools\\npixi global list\\n```\\n\\n**Benefits for bioinformatics workflows:**\\n- Single package manager for all tools\\n- Easy version control (`pixi.lock`)\\n- Reproducible environments across team\\n- No system package conflicts\\n\\n**Create a `.pixi.toml` for your project:**\\n```toml\\n[dependencies]\\npython = \\"3.12\\"\\npytest = \\">=7.0.0\\"\\nnumpy = \\">=1.24.0\\"\\npandas = \\">=1.5.0\\"\\n\\n[tasks]\\ntest = \\"pytest tests/ -v\\"\\nlint = \\"pylint src/\\"\\nci = \\"act push\\"  # Run GitHub Actions locally\\n```\\n\\nThen your team can just run:\\n```bash\\npixi run test      # Run tests\\npixi run lint      # Run linter\\npixi run ci        # Run GitHub Actions locally\\n```\\n\\n---\\n\\n## Key Takeaways\\n\\n1. **`act` runs GitHub Actions locally** \u2192 test before pushing to GitHub\\n2. **5-30x faster feedback** \u2192 iterate quickly during development\\n3. **Docker-based** \u2192 identical environment to GitHub Cloud runners\\n4. **Supports secrets and environment variables** \u2192 test real workflows\\n5. **Performance optimization** \u2192 use smaller images, caching, parallel jobs\\n6. **Free and open-source** \u2192 no additional costs beyond your machine\\n\\n---\\n\\n## What\'s Next?\\n\\nNow that you can test workflows locally with `act`, consider:\\n- Setting up pre-commit hooks to run `act` automatically\\n- Creating reusable workflow templates for bioinformatics pipelines\\n- Integrating `act` into your team\'s development process\\n- Exploring GitHub Actions Marketplace for bioinformatics tools\\n\\n---\\n\\n## References\\n\\n- [act GitHub Repository](https://github.com/nektos/act)\\n- [GitHub Actions Documentation](https://docs.github.com/en/actions)\\n- [Docker Documentation](https://docs.docker.com/)\\n- [GitHub Actions Best Practices](https://docs.github.com/en/actions/guides)\\n\\n**Happy local testing! \ud83d\ude80**"},{"id":"machine-learning-bioinformatics-part1-knn","metadata":{"permalink":"/river-docs/blog/machine-learning-bioinformatics-part1-knn","source":"@site/blog/2026-02/2026-02-02.md","title":"Machine Learning in Bioinformatics Part 1: Building KNN from Scratch","description":"Machine learning is transforming bioinformatics, enabling us to discover patterns in biological data. In this first part, we\'ll build a K-Nearest Neighbors (KNN) classifier from scratch using only Python, then apply it to simulated gene expression data. This post is designed for anyone who knows basic Python and biology\u2014no advanced ML experience required!","date":"2026-02-02T00:00:00.000Z","tags":[{"inline":true,"label":"machine-learning","permalink":"/river-docs/blog/tags/machine-learning"},{"inline":true,"label":"bioinformatics","permalink":"/river-docs/blog/tags/bioinformatics"},{"inline":true,"label":"python","permalink":"/river-docs/blog/tags/python"},{"inline":true,"label":"knn","permalink":"/river-docs/blog/tags/knn"},{"inline":true,"label":"gene-expression","permalink":"/river-docs/blog/tags/gene-expression"}],"readingTime":11.01,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"machine-learning-bioinformatics-part1-knn","title":"Machine Learning in Bioinformatics Part 1: Building KNN from Scratch","authors":["river"],"tags":["machine-learning","bioinformatics","python","knn","gene-expression"],"image":"./imgs/intro.png"},"unlisted":false,"prevItem":{"title":"Running GitHub Actions Locally with act: 5x Faster Development","permalink":"/river-docs/blog/cicd-bioinformatics-act-local-github-action"},"nextItem":{"title":"Introduction to AI/ML in Bioinformatics: Classification Models & Evaluation","permalink":"/river-docs/blog/intro-ai-ml-bioinformatics-applications"}},"content":"Machine learning is transforming bioinformatics, enabling us to discover patterns in biological data. In this first part, we\'ll build a K-Nearest Neighbors (KNN) classifier from scratch using only Python, then apply it to simulated gene expression data. This post is designed for anyone who knows basic Python and biology\u2014no advanced ML experience required!\\n\\n\x3c!-- truncate --\x3e\\n\\n## Why Machine Learning in Bioinformatics?\\n\\nBiologists today collect massive amounts of data:\\n- **Gene expression** from thousands of genes across different conditions\\n- **Patient samples** with different disease states or treatment responses\\n- **Protein sequences** that need classification\\n- **Imaging data** from microscopy or medical scans\\n\\nManually analyzing this data is impossible. Machine learning helps us:\\n1. **Find patterns**: Automatically discover which genes distinguish disease types\\n2. **Make predictions**: Classify new patients based on gene expression patterns\\n3. **Understand biology**: Identify important features in biological systems\\n\\n## What is KNN?\\n\\nK-Nearest Neighbors (KNN) is one of the simplest yet powerful machine learning algorithms:\\n\\n**The Idea**: To classify a new sample, look at its K closest neighbors in your training data and vote on the class.\\n\\n**Simple Example**:\\n- Imagine you have cancer and normal patient samples plotted by gene expression\\n- A new patient arrives\\n- You find the 3 nearest patient samples (K=3)\\n- If 2 are cancer and 1 is normal, you classify the new patient as cancer\\n\\n**Why start with KNN?**\\n- Easy to understand (no complex math)\\n- Works well for bioinformatics data\\n- Teaches core ML concepts: distance metrics, classification, and parameter tuning\\n\\n---\\n\\n## Part 1: Building KNN from Scratch\\n\\nLet\'s code KNN step-by-step in Python. You\'ll understand exactly what\'s happening.\\n\\n### Step 1: Import Libraries and Create Simulated Gene Expression Data\\n\\n```python\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom collections import Counter\\n\\n# Set random seed for reproducibility\\nnp.random.seed(42)\\n\\n# Simulate gene expression data\\n# Let\'s say we have 100 samples with 2 genes (for easy visualization)\\n# In reality, we\'d have thousands of genes\\n\\ndef create_gene_expression_data():\\n    \\"\\"\\"\\n    Simulate gene expression data for two disease types\\n    \\n    Gene expression = measured as normalized counts (e.g., from RNA-seq)\\n    \\"\\"\\"\\n    # Class 0: Normal samples\\n    # These samples show low expression in both genes\\n    normal = np.random.normal(loc=5, scale=1.5, size=(50, 2))\\n    \\n    # Class 1: Disease samples\\n    # These samples show higher expression, especially in gene 1\\n    disease = np.random.normal(loc=8, scale=1.5, size=(50, 2))\\n    \\n    # Combine data and create labels\\n    X = np.vstack([normal, disease])  # Features (gene expression)\\n    y = np.hstack([np.zeros(50), np.ones(50)])  # Labels (0=normal, 1=disease)\\n    \\n    return X, y\\n\\n# Load data\\nX, y = create_gene_expression_data()\\n\\n# Split into training and test sets\\ntrain_size = 80\\nX_train, X_test = X[:train_size], X[train_size:]\\ny_train, y_test = y[:train_size], y[train_size:]\\n\\nprint(f\\"Training set: {X_train.shape[0]} samples with {X_train.shape[1]} genes\\")\\nprint(f\\"Test set: {X_test.shape[0]} samples with {X_test.shape[1]} genes\\")\\n```\\n\\n**Output:**\\n```\\nTraining set: 80 samples with 2 genes\\nTest set: 20 samples with 2 genes\\nClasses: 50 Normal, 50 Disease\\n```\\n\\n**What we did:**\\n- Created 100 simulated patients (50 normal, 50 disease)\\n- Each patient has expression levels for 2 genes\\n- Normal patients: ~5 expression (biologically, lower expression)\\n- Disease patients: ~8 expression (biologically, higher expression)\\n- Split 80 samples for training, 20 for testing\\n\\n### Step 2: Calculate Distance Between Samples\\n\\nKNN finds the \\"closest\\" neighbors. We need a distance metric. Let\'s implement **Euclidean distance**:\\n\\n```python\\ndef euclidean_distance(sample1, sample2):\\n    \\"\\"\\"\\n    Calculate Euclidean distance between two samples.\\n    \\n    Distance = sqrt((x1-x2)\xb2 + (y1-y2)\xb2 + ...)\\n    \\n    This measures \\"how different\\" two patients\' gene expression profiles are.\\n    \\"\\"\\"\\n    return np.sqrt(np.sum((sample1 - sample2) ** 2))\\n\\n# Test distance calculation\\nsample_a = np.array([5.2, 4.8])  # Normal patient (low expression)\\nsample_b = np.array([8.1, 8.3])  # Disease patient (high expression)\\nsample_c = np.array([5.5, 4.9])  # Another normal patient (similar to A)\\n\\ndist_a_to_b = euclidean_distance(sample_a, sample_b)\\ndist_a_to_c = euclidean_distance(sample_a, sample_c)\\n\\nprint(f\\"Distance from normal A to disease B: {dist_a_to_b:.2f}\\")\\nprint(f\\"Distance from normal A to normal C: {dist_a_to_c:.2f}\\")\\nprint(f\\"\u2192 A is closer to C (similar class) than B\\")\\n```\\n\\n**Output:**\\n```\\nDistance from normal A to disease B: 4.55\\nDistance from normal A to normal C: 0.32\\n\u2192 A is closer to C (similar class) than B \u2713\\n```\\n\\nThis is the key insight: **samples from the same class are closer together!**\\n\\n### Step 3: Find K Nearest Neighbors\\n\\n```python\\ndef find_nearest_neighbors(query_sample, training_data, training_labels, k=3):\\n    \\"\\"\\"\\n    Find the K nearest neighbors to a query sample.\\n    \\n    Returns:\\n    - distances: distance to each training sample\\n    - indices: index of each training sample\\n    - neighbor_labels: class labels of the K nearest neighbors\\n    \\"\\"\\"\\n    # Calculate distance from query to all training samples\\n    distances = []\\n    for i, train_sample in enumerate(training_data):\\n        dist = euclidean_distance(query_sample, train_sample)\\n        distances.append(dist)\\n    \\n    distances = np.array(distances)\\n    \\n    # Find indices of K smallest distances\\n    # argsort returns indices in ascending order\\n    k_indices = np.argsort(distances)[:k]\\n    \\n    # Get labels of K nearest neighbors\\n    neighbor_labels = training_labels[k_indices]\\n    neighbor_distances = distances[k_indices]\\n    \\n    return neighbor_distances, k_indices, neighbor_labels\\n\\n# Test with a new patient (test sample)\\nnew_patient = X_test[0]  # First test patient\\nk = 3\\n\\ndistances, indices, labels = find_nearest_neighbors(new_patient, X_train, y_train, k=k)\\n\\nprint(f\\"New patient gene expression: {new_patient}\\")\\nprint(f\\"\\\\nK={k} nearest neighbors:\\")\\nfor i, (idx, dist, label) in enumerate(zip(indices, distances, labels)):\\n    class_name = \\"Normal\\" if label == 0 else \\"Disease\\"\\n    print(f\\"  Neighbor {i+1}: {class_name} patient (distance: {dist:.2f})\\")\\n```\\n\\n**Output:**\\n```\\nNew patient gene expression: [6.53797749 9.18062691]\\n\\nK=3 nearest neighbors:\\n  Neighbor 1: Disease patient (distance: 0.38)\\n  Neighbor 2: Disease patient (distance: 0.49)\\n  Neighbor 3: Disease patient (distance: 0.94)\\n```\\n\\n**What\'s happening**: All 3 nearest neighbors are disease patients, so the KNN algorithm will predict \\"Disease\\" for this new patient.\\n\\n### Step 4: Vote for the Class\\n\\n```python\\ndef predict_single_sample(query_sample, training_data, training_labels, k=3):\\n    \\"\\"\\"\\n    Predict the class of a query sample using KNN voting.\\n    \\n    Algorithm:\\n    1. Find K nearest neighbors\\n    2. Count votes for each class\\n    3. Return the most common class (majority vote)\\n    \\"\\"\\"\\n    distances, indices, neighbor_labels = find_nearest_neighbors(\\n        query_sample, training_data, training_labels, k\\n    )\\n    \\n    # Count votes for each class\\n    votes = Counter(neighbor_labels)\\n    \\n    # Return class with most votes\\n    prediction = votes.most_common(1)[0][0]\\n    \\n    return prediction\\n\\n# Predict on a new patient\\nprediction = predict_single_sample(new_patient, X_train, y_train, k=3)\\ntrue_label = y_test[0]\\n\\nprint(f\\"Predicted class: {\'Normal\' if prediction == 0 else \'Disease\'}\\")\\nprint(f\\"True class: {\'Normal\' if true_label == 0 else \'Disease\'}\\")\\nprint(f\\"Correct: {prediction == true_label} \u2713\\" if prediction == true_label else f\\"Correct: {prediction == true_label} \u2717\\")\\n```\\n\\n**Output:**\\n```\\nPredicted class: Disease\\nTrue class: Disease\\nCorrect: True \u2713\\n```\\n\\nPerfect! Our KNN classifier correctly predicted this patient as having the disease. This shows the power of voting: even though one neighbor had distance 0.94, the majority (2 out of 3) voted for disease, leading to the correct prediction.\\n\\n### Step 5: Build the Complete KNN Classifier\\n\\n```python\\nclass KNNClassifier:\\n    \\"\\"\\"\\n    A simple KNN classifier for bioinformatics data classification.\\n    \\n    Parameters:\\n    - k: number of nearest neighbors to consider\\n    - distance_metric: \'euclidean\' (default) or \'manhattan\'\\n    \\"\\"\\"\\n    \\n    def __init__(self, k=3, distance_metric=\'euclidean\'):\\n        self.k = k\\n        self.distance_metric = distance_metric\\n        self.X_train = None\\n        self.y_train = None\\n    \\n    def distance(self, sample1, sample2):\\n        \\"\\"\\"Calculate distance between two samples.\\"\\"\\"\\n        if self.distance_metric == \'euclidean\':\\n            # Euclidean: sqrt(sum of squared differences)\\n            return np.sqrt(np.sum((sample1 - sample2) ** 2))\\n        elif self.distance_metric == \'manhattan\':\\n            # Manhattan: sum of absolute differences\\n            # Like walking on a city grid instead of straight line\\n            return np.sum(np.abs(sample1 - sample2))\\n        else:\\n            raise ValueError(\\"Unknown distance metric\\")\\n    \\n    def fit(self, X_train, y_train):\\n        \\"\\"\\"\\n        Store training data.\\n        KNN is a \\"lazy learner\\" - it doesn\'t learn patterns,\\n        it just memorizes the training data.\\n        \\"\\"\\"\\n        self.X_train = X_train\\n        self.y_train = y_train\\n        print(f\\"\u2713 Stored {len(X_train)} training samples\\")\\n    \\n    def predict(self, X_test):\\n        \\"\\"\\"Predict class for all test samples.\\"\\"\\"\\n        predictions = []\\n        for sample in X_test:\\n            # Calculate distances to all training samples\\n            distances = np.array([\\n                self.distance(sample, train_sample) \\n                for train_sample in self.X_train\\n            ])\\n            \\n            # Find K nearest neighbors\\n            k_indices = np.argsort(distances)[:self.k]\\n            neighbor_labels = self.y_train[k_indices]\\n            \\n            # Vote for the most common class\\n            votes = Counter(neighbor_labels)\\n            prediction = votes.most_common(1)[0][0]\\n            predictions.append(prediction)\\n        \\n        return np.array(predictions)\\n    \\n    def score(self, X_test, y_test):\\n        \\"\\"\\"Calculate accuracy on test set.\\"\\"\\"\\n        predictions = self.predict(X_test)\\n        accuracy = np.mean(predictions == y_test)\\n        return accuracy\\n\\n# Create and train the classifier\\nknn = KNNClassifier(k=3)\\nknn.fit(X_train, y_train)\\n\\n# Make predictions\\ny_pred = knn.predict(X_test)\\naccuracy = knn.score(X_test, y_test)\\n\\nprint(f\\"\\\\nKNN Classifier Performance:\\")\\nprint(f\\"K = 3\\")\\nprint(f\\"Accuracy on test set: {accuracy:.2%}\\")\\nprint(f\\"Correct predictions: {(y_pred == y_test).sum()}/{len(y_test)}\\")\\n```\\n\\n**Output:**\\n```\\n\u2713 Stored 80 training samples\\n\\nKNN Classifier Performance:\\nK = 3\\nAccuracy on test set: 90.00%\\nCorrect predictions: 18/20\\n```\\n\\nExcellent! Our KNN classifier achieves 90% accuracy on the test set\u2014correctly predicting the disease status of 18 out of 20 patients based on gene expression patterns.\\n\\n---\\n\\n## Part 2: Tuning Parameters\\n\\nThe power of KNN comes from parameter tuning. Let\'s experiment!\\n\\n### Experiment 1: Effect of K Value\\n\\nK controls how many neighbors to consider. What\'s the best value?\\n\\n```python\\n# Test different K values\\nk_values = [1, 3, 5, 7, 9, 15]\\ntrain_accuracies = []\\ntest_accuracies = []\\n\\nfor k in k_values:\\n    knn = KNNClassifier(k=k)\\n    knn.fit(X_train, y_train)\\n    \\n    train_acc = knn.score(X_train, y_train)\\n    test_acc = knn.score(X_test, y_test)\\n    \\n    train_accuracies.append(train_acc)\\n    test_accuracies.append(test_acc)\\n    \\n    print(f\\"K={k:2d} | Train: {train_acc:.2%} | Test: {test_acc:.2%}\\")\\n```\\n\\n**Output:**\\n```\\nK= 1 | Train: 100.00% | Test: 90.00%\\nK= 3 | Train: 96.25% | Test: 90.00%\\nK= 5 | Train: 97.50% | Test: 95.00%\\nK= 7 | Train: 93.75% | Test: 95.00%\\nK= 9 | Train: 96.25% | Test: 95.00%\\nK=15 | Train: 95.00% | Test: 90.00%\\n```\\n\\n**Observations:**\\n- **K=1**: Perfect on training (100%) but may overfit (memorizing individual patients)\\n- **K=3-5**: Good balance on both training and test sets\\n- **K=7-9**: Smoother, robust predictions\\n- **K=15**: All neighbors voted, might be too general (underfit)\\n\\n**Biological interpretation:**\\n- Small K (1-3): Very sensitive to each patient\'s unique gene expression\\n- Medium K (5-7): Balanced\u2014considers local gene expression patterns while being robust\\n- Large K (15): Very general\u2014might miss subtle disease signatures but robust to noise\\n\\nThe sweet spot for this dataset appears to be **K=5-7**, where test accuracy plateaus at 95%.\\n\\n### Experiment 2: Distance Metric Comparison\\n\\nDoes it matter how we measure distance?\\n\\n```python\\n# Compare Euclidean vs Manhattan distance\\ndistances = [\'euclidean\', \'manhattan\']\\nresults = {}\\n\\nprint(\\"\\\\nComparing Distance Metrics:\\")\\nprint(\\"-\\" * 50)\\n\\nfor dist_metric in distances:\\n    accuracies = []\\n    for k in k_values:\\n        knn = KNNClassifier(k=k, distance_metric=dist_metric)\\n        knn.fit(X_train, y_train)\\n        test_acc = knn.score(X_test, y_test)\\n        accuracies.append(test_acc)\\n    results[dist_metric] = accuracies\\n\\nprint(\\"\\\\nDistance Metric Comparison:\\")\\nprint(f\\"{\'K\':<5} {\'Euclidean\':<15} {\'Manhattan\':<15}\\")\\nprint(\\"-\\" * 35)\\nfor k, eucl_acc, manh_acc in zip(k_values, results[\'euclidean\'], results[\'manhattan\']):\\n    print(f\\"{k:<5} {eucl_acc:<14.2%} {manh_acc:<14.2%}\\")\\n\\nprint(\\"\\\\nBest parameters:\\")\\nfor dist_metric in distances:\\n    best_k = k_values[np.argmax(results[dist_metric])]\\n    best_acc = max(results[dist_metric])\\n    print(f\\"  {dist_metric}: K={best_k}, Accuracy={best_acc:.2%}\\")\\n```\\n\\n**Output:**\\n```\\nComparing Distance Metrics:\\n--------------------------------------------------\\n\\nDistance Metric Comparison:\\nK     Euclidean       Manhattan       \\n-------------------------------------\\n1     90.00%          90.00%          \\n3     90.00%          90.00%          \\n5     95.00%          95.00%          \\n7     95.00%          95.00%          \\n9     95.00%          95.00%          \\n15    90.00%          90.00%          \\n\\nBest parameters:\\n  euclidean: K=5, Accuracy=95.00%\\n  manhattan: K=5, Accuracy=95.00%\\n```\\n\\n**Key Findings:**\\n- **Both metrics perform identically** for this gene expression dataset\\n- **Euclidean distance** is generally preferred for continuous data like gene expression\\n- **Manhattan distance** (sum of absolute differences) can be useful for high-dimensional data or when features have different scales\\n- For bioinformatics applications with normalized gene expression data, **Euclidean distance is the safer default**\\n\\n---\\n\\n## How KNN Learns the Decision Boundary\\n\\nEven though KNN is a \\"lazy learner\\" that doesn\'t explicitly learn patterns, it implicitly learns decision boundaries through distance-based voting. \\n\\n**What happened in our experiment:**\\n- Normal patients cluster around gene expression \u2248 [5, 5]\\n- Disease patients cluster around gene expression \u2248 [8, 8]\\n- When classifying a new patient, KNN votes based on proximity to these clusters\\n- This creates a natural decision boundary between the two classes\\n\\n**For our 2-gene example with K=5:**\\n- Test accuracy: 95% (18 out of 20 patients correct)\\n- The 2 misclassified patients were likely on the boundary between normal and disease\\n- In real bioinformatics with thousands of genes, KNN works even better because true disease signatures are stronger across many genes\\n\\n**Why KNN works for gene expression data:**\\n1. **Genes cluster by function**: Genes involved in the same pathway have correlated expression\\n2. **Disease is distinctive**: Disease samples show consistent expression patterns different from normal\\n3. **K controls smoothness**: Small K captures local patterns, large K finds global trends\\n4. **Distance captures similarity**: Euclidean distance naturally measures how different two gene expression profiles are\\n\\n---\\n\\n## Key Takeaways\\n\\n1. **KNN is intuitive**: \\"You are similar to your neighbors\\" - easy to explain to biologists\\n2. **K matters**: Small K is sensitive, large K is robust. K=3-5 often works well\\n3. **Distance metrics matter**: Euclidean works well for continuous data like gene expression\\n4. **No learning phase**: KNN just memorizes training data and votes at prediction time\\n5. **Works for bioinformatics**: Genes naturally cluster by function and disease state\\n\\n---\\n\\n## What\'s Next?\\n\\nIn Part 2, we\'ll:\\n- Use **real gene expression data** (not simulated)\\n- Scale to **thousands of genes**\\n- Learn about **feature selection** (which genes matter?)\\n- Evaluate using **cross-validation** for robust performance estimates\\n\\n---\\n\\n## Summary Code: Full Example\\n\\n```python\\n# Quick reference - full KNN from scratch\\nimport numpy as np\\nfrom collections import Counter\\n\\nclass SimpleKNN:\\n    def __init__(self, k=3):\\n        self.k = k\\n        self.X_train = None\\n        self.y_train = None\\n    \\n    def fit(self, X, y):\\n        self.X_train = X\\n        self.y_train = y\\n    \\n    def predict(self, X_test):\\n        predictions = []\\n        for sample in X_test:\\n            distances = np.sqrt(np.sum((self.X_train - sample) ** 2, axis=1))\\n            k_indices = np.argsort(distances)[:self.k]\\n            labels = self.y_train[k_indices]\\n            prediction = Counter(labels).most_common(1)[0][0]\\n            predictions.append(prediction)\\n        return np.array(predictions)\\n\\n# Usage\\nknn = SimpleKNN(k=5)\\nknn.fit(X_train, y_train)\\npredictions = knn.predict(X_test)\\naccuracy = np.mean(predictions == y_test)\\nprint(f\\"Accuracy: {accuracy:.2%}\\")\\n```\\n\\nBy coding KNN from scratch, you\'ve learned machine learning fundamentals that apply to every algorithm. You now understand distance metrics, classification, and parameter tuning\u2014all essential concepts in bioinformatics!"},{"id":"intro-ai-ml-bioinformatics-applications","metadata":{"permalink":"/river-docs/blog/intro-ai-ml-bioinformatics-applications","source":"@site/blog/2026-02/2026-02-01.md","title":"Introduction to AI/ML in Bioinformatics: Classification Models & Evaluation","description":"Machine learning is transforming bioinformatics by automating pattern discovery from biological data. But what problems can it actually solve? This post shows real-world applications of classification models, then builds the simplest possible classifiers to understand how they work and how to evaluate them. This is Part 0\u2014the practical foundation before diving into complex algorithms like KNN.","date":"2026-02-01T00:00:00.000Z","tags":[{"inline":true,"label":"machine-learning","permalink":"/river-docs/blog/tags/machine-learning"},{"inline":true,"label":"bioinformatics","permalink":"/river-docs/blog/tags/bioinformatics"},{"inline":true,"label":"classification","permalink":"/river-docs/blog/tags/classification"},{"inline":true,"label":"evaluation","permalink":"/river-docs/blog/tags/evaluation"},{"inline":true,"label":"disease-prediction","permalink":"/river-docs/blog/tags/disease-prediction"}],"readingTime":11.67,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"intro-ai-ml-bioinformatics-applications","title":"Introduction to AI/ML in Bioinformatics: Classification Models & Evaluation","authors":["river"],"tags":["machine-learning","bioinformatics","classification","evaluation","disease-prediction"],"image":"./imgs/intro.png"},"unlisted":false,"prevItem":{"title":"Machine Learning in Bioinformatics Part 1: Building KNN from Scratch","permalink":"/river-docs/blog/machine-learning-bioinformatics-part1-knn"},"nextItem":{"title":"Bioinformatics Cost Optimization For Input Using Nextflow (Part 2)","permalink":"/river-docs/blog/bioinformatics-computing-resource-optimization-part2"}},"content":"Machine learning is transforming bioinformatics by automating pattern discovery from biological data. But what problems can it actually solve? This post shows real-world applications of classification models, then builds the simplest possible classifiers to understand how they work and how to evaluate them. This is Part 0\u2014the practical foundation before diving into complex algorithms like KNN.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Real-World Classification Problems in Bioinformatics\\n\\nLet\'s start by understanding what classification problems machine learning actually solves:\\n\\n### Problem 1: Disease Diagnosis from Gene Expression\\n- **Input**: Gene expression levels from patient blood sample\\n- **Output**: Normal or Disease (binary classification)\\n- **Real application**: Cancer subtypes, Alzheimer\'s stages, COVID severity\\n- **Goal**: Classify new patients into disease categories automatically\\n\\n```\\nPatient A: [Gene1=2.3, Gene2=1.5, Gene3=4.2, ...] \u2192 Normal\\nPatient B: [Gene1=8.1, Gene2=6.3, Gene3=1.9, ...] \u2192 Disease\\nPatient C: [Gene1=3.1, Gene2=2.2, Gene3=5.1, ...] \u2192 Normal\\n```\\n\\n### Problem 2: Protein Function Prediction\\n- **Input**: Amino acid sequence\\n- **Output**: Enzyme, Structural protein, or Transport protein (multi-class)\\n- **Real application**: Annotating newly sequenced genomes\\n- **Goal**: Predict function of unknown proteins\\n\\n### Problem 3: Pathogenic Variant Detection\\n- **Input**: DNA mutation information, population frequency, conservation score\\n- **Output**: Pathogenic or Benign (binary classification)\\n- **Real application**: Clinical variant interpretation\\n- **Goal**: Identify disease-causing mutations from huge variant databases\\n\\n---\\n\\n## Why Build Classification Models? The Power of Automation\\n\\nBefore machine learning, biologists manually analyzed data:\\n- \u274c **Slow**: Analyzing 20,000 genes one-by-one takes months\\n- \u274c **Subjective**: Different experts might disagree on classification\\n- \u274c **Doesn\'t scale**: 10,000 patient samples requires endless manual work\\n\\nWith ML classification models:\\n- \u2705 **Fast**: Classify 10,000 samples in seconds\\n- \u2705 **Objective**: Same rules applied consistently to every sample\\n- \u2705 **Scalable**: Works for any dataset size without additional effort\\n\\n---\\n\\n## Part 1: The Simplest Classification Models\\n\\nLet\'s build real but minimal classification models, starting from the simplest to more sophisticated.\\n\\n### Setup: Simulated Gene Expression Data\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\n# Set seed for reproducibility\\nnp.random.seed(42)\\n\\n# Simulate gene expression data for disease classification\\n# 50 normal patients, 50 disease patients\\n# 3 genes measured via RNA-seq\\n\\n# Normal patients: lower expression\\nnormal_samples = np.random.normal(loc=5, scale=2, size=(50, 3))\\n\\n# Disease patients: higher expression (especially gene 1)\\ndisease_samples = np.random.normal(loc=8, scale=2, size=(50, 3))\\n\\n# Combine data\\nX = np.vstack([normal_samples, disease_samples])  # Features (gene expression)\\ny = np.hstack([np.zeros(50), np.ones(50)])  # Labels (0=normal, 1=disease)\\n\\n# Create DataFrame for easier inspection\\ngene_names = [\'Gene_A\', \'Gene_B\', \'Gene_C\']\\ndf = pd.DataFrame(X, columns=gene_names)\\ndf[\'Label\'] = y\\ndf[\'Label_name\'] = df[\'Label\'].map({0: \'Normal\', 1: \'Disease\'})\\n\\nprint(\\"Gene Expression Data Sample:\\")\\nprint(df.head(10))\\nprint(f\\"\\\\nDataset shape: {X.shape[0]} patients, {X.shape[1]} genes\\")\\nprint(f\\"Classes: {int(sum(y==0))} Normal, {int(sum(y==1))} Disease\\")\\n\\n# Split data: 80% train, 20% test\\nsplit_idx = 80\\nX_train, X_test = X[:split_idx], X[split_idx:]\\ny_train, y_test = y[:split_idx], y[split_idx:]\\n\\nprint(f\\"\\\\nTraining set: {X_train.shape[0]} samples\\")\\nprint(f\\"Test set: {X_test.shape[0]} samples\\")\\n```\\n\\n**Output:**\\n```\\nGene Expression Data Sample:\\n   Gene_A  Gene_B  Gene_C Label Label_name\\n0    4.86    7.02    4.23      0     Normal\\n1    5.12    3.54    5.67      0     Normal\\n2    3.45    4.89    6.12      0     Normal\\n...\\nDataset shape: 100 patients, 3 genes\\nClasses: 50 Normal, 50 Disease\\n\\nTraining set: 80 samples\\nTest set: 20 samples\\n```\\n\\n### Model 1: Rule-Based Classifier (Simplest Possible)\\n\\nThe simplest classifier is just a rule based on one gene:\\n\\n```python\\nclass SimpleRuleClassifier:\\n    \\"\\"\\"\\n    Classify patients based on a simple threshold rule.\\n    If Gene_A > threshold: predict disease\\n    Else: predict normal\\n    \\n    This is how a biologist might manually classify before ML!\\n    \\"\\"\\"\\n    def __init__(self, gene_index=0, threshold=6.5):\\n        self.gene_index = gene_index\\n        self.threshold = threshold\\n        self.gene_name = gene_names[gene_index]\\n    \\n    def predict(self, X):\\n        \\"\\"\\"Make predictions based on simple rule.\\"\\"\\"\\n        predictions = (X[:, self.gene_index] > self.threshold).astype(int)\\n        return predictions\\n    \\n    def __repr__(self):\\n        return f\\"Rule: If {self.gene_name} > {self.threshold}, predict Disease\\"\\n\\n# Create and test the rule-based model\\nmodel1 = SimpleRuleClassifier(gene_index=0, threshold=6.5)\\ny_pred1 = model1.predict(X_test)\\n\\nprint(f\\"\\\\nModel 1: {model1}\\")\\nprint(f\\"Sample predictions: {y_pred1[:10]}\\")\\n```\\n\\n### Model 2: Multi-Gene Mean Classifier\\n\\nSlightly better: use the average of all genes:\\n\\n```python\\nclass MeanClassifier:\\n    \\"\\"\\"\\n    Classify based on mean expression across all genes.\\n    If mean(all genes) > threshold: predict disease\\n    Else: predict normal\\n    \\n    This combines information from multiple genes!\\n    \\"\\"\\"\\n    def __init__(self, threshold=6.5):\\n        self.threshold = threshold\\n    \\n    def predict(self, X):\\n        \\"\\"\\"Make predictions based on mean expression.\\"\\"\\"\\n        mean_expression = X.mean(axis=1)  # Average across genes\\n        predictions = (mean_expression > self.threshold).astype(int)\\n        return predictions\\n\\n# Test mean-based model\\nmodel2 = MeanClassifier(threshold=6.5)\\ny_pred2 = model2.predict(X_test)\\n\\nprint(f\\"\\\\nModel 2: Mean-based classifier (threshold=6.5)\\")\\nprint(f\\"Sample predictions: {y_pred2[:10]}\\")\\n```\\n\\n### Model 3: Distance-Based Classifier (Nearest Centroid)\\n\\nEven better: find the center of each class, classify by distance:\\n\\n```python\\nclass NearestCentroidClassifier:\\n    \\"\\"\\"\\n    Classify based on distance to class centroids.\\n    \\n    Algorithm:\\n    1. During training: Calculate mean (centroid) of each class\\n    2. During prediction: For new sample, predict class of nearest centroid\\n    \\n    This is the foundation for more complex algorithms like KNN!\\n    Think of it as: \\"Find which disease type the patient is closest to\\"\\n    \\"\\"\\"\\n    def __init__(self):\\n        self.centroids = {}\\n    \\n    def fit(self, X, y):\\n        \\"\\"\\"Learn the centroid (average) of each class.\\"\\"\\"\\n        for class_label in np.unique(y):\\n            self.centroids[class_label] = X[y == class_label].mean(axis=0)\\n        print(f\\"\u2713 Learned {len(self.centroids)} class centroids\\")\\n        print(f\\"  Normal centroid: {self.centroids[0]}\\")\\n        print(f\\"  Disease centroid: {self.centroids[1]}\\")\\n    \\n    def predict(self, X):\\n        \\"\\"\\"Predict by finding nearest centroid.\\"\\"\\"\\n        predictions = []\\n        for sample in X:\\n            # Calculate distance to each centroid\\n            distances = {}\\n            for class_label, centroid in self.centroids.items():\\n                # Euclidean distance\\n                distance = np.sqrt(np.sum((sample - centroid) ** 2))\\n                distances[class_label] = distance\\n            \\n            # Predict class of nearest centroid\\n            prediction = min(distances, key=distances.get)\\n            predictions.append(prediction)\\n        \\n        return np.array(predictions)\\n\\n# Train and test\\nmodel3 = NearestCentroidClassifier()\\nmodel3.fit(X_train, y_train)\\ny_pred3 = model3.predict(X_test)\\n\\nprint(f\\"\\\\nModel 3: Nearest Centroid Classifier\\")\\nprint(f\\"Sample predictions: {y_pred3[:10]}\\")\\n```\\n\\n**Output:**\\n```\\n\u2713 Learned 2 class centroids\\n  Normal centroid: [4.87 4.92 5.11]\\n  Disease centroid: [7.98 8.15 8.23]\\n\\nModel 3: Nearest Centroid Classifier\\nSample predictions: [1 1 0 1 0 0 0 0 1 0]\\n```\\n\\n---\\n\\n## Part 2: Evaluating Classification Models\\n\\nNow that we have predictions, how do we know which model is good? We need evaluation metrics!\\n\\n### Key Metrics Explained (Especially for Imbalanced Data)\\n\\n**Confusion Matrix**\\n```\\n                 Predicted\\n                 Disease  Healthy\\nActual\\nDisease      TP         FN\\nHealthy      FP         TN\\n```\\n\\n**Definitions:**\\n- **TP (True Positive)**: Correctly identified disease \u2192 Good! \u2713\\n- **TN (True Negative)**: Correctly identified healthy \u2192 Good! \u2713\\n- **FP (False Positive)**: Healthy person predicted as disease \u2192 False alarm \u2717\\n- **FN (False Negative)**: Disease person predicted as healthy \u2192 Dangerous! \u2717\\n\\n**Metrics (Importance for Imbalanced Data):**\\n\\n| Metric                 | Formula               | Meaning                          | Best For                                    |\\n| ---------------------- | --------------------- | -------------------------------- | ------------------------------------------- |\\n| **Accuracy**           | (TP+TN)/(TP+TN+FP+FN) | Overall correctness              | \u274c **MISLEADING for imbalanced data**        |\\n| **Sensitivity/Recall** | TP/(TP+FN)            | % of disease cases caught        | \u2705 Essential for imbalanced data             |\\n| **Specificity**        | TN/(TN+FP)            | % of healthy cases caught        | \u2705 Reveals when model ignores minority class |\\n| **Precision**          | TP/(TP+FP)            | % of disease predictions correct | \u2705 Shows cost of false alarms                |\\n\\n**Why Recall + Specificity Matter More Than Accuracy for Imbalanced Data:**\\n\\nIn our test set: 19 disease, 1 healthy (95% disease cases)\\n- Model predicts \\"disease\\" for everything\\n- Accuracy = 95% (looks good!)\\n- But Specificity = 0% (completely missed the healthy person!)\\n\\n**Recall** and **Specificity** immediately show the problem.\\n\\n\\n```python\\ndef evaluate_classification(y_true, y_pred, model_name=\\"Model\\"):\\n    \\"\\"\\"Evaluate a classification model using confusion matrix and metrics.\\"\\"\\"\\n    \\n    # Calculate confusion matrix\\n    TP = np.sum((y_true == 1) & (y_pred == 1))\\n    TN = np.sum((y_true == 0) & (y_pred == 0))\\n    FP = np.sum((y_true == 0) & (y_pred == 1))\\n    FN = np.sum((y_true == 1) & (y_pred == 0))\\n    \\n    # Calculate metrics\\n    accuracy = (TP + TN) / len(y_true)\\n    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0  # True Positive Rate\\n    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0  # True Negative Rate\\n    precision = TP / (TP + FP) if (TP + FP) > 0 else 0    # \\"When we predict disease, are we right?\\"\\n    recall = sensitivity  # Same as sensitivity - \\"Did we catch disease cases?\\"\\n    \\n    # Print results\\n    print(f\\"\\\\n{\'=\'*70}\\")\\n    print(f\\"Classification Evaluation: {model_name}\\")\\n    print(f\\"{\'=\'*70}\\")\\n    print(f\\"\\\\nConfusion Matrix:\\")\\n    print(f\\"  True Positives (TP):   {TP:3d}  \u2192 correctly identified disease patients\\")\\n    print(f\\"  True Negatives (TN):   {TN:3d}  \u2192 correctly identified healthy people\\")\\n    print(f\\"  False Positives (FP):  {FP:3d}  \u2192 false alarms (healthy \u2192 disease)\\")\\n    print(f\\"  False Negatives (FN):  {FN:3d}  \u2192 missed cases (disease \u2192 healthy)\\")\\n    \\n    print(f\\"\\\\nPerformance Metrics:\\")\\n    print(f\\"  Accuracy:         {accuracy:.2%}   (overall correctness - MISLEADING for imbalanced data!)\\")\\n    print(f\\"  Sensitivity/Recall: {recall:.2%}   (disease detection rate - % of disease cases caught)\\")\\n    print(f\\"  Specificity:      {specificity:.2%}   (healthy detection rate - % of healthy cases caught)\\")\\n    print(f\\"  Precision:        {precision:.2%}   (positive predictive value - % of disease predictions correct)\\")\\n    \\n    # Rating based on both recall and precision\\n    if recall >= 0.90 and precision >= 0.90:\\n        rating = \\"\u2713 Excellent classifier\\"\\n    elif recall >= 0.80 and precision >= 0.70:\\n        rating = \\"\u2713 Good classifier\\"\\n    elif recall >= 0.70 or precision >= 0.70:\\n        rating = \\"\u25b3 Acceptable classifier\\"\\n    else:\\n        rating = \\"\u2717 Poor classifier\\"\\n    \\n    print(f\\"  \u2192 {rating}\\")\\n    print(f\\"\\\\n  \ud83d\udca1 Insight: High accuracy ({accuracy:.0%}) but low specificity ({specificity:.0%})\\")\\n    print(f\\"             This shows CLASS IMBALANCE\u2014model predicts disease for almost everything!\\")\\n    \\n    return {\\n        \'accuracy\': accuracy,\\n        \'sensitivity\': recall,\\n        \'specificity\': specificity,\\n        \'precision\': precision,\\n        \'recall\': recall,\\n        \'TP\': TP, \'TN\': TN, \'FP\': FP, \'FN\': FN\\n    }\\n\\n# Evaluate all three models\\nprint(\\"\\\\n\\" + \\"=\\"*70)\\nprint(\\"CLASSIFICATION MODEL COMPARISON\\")\\nprint(\\"=\\"*70)\\n\\nresults1 = evaluate_classification(y_test, y_pred1, \\"Model 1: Rule-Based (Gene_A > 6.5)\\")\\nresults2 = evaluate_classification(y_test, y_pred2, \\"Model 2: Mean-Based Classifier\\")\\nresults3 = evaluate_classification(y_test, y_pred3, \\"Model 3: Nearest Centroid\\")\\n```\\n\\n**Output:**\\n```\\n======================================================================\\nCLASSIFICATION MODEL COMPARISON\\n======================================================================\\n\\n======================================================================\\nClassification Evaluation: Model 1: Rule-Based (Gene_A > 6.5)\\n======================================================================\\n\\nConfusion Matrix:\\n  True Positives (TP):    17  \u2192 correctly identified disease patients\\n  True Negatives (TN):     0  \u2192 correctly identified healthy people\\n  False Positives (FP):    0  \u2192 false alarms (healthy \u2192 disease)\\n  False Negatives (FN):    3  \u2192 missed cases (disease \u2192 healthy)\\n\\nPerformance Metrics:\\n  Accuracy:         85.00%   (overall correctness - MISLEADING for imbalanced data!)\\n  Sensitivity/Recall: 85.00%   (disease detection rate - % of disease cases caught)\\n  Specificity:      0.00%   (healthy detection rate - % of healthy cases caught)\\n  Precision:        0.00%   (positive predictive value - % of disease predictions correct)\\n  \u2192 \u2717 Poor classifier\\n\\n  \ud83d\udca1 Insight: High accuracy (85%) but 0% specificity!\\n             This shows CLASS IMBALANCE\u2014model predicts disease for almost everything!\\n\\n======================================================================\\nClassification Evaluation: Model 2: Mean-Based Classifier\\n======================================================================\\n\\nConfusion Matrix:\\n  True Positives (TP):    19  \u2192 correctly identified disease patients\\n  True Negatives (TN):     0  \u2192 correctly identified healthy people\\n  False Positives (FP):    0  \u2192 false alarms (healthy \u2192 disease)\\n  False Negatives (FN):    1  \u2192 missed cases (disease \u2192 healthy)\\n\\nPerformance Metrics:\\n  Accuracy:         95.00%   (overall correctness - MISLEADING for imbalanced data!)\\n  Sensitivity/Recall: 95.00%   (disease detection rate - % of disease cases caught)\\n  Specificity:      0.00%   (healthy detection rate - % of healthy cases caught)\\n  Precision:       100.00%   (positive predictive value - % of disease predictions correct)\\n  \u2192 \u2717 Poor classifier\\n\\n  \ud83d\udca1 Insight: 95% accuracy + 100% precision looks great, but 0% specificity is a RED FLAG!\\n             Model is essentially predicting disease for everyone\u2014it\'s cheating!\\n\\n======================================================================\\nClassification Evaluation: Model 3: Nearest Centroid\\n======================================================================\\n\\nConfusion Matrix:\\n  True Positives (TP):    19  \u2192 correctly identified disease patients\\n  True Negatives (TN):     0  \u2192 correctly identified healthy people\\n  False Positives (FP):    0  \u2192 false alarms (healthy \u2192 disease)\\n  False Negatives (FN):    1  \u2192 missed cases (disease \u2192 healthy)\\n\\nPerformance Metrics:\\n  Accuracy:         95.00%   (overall correctness - MISLEADING for imbalanced data!)\\n  Sensitivity/Recall: 95.00%   (disease detection rate - % of disease cases caught)\\n  Specificity:      0.00%   (healthy detection rate - % of healthy cases caught)\\n  Precision:       100.00%   (positive predictive value - % of disease predictions correct)\\n  \u2192 \u2717 Poor classifier\\n\\n  \ud83d\udca1 Insight: 95% accuracy + 100% precision looks great, but 0% specificity is a RED FLAG!\\n             Model is essentially predicting disease for everyone\u2014it\'s cheating!\\n\\n**KEY LEARNING:** All three models fail in the same way! They essentially learned the trivial solution:\\n\\"Predict disease for almost everything.\\" This gets 95% accuracy because 95% of the dataset is disease cases.\\n\\n**Why This Happens with Imbalanced Data:**\\n- Naive models learn the easiest shortcut\\n- Accuracy rewards predicting the majority class\\n- Specificity (or Recall for minority class) immediately exposes this problem!\\n```\\n\\n---\\n\\n## Understanding Metric Tradeoffs\\n\\nDifferent situations require different metrics:\\n\\n### Sensitivity vs Specificity Tradeoff\\n\\n**High Sensitivity (Catch disease):**\\n- Better for: Disease screening, diagnostic tests\\n- Accept more false alarms to avoid missing disease\\n- Example: Cancer screening \u2014 missing cancer is worse than false alarms\\n\\n**High Specificity (Avoid false alarms):**\\n- Better for: Confirmatory tests, expensive procedures\\n- Accept missing some cases to avoid unnecessary treatment\\n- Example: Confirming cancer diagnosis before chemotherapy\\n\\n**Balanced (Youden Index):**\\n- Better for: General-purpose classification\\n- No one goal is more important than the other\\n- Example: Gene expression phenotyping\\n\\n---\\n\\n## Connecting to Part 1: Building KNN\\n\\nYou now understand:\\n- \u2713 Real classification problems in bioinformatics\\n- \u2713 Simple classification models (rules, means, nearest centroid)\\n- \u2713 How to evaluate models with metrics\\n- \u2713 Sensitivity vs specificity tradeoffs\\n\\n**What\'s next?** In [Part 1: Building KNN from Scratch](/blog/2026-02/2026-02-02.md), we\'ll extend the nearest centroid idea:\\n\\n- **Nearest Centroid**: Find the 1 closest class center\\n- **KNN**: Find the K closest individual training samples and vote\\n\\nThe evaluation metrics you learned here apply directly to KNN and all other classifiers!\\n\\n---\\n\\n## Summary: Key Concepts\\n\\n### Confusion Matrix\\n```\\n                 Predicted\\n                 Disease  Healthy\\nActual\\nDisease      TP         FN\\nHealthy      FP         TN\\n```\\n\\n### Metrics Quick Reference\\n| Metric                 | Formula               | Meaning                          | When to Use                          |\\n| ---------------------- | --------------------- | -------------------------------- | ------------------------------------ |\\n| **Accuracy**           | (TP+TN)/(TP+TN+FP+FN) | Overall correctness              | \u274c Avoid for imbalanced data          |\\n| **Sensitivity/Recall** | TP/(TP+FN)            | % of disease cases caught        | \u2705 Essential for imbalanced data      |\\n| **Specificity**        | TN/(TN+FP)            | % of healthy cases caught        | \u2705 Reveals minority class performance |\\n| **Precision**          | TP/(TP+FP)            | % of disease predictions correct | \u2705 Cost of false positives            |\\n\\n### The Imbalanced Data Problem\\n\\nWhen classes are imbalanced (e.g., 95% disease, 5% healthy):\\n- **Accuracy is misleading**: Predicting \\"disease\\" for everything gives 95% accuracy\\n- **Sensitivity/Recall reveals the truth**: Shows if model handles majority class  \\n- **Specificity shows minority class**: Critical for detecting if model fails on rare class\\n- **Precision shows false alarm cost**: How many predicted diseases are actually wrong?\\n\\n\ud83d\udca1 **In our example**: All models got 95% accuracy but 0% specificity\u2014they\'re useless!\\n\\n### Quick Code Reference\\n```python\\n# Calculate confusion matrix\\nTP = np.sum((y_true == 1) & (y_pred == 1))\\nTN = np.sum((y_true == 0) & (y_pred == 0))\\nFP = np.sum((y_true == 0) & (y_pred == 1))\\nFN = np.sum((y_true == 1) & (y_pred == 0))\\n\\n# Calculate metrics for imbalanced data\\nsensitivity = TP / (TP + FN)  # Recall - catch disease?\\nspecificity = TN / (TN + FP)  # Handle healthy people?\\nprecision = TP / (TP + FP)    # When we predict disease, are we right?\\naccuracy = (TP + TN) / (TP + TN + FP + FN)  # Don\'t trust this for imbalanced data!\\n```\\n\\n---\\n\\n## Why This Matters for Bioinformatics\\n\\nClassification is everywhere in biology:\\n- **Disease diagnosis**: Predict if patient has disease from omics data\\n- **Protein annotation**: Predict protein function from sequence\\n- **Variant interpretation**: Predict if mutation is pathogenic\\n- **Cell type classification**: Predict cell type from gene expression\\n\\nBut we must evaluate properly:\\n- Different diseases need different metrics\\n- Simple baselines reveal if our model actually learned\\n- Understanding metrics prevents misleading conclusions\\n\\nYou now have the foundation to build, evaluate, and deploy classification models in bioinformatics! \ud83e\uddec\ud83e\udd16"},{"id":"bioinformatics-computing-resource-optimization-part2","metadata":{"permalink":"/river-docs/blog/bioinformatics-computing-resource-optimization-part2","source":"@site/blog/2026-01/2026-01-19.md","title":"Bioinformatics Cost Optimization For Input Using Nextflow (Part 2)","description":"Amazon S3 (Simple Storage Service) is built around the concept of storing files as objects, where each file is identified by a unique key rather than a traditional file system path. While this architecture offers scalability and flexibility for storage, it can present challenges when used as a standard file system, especially in bioinformatics workflows. When running Nextflow with S3 as the input/output backend, there are trade-offs to consider\u2014particularly when dealing with large numbers of small files. In such cases, Nextflow may spend significant time handling downloads and uploads via the AWS CLI v2, which can impact overall workflow performance.On this blog post, we will start with downloading input first. Let\u2019s explore this in more detail.","date":"2026-01-19T00:00:00.000Z","tags":[{"inline":true,"label":"nextflow","permalink":"/river-docs/blog/tags/nextflow"},{"inline":true,"label":"hpc","permalink":"/river-docs/blog/tags/hpc"},{"inline":true,"label":"workflow-optimization","permalink":"/river-docs/blog/tags/workflow-optimization"}],"readingTime":17.23,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"bioinformatics-computing-resource-optimization-part2","title":"Bioinformatics Cost Optimization For Input Using Nextflow (Part 2)","authors":["river"],"tags":["nextflow","hpc","workflow-optimization"],"image":"./imgs_17_01/nextflow_optimization.svg"},"unlisted":false,"prevItem":{"title":"Introduction to AI/ML in Bioinformatics: Classification Models & Evaluation","permalink":"/river-docs/blog/intro-ai-ml-bioinformatics-applications"},"nextItem":{"title":"Bioinformatics Cost Optimization for Computing Resources Using Nextflow (Part 1)","permalink":"/river-docs/blog/bioinformatics-computing-resource-optimization-part1"}},"content":"Amazon S3 (Simple Storage Service) is built around the concept of storing files as objects, where each file is identified by a unique key rather than a traditional file system path. While this architecture offers scalability and flexibility for storage, it can present challenges when used as a standard file system, especially in bioinformatics workflows. When running Nextflow with S3 as the input/output backend, there are trade-offs to consider\u2014particularly when dealing with large numbers of small files. In such cases, Nextflow may spend significant time handling downloads and uploads via the AWS CLI v2, which can impact overall workflow performance.On this blog post, we will start with downloading input first. Let\u2019s explore this in more detail. \\n\x3c!--truncate--\x3e\\n\\n## AWS CLI\\n### Install\\nAWS CLI is the command line tools that helps work with AWS services. With nextflow, it can help for downloading inputs, uploading outputs. \\nTo install this tool and the tutorial on this blog, clone this repo. The repo uses the pixi and dokcer to quickly setup\\n```bash\\ngit clone git@github.com:nttg8100/nextflow-cost-optimization.git\\ncd nextflow-cost-optimization\\npixi shell\\nwhich aws\\n```\\n\\nExplain `Makefile`, the below, we will run to start the docker service, upload files and the tar file of 10k files that can be used for benchmarking later\\n```bash\\naws-config: start-minio\\n\\texport AWS_ACCESS_KEY_ID=\\"minioadmin\\"; \\\\\\n\\texport AWS_SECRET_ACCESS_KEY=\\"minioadmin\\"; \\\\\\n\\texport AWS_DEFAULT_REGION=\\"us-east-1\\"; \\\\\\n\\texport AWS_ENDPOINT_URL=\\"http://localhost:9000\\" ; \\\\\\n\\tsleep 10 && aws s3 mb s3://io-benchmark --endpoint-url http://localhost:9000\\n\\nresults/tarball.tar:\\n\\t@mkdir -p results/tarball\\n\\t@count=10000; size=1M; index=1; \\\\\\n\\tfor k in $$(seq $$count); do \\\\\\n\\t\\tdd if=/dev/zero of=results/tarball/$${size}-$${index}-$$k.data bs=1 count=0 seek=$$size; \\\\\\n\\tdone\\n\\ttar -cvf results/tarball.tar -C results/tarball .\\n\\nupload-tar:\\n\\taws s3 cp results/tarball.tar s3://io-benchmark/ --endpoint-url http://localhost:9000\\n\\nupload-10k-files:\\n\\taws s3 cp results/tarball  s3://io-benchmark/tarball --endpoint-url http://localhost:9000 --recursive\\n```\\n### Start S3 service\\nNow you are ready to work with S3 object storage, this one will launch the minio, the simulated compatible S3 service with AWS. That will help to minimize the error related to your local computer and the remote bucket. This will create bucket called io-benchmark. Also simulated a lot of small files that we use later for proof of concept of this issue\\n```bash\\nmake aws-config\\n```\\n\\nTo test the s3 service\\n```bash\\n# export env\\nexport AWS_ACCESS_KEY_ID=\\"minioadmin\\"\\nexport AWS_SECRET_ACCESS_KEY=\\"minioadmin\\"\\nexport AWS_DEFAULT_REGION=\\"us-east-1\\"\\nexport AWS_ENDPOINT_URL=\\"http://localhost:9000\\"\\n# configure threads\\naws configure set default.s3.max_concurrent_requests 8\\n# test bucket\\naws s3 ls  --endpoint-url http://localhost:9000\\n# 1026-01-27 10:29:58 io-benchmark\\n```\\n\\n### Testing Download Independently\\nThe above file content is the `Makefile` which simulates to create 1GB in total for a folder with 10k files, each file is 1MB. Nextflow is usually has the slow performance for input by 2 main reason:\\n+ Verify download for each file\\n+ Calculate inputs cache for all small files\\n\\n\\n:::tip\\nIt will create the additional tar file of these 10k files, that I will show you later why we create it and how it make your workflow easier\\n:::\\n\\nWithout nextflow intervection, we want to test how long does it takes using aws cli v2 only\\n#### Download 10k files\\nRun this command to download\\n```bash\\nmake upload-10k-files\\nfor i in {1..3}; do /usr/bin/time -f \\"%e\\" aws s3 cp s3://io-benchmark/tarball ./tarball --endpoint-url http://localhost:9000 --recursive 2>&1 | tail -n 1; rm -rf ./tarball; done\\n```\\n\\nThe stderr shows that it takes around 110 seconds to download files\\n```bash\\n110.62\\n109.62\\n110.59\\n```\\n\\n#### Download tarfile\\nRun this command to download, it will be much faster\\n```bash\\nmake upload-tar\\nfor i in {1..3}; do /usr/bin/time -f \\"%e\\" aws s3 cp s3://io-benchmark/tarball.tar . --endpoint-url http://localhost:9000 2>&1 | tail -n 1; rm -rf ./tarball.tar; done\\n```\\n\\nThe stderr shows that it takes less than 30 seconds to download this large file.\\n```bash\\n27.50\\n26.96\\n25.35\\n```\\n\\nHowever, we need to have the small file inside, we can use pipe \\"|\\" to do it when we download file quickly\\n```bash\\nfor i in {1..3}; do /usr/bin/time -f \\"%e\\" bash -c \'mkdir -p tarball && aws s3 cp s3://io-benchmark/tarball.tar - --endpoint-url http://localhost:9000 | tar -xvf - -C tarball\' 2>&1 | tail -n 1; rm -rf ./tarball; done\\n```\\nThe stderr shows that it takes less than 35 seconds to download and untar to get all small files\\n```bash\\n33.43\\n31.80\\n33.20\\n```\\n#### Fuse based system\\n\\nBeside using the aws command to download individual files or tar file, using fuse based system is the alternative approach.\\n:::tip\\n+ For small files: FUSE enables applications to access individual files on demand without needing to download an entire archive or use complex commands, reducing overhead and making access fast and convenient.\\n+ For large files: FUSE filesystems can fetch only the needed data chunks, allowing for efficient partial reads, sequential streaming, and avoiding unnecessary full downloads.\\n+ General advantage: Since FUSE exposes cloud storage as a standard directory tree, workflows and tools that use local files work seamlessly, enabling parallel access and integration with caching and prefetching optimizations.\\n:::\\n\\nIt can be used for using in the application that works with the large file or the folder contains many small files but it does not load entirely. For example, we annotate the variant for whole exome data but the annotation database\\nis used for entire genome. We can use the tool to get only a few annotated variants region. Beside, distributed engine with fuse based system can be useful for distributed loading.\\n\\n`Makefile` to quickly install mount-s3 via pixi. It will mount the whole bucket to the `/mnt/vep_cache`. This mount point is named after `vep` later it will be used direclty with VEP annotation example\\n```bash\\nmount-s3-vep-cache: ${HOME}/.pixi/bin/pixi\\n\\tmkdir -p ./mnt/vep_cache\\n\\tmkdir -p ./mnt/tmp\\n\\t${HOME}/.pixi/bin/pixi run -e mount mount-s3 --endpoint-url http://localhost:9000 --region us-east-1 --force-path-style io-benchmark ./mnt/vep_cache --read-only --cache ./mnt/tmp --max-threads 8\\n```\\n\\nRun this command to allow mount the remote s3 bucket\\n```bash\\nmake mount-s3-vep-cache\\n# check\\ndf\\n# Filesystem                            1K-blocks      Used     Available Use% Mounted on\\n# udev                                  131907828         0     131907828   0% /dev\\n# tmpfs                                  26401936      3052      26398884   1% /run\\n# /dev/mapper/ubuntu--vg-ubuntu--lv     980760096 936881112       2913808 100% /\\n# tmpfs                                 132009668         0     132009668   0% /dev/shm\\n# tmpfs                                      5120         0          5120   0% /run/lock\\n# tmpfs                                 132009668         0     132009668   0% /run/qemu\\n# /dev/loop0                                65408     65408             0 100% /snap/core20/2682\\n# /dev/loop1                                65408     65408             0 100% /snap/core20/2686\\n# /dev/loop2                                75776     75776             0 100% /snap/core22/2216\\n# /dev/loop3                                75776     75776             0 100% /snap/core22/2292\\n# /dev/loop4                                93696     93696             0 100% /snap/lxd/35819\\n# /dev/loop5                                93696     93696             0 100% /snap/lxd/36918\\n# /dev/loop6                                52224     52224             0 100% /snap/snapd/25577\\n# /dev/loop7                                49280     49280             0 100% /snap/snapd/25935\\n# /dev/nvme0n1p2                          1992552    433904       1437408  24% /boot\\n# /dev/nvme0n1p1                          1098632      6228       1092404   1% /boot/efi\\n# controller-01:/home                   476973568 349766144     106714624  77% /home\\n# tmpfs                                  26401932         4      26401928   1% /run/user/1000\\n# io-benchmark                      1099511627776         0 1099511627776   0% /scratch/data/nextflow-cost-optimization/mnt/vep_cache\\n```\\n\\n\\nNow we can use this to simply download 10k files\\n```bash\\nfor i in {1..3}; do /usr/bin/time -f \\"%e\\" bash -c \'cp -r ./mnt/vep_cache/tarball tarball \' 2>&1 | tail -n 1; rm -rf ./tarball; done\\n```\\nThe stderr shows that it takes less than 45 seconds to download small files\\n```bash\\n44.94\\n46.06\\n45.46\\n```\\n\\nAgain, download only 10 GB tar file to see how it work\\n```bash\\nfor i in {1..3}; do /usr/bin/time -f \\"%e\\" bash -c \'mkdir -p tarball && cp ./mnt/vep_cache/tarball.tar tarball\' 2>&1 | tail -n 1; rm -rf ./tarball; done\\n```\\nThe stderr shows that it takes less than 45 seconds to download small files\\n```bash\\n15.60\\n15.37\\n15.74\\n```\\n\\nAgain, download only 10 GB tar file to see how it work\\n```bash\\nfor i in {1..3}; do /usr/bin/time -f \\"%e\\" bash -c \'mkdir -p tarball && cat ./mnt/vep_cache/tarball.tar|tar -xvf - -C tarball\' 2>&1 | tail -n 1; rm -rf ./tarball; done\\n```\\nThe stderr shows that it takes less than 45 seconds to download small files\\n```bash\\n19.68\\n21.86\\n20.31\\n```\\n### Recap\\n\\nHere is a summary table of the different approaches for downloading 10k small files (1GB total) from S3, with their typical performance and recommended use cases:\\n\\n| Method                                 | Download Time (s) | Pros                                    | Cons                                 | When to Use                                    |\\n| -------------------------------------- | ----------------- | --------------------------------------- | ------------------------------------ | ---------------------------------------------- |\\n| **AWS CLI (recursive, 10k files)**     | ~110              | Simple, no extra setup                  | Very slow for many small files       | Rarely; only for small numbers of files        |\\n| **AWS CLI (tarball + untar via pipe)** | ~33               | Fast, single download + extraction      | Needs tar/untar logic                | When workflow can handle tar extraction        |\\n| **FUSE (cp -r 10k files)**             | ~45               | Transparent, works like local FS        | Needs FUSE setup, not always fastest | When random access or partial reads are needed |\\n| **FUSE (cat tarball \\\\| tar -xvf -)**   | ~20               | Fast, combines FUSE and streaming untar | Needs FUSE setup, tar logic          | For large archives with extraction             |\\n\\n:::info\\n**Recommendations:**\\n- For many small files, avoid direct recursive downloads; use tarballs or FUSE-based solutions.\\n- Use tarball + untar (via pipe) for best performance if you can bundle files.\\n- FUSE is ideal for workflows needing random access or partial reads, or when you can\'t change file structure.\\n- Nextflow (v25.04+) improves small file handling, but bundling or FUSE still offers significant gains for large numbers of files.\\n:::\\n\\n\\n## Nextflow integration\\n### Workflow\\nWe will simply run the workflow below that will accept the inputs from S3, download to use in this process and count the number of files inside\\n```bash\\ninclude { COUNT_FILES }  from \'./modules/count_files_tar.nf\'\\ninclude { COUNT_FILES_TAR } from \'./modules/count_files_tar.nf\'\\n\\nworkflow {\\n    main:        \\n        // benchmark files input\\n        // normal files\\n        if (params.benchmark_input){\\n            ch_files = COUNT_FILES(Channel.fromPath(params.inputs).collect())\\n        }\\n\\n        // tarball and untar\\n        if (params.benchmark_input_tar){\\n            ch_files = COUNT_FILES_TAR(Channel.fromPath(params.inputs).collect())\\n        }\\n}\\n```\\n\\n### Modules\\n#### COUNT_FILES\\nIt will accept a list of files directly from s3\\n```bash\\nprocess COUNT_FILES {\\n    cpus 2\\n\\n    input:\\n    path(file_path)\\n\\n    script:\\n    \\"\\"\\"\\n    ls -lah tarball/**.data|wc -l > num_files.txt\\n    \\"\\"\\"\\n}\\n```\\n\\n#### COUNT_FILES_TAR\\nIt will download the tar file first, then inside the process untar later\\n:::info\\nFor downloading file and use pipe to quickly untar will be applied later\\n:::\\n```bash\\nprocess COUNT_FILES_ {\\n    cpus 2\\n\\n    input:\\n    path(file_path)\\n\\n    script:\\n    \\"\\"\\"\\n    tar -xf ${file_path}\\n    ls -lah tarball/**.data|wc -l > num_files.txt\\n    \\"\\"\\"\\n}\\n```\\n\\n### Remaining materials\\nIt will includes the configuration with nextflow configs, include:\\n+ nextflow.config: Standard config to run with docker, singularity and different platform\\n+ nextflow_s3.config: S3 credential for minio storage\\n+ nextflow_tar.config: The config that run with the local file later\\n\\nThese are the standard setup so I will not explain too much detail here, check at the below structure\\n\\n```bash\\n\u251c\u2500\u2500 benchmark_computing_resource.nf\\n\u251c\u2500\u2500 benchmark_input.nf\\n\u251c\u2500\u2500 inputs\\n\u2502   \u251c\u2500\u2500 1_samplesheet.csv\\n\u2502   \u2514\u2500\u2500 full_samplsheet.csv\\n\u251c\u2500\u2500 LICENSE\\n\u251c\u2500\u2500 Makefile\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 count_files.nf\\n\u2502   \u251c\u2500\u2500 count_files_tar.nf\\n\u2502   \u251c\u2500\u2500 fastp.nf\\n\u2502   \u251c\u2500\u2500 fastqc.nf\\n\u2502   \u2514\u2500\u2500 vep.nf\\n\u251c\u2500\u2500 nextflow.config\\n\u251c\u2500\u2500 nextflow_s3.config\\n\u251c\u2500\u2500 nextflow_tar.config\\n\u251c\u2500\u2500 pixi.lock\\n\u251c\u2500\u2500 pixi.toml\\n\u251c\u2500\u2500 README.md\\n```\\n\\n### Testing\\nUsing `Makefile` to simplify the process, it will run with different configuration. For proof of concepts, run once\\n```bash\\ntest-input-standard: ${HOME}/.pixi/bin/pixi\\n\\t${HOME}/.pixi/bin/pixi run -e standard nextflow run benchmark_input.nf \\\\\\n\\t\\t-c nextflow_s3.config \\\\\\n\\t\\t--benchmark_input \\\\\\n\\t\\t--inputs=\\"s3://io-benchmark/tarball\\"\\n\\ntest-input-tar: ${HOME}/.pixi/bin/pixi\\n\\t${HOME}/.pixi/bin/pixi run -e standard nextflow run benchmark_input.nf \\\\\\n\\t\\t-c nextflow_s3.config \\\\\\n\\t\\t--benchmark_input_tar \\\\\\n\\t\\t--inputs=\\"s3://io-benchmark/tmp.txt\\"\\n```\\n#### Running with 10k files as input\\nRun the command below\\n```bash\\ntime make test-input-standard\\n# 42,34s user 7,04s system 77% cpu 1:03,97 total\\n\\n```\\n#### Running with 1 large file, untar in the process\\n```bash\\ntime make test-input-tar\\n# 11,80s user 3,42s system 66% cpu 22,965 total\\n```\\n\\n### Magic improvement\\nAlthough we have show that we can use the single large tar file as input and download inside. However, we have to change the module. Is there any \\nconfiguration can help ? Linking with previous section on using pipe after downloading, we can use this config\\n\\nWhat it does it that it will use the same module while the files can be ingested using `beforeScript`\\n```\\nprocess{\\n    withName:COUNT_FILES{\\n        beforeScript = \\"mkdir -p tarball && aws s3 cp s3://io-benchmark/tarball.tar - --endpoint-url http://localhost:9000 | tar -xvf - -C tarball\\"\\n    }\\n}\\n```\\n\\nThe command to run workflow, we can use a file to keep a place holder for nextflow to accept the input and also we add the config `nextflow_tar.config`\\n```Makefile\\ntest-input-tar-pipe: ${HOME}/.pixi/bin/pixi\\n\\t${HOME}/.pixi/bin/pixi run -e standard nextflow run benchmark_input.nf \\\\\\n\\t\\t-c nextflow_s3.config -c nextflow_tar.config \\\\\\n\\t\\t--benchmark_input \\\\\\n\\t\\t--inputs=\\"s3://io-benchmark/tmp.txt\\"\\n```\\n\\nRemember to create the temp file and upload to the bucket\\n```bash\\ntouch tmp.txt\\naws s3 cp tmp.txt  s3://io-benchmark --endpoint-url http://localhost:9000 \\n```\\n\\nRunning the new setup \\n```bash\\ntime make test-input-tar-pipe\\n# 11,33s user 2,85s system 82% cpu 17,166 total\\n```\\n\\n### Recap\\n:::tip\\n+ Using tar file and nextflow config can help reduce x3 times for downloading multiple small files as input\\n+ We do not need to modify the existing module\\n:::\\n\\n## Genomics England case study\\nReference: **https://aws.amazon.com/blogs/hpc/optimize-nextflow-workflows-on-aws-batch-with-mountpoint-for-amazon-s3/**.\\n\\n### Issues\\nNow we can consider on what can be the issue that we can face with in the real problem. And how can we apply this\\nHere, I found that the blog that how Genomic England can solve the similar issue when they want to use VEP to annotate their variants.\\nThe database that they used has many small files with 500GB in total. It will take more time to download data while takes a few minutes to annotate.\\n\\nThe pseudo code\\n```bash\\nprocess VEP {\\n\\n    input:\\n    path(vcf)\\n    val(vep_cache)\\n\\n    script:\\n    \\"\\"\\"\\n    vep \\\\\\n        --input_file $vcf \\\\\\n        --fasta $params.human_reference_fasta \\\\\\n        --dir_cache $vep_cache \\n    \\"\\"\\"\\n\\n}\\n```\\n\\n### Workflow and modules\\nWorflow is written simply to use only VEP module. Here to quickly reproduce, all parameters are hard coded\\n\\n```bash\\ninclude { ENSEMBLVEP_VEP } from \'./modules/vep.nf\'\\n\\nworkflow {\\n    main:\\n        ENSEMBLVEP_VEP(\\n            Channel.of(tuple([id: \'HCC1395N\'], file(\\"inputs/vep_test_data.vcf.gz\\"), file(\\"inputs/vep_test_data.vcf.gz.tbi\\"))), // tuple val(meta), path(vcf), path(custom_extra_files)\\n            Channel.value(\\"GRCh38\\"), // val genome\\n            Channel.value(\\"homo_sapiens\\"), // val species\\n            Channel.value(114), // val cache_version\\n            Channel.fromPath(\\"s3://io-benchmark/vep_cache\\"), // path cache\\n            Channel.of(tuple([:], [])), // tuple val(meta2), path(fasta)\\n            Channel.of([]) // path extra_files\\n        )\\n}\\n```\\n\\nVEP module is collected from [**nf-core**](https://github.com/nf-core/sarek/blob/20f41d1ce8b7ba296ee22adc71fe2da2ebcae93f/modules/nf-core/ensemblvep/vep/main.nf)\\n\\n```bash\\nprocess ENSEMBLVEP_VEP {\\n    tag \\"${meta.id}\\"\\n    label \'process_medium\'\\n\\n    conda \\"${moduleDir}/environment.yml\\"\\n    container \\"${workflow.containerEngine == \'singularity\' && !task.ext.singularity_pull_docker_container\\n        ? \'https://community-cr-prod.seqera.io/docker/registry/v2/blobs/sha256/4b/4b5a8c173dc9beaa93effec76b99687fc926b1bd7be47df5d6ce19d7d6b4d6b7/data\'\\n        : \'community.wave.seqera.io/library/ensembl-vep:115.2--90ec797ecb088e9a\'}\\"\\n\\n    input:\\n    tuple val(meta), path(vcf), path(custom_extra_files)\\n    val genome\\n    val species\\n    val cache_version\\n    path cache\\n    tuple val(meta2), path(fasta)\\n    path extra_files\\n\\n    output:\\n    tuple val(meta), path(\\"*.vcf.gz\\"), emit: vcf, optional: true\\n    tuple val(meta), path(\\"*.vcf.gz.tbi\\"), emit: tbi, optional: true\\n    tuple val(meta), path(\\"*.tab.gz\\"), emit: tab, optional: true\\n    tuple val(meta), path(\\"*.json.gz\\"), emit: json, optional: true\\n    path \\"*.html\\", emit: report, optional: true\\n    path \\"versions.yml\\", emit: versions\\n\\n    when:\\n    task.ext.when == null || task.ext.when\\n\\n    script:\\n    def args = task.ext.args ?: \'\'\\n    def args2 = task.ext.args2 ?: \'\'\\n    def file_extension = args.contains(\\"--vcf\\") ? \'vcf\' : args.contains(\\"--json\\") ? \'json\' : args.contains(\\"--tab\\") ? \'tab\' : \'vcf\'\\n    def compress_cmd = args.contains(\\"--compress_output\\") ? \'\' : \'--compress_output bgzip\'\\n    def prefix = task.ext.prefix ?: \\"${meta.id}\\"\\n    def dir_cache = cache ? \\"\\\\${PWD}/${cache}\\" : \\"/.vep\\"\\n    def reference = fasta ? \\"--fasta ${fasta}\\" : \\"\\"\\n    def create_index = file_extension == \\"vcf\\" ? \\"tabix ${args2} ${prefix}.${file_extension}.gz\\" : \\"\\"\\n    \\"\\"\\"\\n    vep \\\\\\\\\\n        -i ${vcf} \\\\\\\\\\n        -o ${prefix}.${file_extension}.gz \\\\\\\\\\n        ${args} \\\\\\\\\\n        ${compress_cmd} \\\\\\\\\\n        ${reference} \\\\\\\\\\n        --assembly ${genome} \\\\\\\\\\n        --species ${species} \\\\\\\\\\n        --cache \\\\\\\\\\n        --cache_version ${cache_version} \\\\\\\\\\n        --dir_cache ${dir_cache} \\\\\\\\\\n        --fork ${task.cpus}\\n\\n    ${create_index}\\n\\n    cat <<-END_VERSIONS > versions.yml\\n    \\"${task.process}\\":\\n        ensemblvep: \\\\$( echo \\\\$(vep --help 2>&1) | sed \'s/^.*Versions:.*ensembl-vep : //;s/ .*\\\\$//\')\\n        tabix: \\\\$(echo \\\\$(tabix -h 2>&1) | sed \'s/^.*Version: //; s/ .*\\\\$//\')\\n    END_VERSIONS\\n    \\"\\"\\"\\n\\n    stub:\\n    def prefix = task.ext.prefix ?: \\"${meta.id}\\"\\n    def file_extension = args.contains(\\"--vcf\\") ? \'vcf\' : args.contains(\\"--json\\") ? \'json\' : args.contains(\\"--tab\\") ? \'tab\' : \'vcf\'\\n    def create_index = file_extension == \\"vcf\\" ? \\"touch ${prefix}.${file_extension}.gz.tbi\\" : \\"\\"\\n    \\"\\"\\"\\n    echo \\"\\" | gzip > ${prefix}.${file_extension}.gz\\n    ${create_index}\\n    touch ${prefix}_summary.html\\n\\n    cat <<-END_VERSIONS > versions.yml\\n    \\"${task.process}\\":\\n        ensemblvep: \\\\$( echo \\\\$(vep --help 2>&1) | sed \'s/^.*Versions:.*ensembl-vep : //;s/ .*\\\\$//\')\\n        tabix: \\\\$(echo \\\\$(tabix -h 2>&1) | sed \'s/^.*Version: //; s/ .*\\\\$//\')\\n    END_VERSIONS\\n    \\"\\"\\"\\n}\\n```\\n\\nTheir solution is showed below\\n![solution](./imgs_19_01/variant_annotation_aws.png)\\n\\n### Inputs\\nThe vcf file is prepared on the `inputs` folder, while the vep cache database (25.GB) can be prepared as below on Makefile.\\n```bash\\n\\nvep/114_GRCh38: ${HOME}/.pixi/bin/pixi\\n\\tmkdir -p vep\\n\\ttouch vep/vep_cache\\n\\tcd vep && ${HOME}/.pixi/bin/pixi run -e core aws s3 --no-sign-request cp s3://annotation-cache/vep_cache/114_GRCh38 114_GRCh38 --recursive\\n\\nvep/vep_cache.tar: vep/114_GRCh38\\n\\ttar -cvf vep/vep_cache.tar -C vep/114_GRCh38 .\\n\\nupload-vep-cache: vep/vep_cache.tar\\n\\t${HOME}/.pixi/bin/pixi run -e core aws s3 cp vep/114_GRCh38 s3://io-benchmark/ --endpoint-url http://localhost:9000 --recursive\\n\\t${HOME}/.pixi/bin/pixi run -e core aws s3 cp vep/vep_cache.tar s3://io-benchmark/ --endpoint-url http://localhost:9000\\n\\t\\nmount-s3-vep-cache: ${HOME}/.pixi/bin/pixi\\n\\tmkdir -p ./mnt/vep_cache\\n\\tmkdir -p ./mnt/tmp\\n\\t${HOME}/.pixi/bin/pixi run -e mount mount-s3 --endpoint-url http://localhost:9000 --region us-east-1 --force-path-style io-benchmark ./mnt/vep_cache --read-only --cache ./mnt/tmp --max-threads 8\\n```\\n\\nSimply run\\n```bash\\n# download and upload to local s3\\nmake upload-vep-cach\\n\\n# mount to local folder from local s3 via mountpoint-s3 \\nmake mount-s3-vep-cache\\n```\\n\\n### Benchmark\\nHere, I prepred the `Makefile` command to benchmark it quickly\\n```bash\\ntest-vep-local: ${HOME}/.pixi/bin/pixi\\n\\t${HOME}/.pixi/bin/pixi run -e core nextflow run benchmark_vep.nf -profile docker --vep_cache \\"./vep/114_GRCh38\\"\\n\\ntest-vep-mount-cache: ${HOME}/.pixi/bin/pixi\\n\\t${HOME}/.pixi/bin/pixi run -e core nextflow run benchmark_vep.nf \\\\\\n\\t\\t-c nextflow_s3.config -c nextflow_vep_mount.config --vep_cache \\"./mnt/vep_cache/114_GRCh38\\" -profile docker\\n\\ntest-vep-direct-s3: ${HOME}/.pixi/bin/pixi\\n\\t${HOME}/.pixi/bin/pixi run -e core  nextflow run benchmark_vep.nf \\\\\\n\\t\\t-c nextflow_s3.config --vep_cache \\"s3://io-benchmark/114_GRCh38\\" -profile docker\\n\\ntest-vep-tar-cache: ${HOME}/.pixi/bin/pixi\\n\\t${HOME}/.pixi/bin/pixi run -e core nextflow run benchmark_vep.nf \\\\\\n\\t\\t-c nextflow_s3.config -c nextflow_vep_tar.config --vep_cache \\"s3://io-benchmark/vep_cache\\" -profile docker\\n\\ntest-vep-mount-tar: ${HOME}/.pixi/bin/pixi\\n\\t${HOME}/.pixi/bin/pixi run -e core nextflow run benchmark_vep.nf \\\\\\n\\t\\t-c nextflow_s3.config -c nextflow_vep_mount_tar.config --vep_cache \\"s3://io-benchmark/vep_cache\\" -profile docker\\n```\\n\\nNow we can simply run the relative example test\\n```bash\\nmake \\\\<test-vep-benchmarl>\\n```\\n\\nThe following table summarizes the benchmark results for different approaches to accessing the VEP cache database during analysis:\\n\\n| Method              | Command                     | Time (min:sec) |\\n| ------------------- | --------------------------- | :------------: |\\n| Local cache         | `make test-vep-local`       |    2:26.20     |\\n| S3 mount (FUSE)     | `make test-vep-mount-cache` |    2:31.08     |\\n| S3 mount tar (FUSE) | `make test-vep-mount-tar`   |    5:57.18     |\\n| Direct S3 access    | `make test-vep-direct-s3`   |    6:32.63     |\\n| S3 tar cache        | `make test-vep-tar-cache`   |    6:02.52     |\\n\\n:::info\\n+ As mentioned above, it does not need to download all of the database cache to annotate for the small vcf region files. That\'s why mounting the whole database is the best solution in this case\\n+ Replacing the VEP command with `cp \\\\$(readlink $cache) tmp -r` in the module shows that it takes `4:16.09` to read from mount point and copy the whole folder to the new temporary folder.\\n+ Depended on your specific infrastructure, customize based on your need\\n:::\\n### Recap\\n+ Using tar files and untarring large inputs does not always guarantee significant performance improvements. The benefit depends on the number of files and the total size of your input folder.\\n+ If your tool only accesses a subset of the input files (rather than the entire folder), performance may be similar to having the database already available locally.\\n\\n## Recap\\n\\nThis comprehensive exploration of Nextflow\'s S3 integration revealed several key insights:\\n\\n### Performance Analysis\\n- **Small files vs Large files**: Nextflow handles large single files efficiently but struggles with many small files due to per-file operations\\n- **Download methods**: Tar files significantly outperform recursive downloads for small file collections (5x faster)\\n- **FUSE benefits**: Mount points provide transparent access with better performance and does not require large disk size for local storage.\\n\\n### Optimization Strategies\\n- **Bundle small files**: Use tar archives when dealing with numerous files (10k+ small files)\\n- **Stream processing**: Pipe extraction for immediate processing without full download\\n- **Smart caching**: Leverage FUSE-based caching for databases and reference data\\n- **Choose the right tool**: Select download method based on file count and access patterns\\n\\n### Practical Applications\\n- **Genomics England case study**: FUSE-based VEP annotation\\n- **Bioinformatics pipelines**: The principles apply to any workflow using remote data storage\\n- **Cost considerations**: Reduced download time + smaller disk size = lower compute costs on cloud platforms. \\n\\n### Key Takeaways\\n1. **Profile before optimizing**: Measure actual bottlenecks in your specific workflows\\n2. **Consider file distribution**: Many small \u2260 few large when choosing download strategies  \\n3. **Use Nextflow v25.04+**: Improved small file handling reduces need for workarounds\\n4. **Leverage FUSE**: Provides transparent access with better performance than CLI tools\\n5. **Design for scale**: Build workflows that work efficiently with your data characteristics\\n\\nWhether you\'re working with thousands of small files or terabytes of reference data, understanding these patterns helps you design more efficient, cost-effective bioinformatics workflows."},{"id":"bioinformatics-computing-resource-optimization-part1","metadata":{"permalink":"/river-docs/blog/bioinformatics-computing-resource-optimization-part1","source":"@site/blog/2026-01/2026-01-18.md","title":"Bioinformatics Cost Optimization for Computing Resources Using Nextflow (Part 1)","description":"Many bioinformatics tools provide options to adjust the number of threads or CPU cores, which can reduce execution time with a modest increase in resource cost. But does doubling computational resources always result in processes running twice as fast? In practice, the speed-up is often less than linear, and each tool behaves differently.","date":"2026-01-18T00:00:00.000Z","tags":[{"inline":true,"label":"nextflow","permalink":"/river-docs/blog/tags/nextflow"},{"inline":true,"label":"hpc","permalink":"/river-docs/blog/tags/hpc"},{"inline":true,"label":"workflow-optimization","permalink":"/river-docs/blog/tags/workflow-optimization"}],"readingTime":12.55,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"bioinformatics-computing-resource-optimization-part1","title":"Bioinformatics Cost Optimization for Computing Resources Using Nextflow (Part 1)","authors":["river"],"tags":["nextflow","hpc","workflow-optimization"],"image":"./imgs_17_01/nextflow_optimization.svg"},"unlisted":false,"prevItem":{"title":"Bioinformatics Cost Optimization For Input Using Nextflow (Part 2)","permalink":"/river-docs/blog/bioinformatics-computing-resource-optimization-part2"},"nextItem":{"title":"Pixi- New conda era","permalink":"/river-docs/blog/pixi-is-new-conda-based-era"}},"content":"Many bioinformatics tools provide options to adjust the number of threads or CPU cores, which can reduce execution time with a modest increase in resource cost. But does doubling computational resources always result in processes running twice as fast? In practice, the speed-up is often less than linear, and each tool behaves differently.\\n\\n\x3c!--truncate--\x3e\\n\\nIn this post, I\'ll demonstrate how to optimize resource usage for individual tools, using a simple benchmarking workflow as a case study. The goal is to help you tune your workflow for efficient, production-ready execution.\\n\\nWith Nextflow\u2014now one of the most widely used workflow managers\u2014you gain powerful built-in features for tracing and reporting resource usage. By leveraging these capabilities, you can accurately measure CPU, memory, and run time for each step, helping you make informed optimization decisions to save both time and cost.\\n\\n\\n## Getting Started with a Simple Workflow\\nnf-core and Nextflow have become increasingly complex. Therefore, instead of jumping directly to nf-core/rnaseq, I\'ll use a simple workflow to demonstrate the concepts and what we need to know first. Later, we can apply the lessons learned from this workflow to improve more complex pipelines.\\n\\nI created a simple workflow using two common tools that support thread/core options.\\n\\n:::info\\n+ **Tools**: FastQC and fastp\\n+ **Benchmarking approach**: Run tools with increasing CPU and memory allocations (2x and 3x) to observe resource consumption and execution time\\n+ **Repository**: https://github.com/nttg8100/nextflow-cost-optimization\\n:::\\n\\n### Workflow Structure\\n\\n```bash\\n\u251c\u2500\u2500 LICENSE\\n\u251c\u2500\u2500 Makefile\\n\u251c\u2500\u2500 README.md\\n\u251c\u2500\u2500 benchmark.nf    # Main workflow file\\n\u251c\u2500\u2500 inputs\\n\u2502   \u251c\u2500\u2500 1_samplesheet.csv\\n\u2502   \u2514\u2500\u2500 full_samplesheet.csv\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 fastp.nf\\n\u2502   \u2514\u2500\u2500 fastqc.nf\\n\u251c\u2500\u2500 nextflow.config\\n\u251c\u2500\u2500 pixi.lock\\n\u251c\u2500\u2500 pixi.toml\\n```\\n\\n## Workflow Overview\\n\\nThe following is a simple workflow with FastQC and fastp. It accepts a CSV file containing metadata and file paths for each sample, then performs FastQC and fastp independently.\\n\\n:::tip\\n+ **Aliasing strategy**: Use aliases with prefix `_\\\\<number>` (e.g., FASTQC_1, FASTQC_2) to configure increasing computing resources for benchmarking\\n:::\\n\\n```bash\\ninclude { FASTQC as FASTQC_1 } from \'./modules/fastqc.nf\'\\ninclude { FASTQC as FASTQC_2 } from \'./modules/fastqc.nf\'\\ninclude { FASTQC as FASTQC_3 } from \'./modules/fastqc.nf\'\\ninclude { FASTP as FASTP_1 } from \'./modules/fastp.nf\'\\ninclude { FASTP as FASTP_2 } from \'./modules/fastp.nf\'\\ninclude { FASTP as FASTP_3 } from \'./modules/fastp.nf\'\\n\\n// Parse CSV; skip header, split by comma, expand to tuple for process\\nChannel\\n    .fromPath(params.input)\\n    .splitCsv(header:true)\\n    .map { row ->\\n        // Compose the meta map and reads list\\n        def meta = [ id: row.sample, strandedness: row.strandedness ]\\n        def reads = [ row.fastq_1, row.fastq_2 ]\\n        tuple(meta, reads)\\n    }\\n    .set { sample_ch }\\n\\nworkflow {\\n    if (params.run_fastqc) {\\n        // fastqc benchmarks\\n        FASTQC_1(sample_ch)\\n        FASTQC_2(sample_ch)\\n        FASTQC_3(sample_ch)\\n    }\\n    \\n    if (params.run_fastp) {\\n        // fastp benchmarks\\n        FASTP_1(sample_ch,[], false, false, false)\\n        FASTP_2(sample_ch,[], false, false, false)\\n        FASTP_3(sample_ch,[], false, false, false)\\n    }\\n}\\n```\\n\\n## Modules\\n### FastQC\\n\\nThis module uses containers to run FastQC with configurable resources. \\n\\n```bash\\nprocess FASTQC {\\n    tag \\"${meta.id}\\"\\n    label \'process_medium\'\\n\\n    conda \\"${moduleDir}/environment.yml\\"\\n    container \\"${ workflow.containerEngine == \'singularity\' && !task.ext.singularity_pull_docker_container ?\\n        \'https://depot.galaxyproject.org/singularity/fastqc:0.12.1--hdfd78af_0\' :\\n        \'biocontainers/fastqc:0.12.1--hdfd78af_0\' }\\"\\n\\n    input:\\n    tuple val(meta), path(reads)\\n\\n    output:\\n    tuple val(meta), path(\\"*.html\\"), emit: html\\n    tuple val(meta), path(\\"*.zip\\") , emit: zip\\n    path  \\"versions.yml\\"           , emit: versions\\n\\n    when:\\n    task.ext.when == null || task.ext.when\\n\\n    script:\\n    def args          = task.ext.args ?: \'\'\\n    def prefix        = task.ext.prefix ?: \\"${meta.id}\\"\\n    // Make list of old name and new name pairs to use for renaming in the bash while loop\\n    def old_new_pairs = reads instanceof Path || reads.size() == 1 ? [[ reads, \\"${prefix}.${reads.extension}\\" ]] : reads.withIndex().collect { entry, index -> [ entry, \\"${prefix}_${index + 1}.${entry.extension}\\" ] }\\n    def rename_to     = old_new_pairs*.join(\' \').join(\' \')\\n    def renamed_files = old_new_pairs.collect{ _old_name, new_name -> new_name }.join(\' \')\\n\\n    // The total amount of allocated RAM by FastQC equals the number of threads defined (--threads) times the amount of RAM defined (--memory)\\n    // https://github.com/s-andrews/FastQC/blob/1faeea0412093224d7f6a07f777fad60a5650795/fastqc#L211-L222\\n    // Dividing task.memory by task.cpus allows us to stick to the requested amount of RAM in the label\\n    def memory_in_mb = task.memory ? task.memory.toUnit(\'MB\') / task.cpus : null\\n    // FastQC memory value allowed range (100 - 10000)\\n    def fastqc_memory = memory_in_mb > 10000 ? 10000 : (memory_in_mb < 100 ? 100 : memory_in_mb)\\n\\n    \\"\\"\\"\\n    printf \\"%s %s\\\\\\\\n\\" ${rename_to} | while read old_name new_name; do\\n        [ -f \\"\\\\${new_name}\\" ] || ln -s \\\\$old_name \\\\$new_name\\n    done\\n\\n    fastqc \\\\\\\\\\n        ${args} \\\\\\\\\\n        --threads ${task.cpus} \\\\\\\\\\n        --memory ${fastqc_memory} \\\\\\\\\\n        ${renamed_files}\\n\\n    cat <<-END_VERSIONS > versions.yml\\n    \\"${task.process}\\":\\n        fastqc: \\\\$( fastqc --version | sed \'/FastQC v/!d; s/.*v//\' )\\n    END_VERSIONS\\n    \\"\\"\\"\\n\\n    stub:\\n    def prefix = task.ext.prefix ?: \\"${meta.id}\\"\\n    \\"\\"\\"\\n    touch ${prefix}.html\\n    touch ${prefix}.zip\\n\\n    cat <<-END_VERSIONS > versions.yml\\n    \\"${task.process}\\":\\n        fastqc: \\\\$( fastqc --version | sed \'/FastQC v/!d; s/.*v//\' )\\n    END_VERSIONS\\n    \\"\\"\\"\\n}\\n```\\n\\nOverall, this script creates a command to run with threads and memory options. Here, memory is allocated per thread instead of total memory.\\nIf I allocate 8 threads and 16 GB RAM, it will run the command as follows:\\n```bash\\n#!/bin/bash -ue\\nprintf \\"%s %s\\\\n\\" SRX1603630_T1_1.fastq.gz GM12878_REP2_1.gz SRX1603630_T1_2.fastq.gz GM12878_REP2_2.gz | while read old_name new_name; do\\n    [ -f \\"${new_name}\\" ] || ln -s $old_name $new_name\\ndone\\n\\nfastqc \\\\\\n     \\\\\\n    --threads 8 \\\\\\n    --memory 2048 \\\\\\n    GM12878_REP2_1.gz GM12878_REP2_2.gz\\n\\ncat <<-END_VERSIONS > versions.yml\\n\\"FASTQC_3\\":\\n    fastqc: $( fastqc --version | sed \'/FastQC v/!d; s/.*v//\' )\\nEND_VERSIONS\\n```\\n\\n### FASTP\\n\\nfastp is simpler regarding CPUs and memory. It allows direct specification without needing to recalculate memory per thread.\\n\\n```bash\\nprocess FASTP {\\n    tag \\"$meta.id\\"\\n    label \'process_medium\'\\n\\n    conda \\"${moduleDir}/environment.yml\\"\\n    container \\"${ workflow.containerEngine == \'singularity\' && !task.ext.singularity_pull_docker_container ?\\n        \'https://community-cr-prod.seqera.io/docker/registry/v2/blobs/sha256/88/889a182b8066804f4799f3808a5813ad601381a8a0e3baa4ab8d73e739b97001/data\' :\\n        \'community.wave.seqera.io/library/fastp:0.24.0--62c97b06e8447690\' }\\"\\n\\n    input:\\n    tuple val(meta), path(reads)\\n    path  adapter_fasta\\n    val   discard_trimmed_pass\\n    val   save_trimmed_fail\\n    val   save_merged\\n\\n    output:\\n    tuple val(meta), path(\'*.fastp.fastq.gz\') , optional:true, emit: reads\\n    tuple val(meta), path(\'*.json\')           , emit: json\\n    tuple val(meta), path(\'*.html\')           , emit: html\\n    tuple val(meta), path(\'*.log\')            , emit: log\\n    tuple val(meta), path(\'*.fail.fastq.gz\')  , optional:true, emit: reads_fail\\n    tuple val(meta), path(\'*.merged.fastq.gz\'), optional:true, emit: reads_merged\\n    path \\"versions.yml\\"                       , emit: versions\\n\\n    when:\\n    task.ext.when == null || task.ext.when\\n\\n    script:\\n    def args = task.ext.args ?: \'\'\\n    def prefix = task.ext.prefix ?: \\"${meta.id}\\"\\n    def adapter_list = adapter_fasta ? \\"--adapter_fasta ${adapter_fasta}\\" : \\"\\"\\n    def fail_fastq = save_trimmed_fail && meta.single_end ? \\"--failed_out ${prefix}.fail.fastq.gz\\" : save_trimmed_fail && !meta.single_end ? \\"--failed_out ${prefix}.paired.fail.fastq.gz --unpaired1 ${prefix}_1.fail.fastq.gz --unpaired2 ${prefix}_2.fail.fastq.gz\\" : \'\'\\n    def out_fq1 = discard_trimmed_pass ?: ( meta.single_end ? \\"--out1 ${prefix}.fastp.fastq.gz\\" : \\"--out1 ${prefix}_1.fastp.fastq.gz\\" )\\n    def out_fq2 = discard_trimmed_pass ?: \\"--out2 ${prefix}_2.fastp.fastq.gz\\"\\n    // Added soft-links to original fastqs for consistent naming in MultiQC\\n    // Use single ended for interleaved. Add --interleaved_in in config.\\n    if ( task.ext.args?.contains(\'--interleaved_in\') ) {\\n        \\"\\"\\"\\n        [ ! -f  ${prefix}.fastq.gz ] && ln -sf $reads ${prefix}.fastq.gz\\n\\n        fastp \\\\\\\\\\n            --stdout \\\\\\\\\\n            --in1 ${prefix}.fastq.gz \\\\\\\\\\n            --thread $task.cpus \\\\\\\\\\n            --json ${prefix}.fastp.json \\\\\\\\\\n            --html ${prefix}.fastp.html \\\\\\\\\\n            $adapter_list \\\\\\\\\\n            $fail_fastq \\\\\\\\\\n            $args \\\\\\\\\\n            2>| >(tee ${prefix}.fastp.log >&2) \\\\\\\\\\n        | gzip -c > ${prefix}.fastp.fastq.gz\\n\\n        cat <<-END_VERSIONS > versions.yml\\n        \\"${task.process}\\":\\n            fastp: \\\\$(fastp --version 2>&1 | sed -e \\"s/fastp //g\\")\\n        END_VERSIONS\\n        \\"\\"\\"\\n    } else if (meta.single_end) {\\n        \\"\\"\\"\\n        [ ! -f  ${prefix}.fastq.gz ] && ln -sf $reads ${prefix}.fastq.gz\\n\\n        fastp \\\\\\\\\\n            --in1 ${prefix}.fastq.gz \\\\\\\\\\n            $out_fq1 \\\\\\\\\\n            --thread $task.cpus \\\\\\\\\\n            --json ${prefix}.fastp.json \\\\\\\\\\n            --html ${prefix}.fastp.html \\\\\\\\\\n            $adapter_list \\\\\\\\\\n            $fail_fastq \\\\\\\\\\n            $args \\\\\\\\\\n            2>| >(tee ${prefix}.fastp.log >&2)\\n\\n        cat <<-END_VERSIONS > versions.yml\\n        \\"${task.process}\\":\\n            fastp: \\\\$(fastp --version 2>&1 | sed -e \\"s/fastp //g\\")\\n        END_VERSIONS\\n        \\"\\"\\"\\n    } else {\\n        def merge_fastq = save_merged ? \\"-m --merged_out ${prefix}.merged.fastq.gz\\" : \'\'\\n        \\"\\"\\"\\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -sf ${reads[0]} ${prefix}_1.fastq.gz\\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -sf ${reads[1]} ${prefix}_2.fastq.gz\\n        fastp \\\\\\\\\\n            --in1 ${prefix}_1.fastq.gz \\\\\\\\\\n            --in2 ${prefix}_2.fastq.gz \\\\\\\\\\n            $out_fq1 \\\\\\\\\\n            $out_fq2 \\\\\\\\\\n            --json ${prefix}.fastp.json \\\\\\\\\\n            --html ${prefix}.fastp.html \\\\\\\\\\n            $adapter_list \\\\\\\\\\n            $fail_fastq \\\\\\\\\\n            $merge_fastq \\\\\\\\\\n            --thread $task.cpus \\\\\\\\\\n            --detect_adapter_for_pe \\\\\\\\\\n            $args \\\\\\\\\\n            2>| >(tee ${prefix}.fastp.log >&2)\\n\\n        cat <<-END_VERSIONS > versions.yml\\n        \\"${task.process}\\":\\n            fastp: \\\\$(fastp --version 2>&1 | sed -e \\"s/fastp //g\\")\\n        END_VERSIONS\\n        \\"\\"\\"\\n    }\\n\\n    stub:\\n    def prefix              = task.ext.prefix ?: \\"${meta.id}\\"\\n    def is_single_output    = task.ext.args?.contains(\'--interleaved_in\') || meta.single_end\\n    def touch_reads         = (discard_trimmed_pass) ? \\"\\" : (is_single_output) ? \\"echo \'\' | gzip > ${prefix}.fastp.fastq.gz\\" : \\"echo \'\' | gzip > ${prefix}_1.fastp.fastq.gz ; echo \'\' | gzip > ${prefix}_2.fastp.fastq.gz\\"\\n    def touch_merged        = (!is_single_output && save_merged) ? \\"echo \'\' | gzip >  ${prefix}.merged.fastq.gz\\" : \\"\\"\\n    def touch_fail_fastq    = (!save_trimmed_fail) ? \\"\\" : meta.single_end ? \\"echo \'\' | gzip > ${prefix}.fail.fastq.gz\\" : \\"echo \'\' | gzip > ${prefix}.paired.fail.fastq.gz ; echo \'\' | gzip > ${prefix}_1.fail.fastq.gz ; echo \'\' | gzip > ${prefix}_2.fail.fastq.gz\\"\\n    \\"\\"\\"\\n    $touch_reads\\n    $touch_fail_fastq\\n    $touch_merged\\n    touch \\"${prefix}.fastp.json\\"\\n    touch \\"${prefix}.fastp.html\\"\\n    touch \\"${prefix}.fastp.log\\"\\n\\n    cat <<-END_VERSIONS > versions.yml\\n    \\"${task.process}\\":\\n        fastp: \\\\$(fastp --version 2>&1 | sed -e \\"s/fastp //g\\")\\n    END_VERSIONS\\n    \\"\\"\\"\\n}\\n```\\n\\nThis generates the command to run fastp with 2 reads, where memory is allocated automatically.\\n```bash\\n#!/bin/bash -ue\\n[ ! -f  GM12878_REP2_1.fastq.gz ] && ln -sf SRX1603630_T1_1.fastq.gz GM12878_REP2_1.fastq.gz\\n[ ! -f  GM12878_REP2_2.fastq.gz ] && ln -sf SRX1603630_T1_2.fastq.gz GM12878_REP2_2.fastq.gz\\nfastp \\\\\\n    --in1 GM12878_REP2_1.fastq.gz \\\\\\n    --in2 GM12878_REP2_2.fastq.gz \\\\\\n    --out1 GM12878_REP2_1.fastp.fastq.gz \\\\\\n    --out2 GM12878_REP2_2.fastp.fastq.gz \\\\\\n    --json GM12878_REP2.fastp.json \\\\\\n    --html GM12878_REP2.fastp.html \\\\\\n     \\\\\\n     \\\\\\n     \\\\\\n    --thread 2 \\\\\\n    --detect_adapter_for_pe \\\\\\n     \\\\\\n    2>| >(tee GM12878_REP2.fastp.log >&2)\\n\\ncat <<-END_VERSIONS > versions.yml\\n\\"FASTP_1\\":\\n    fastp: $(fastp --version 2>&1 | sed -e \\"s/fastp //g\\")\\nEND_VERSIONS\\n```\\n\\n## Configuration\\n\\nThis configuration allows the workflow to run with:\\n+ Docker/Singularity for containerization\\n+ Tracing and reporting capabilities\\n+ Named processes for memory and CPU allocation\\n\\n```bash\\nparams{\\n    outdir = \\"result\\"\\n    trace_report_suffix = new java.util.Date().format( \'yyyy-MM-dd_HH-mm-ss\')\\n    input = \\"inputs/full_samplsheet.csv\\"\\n}\\n\\nprofiles {\\n    singularity {\\n        singularity.enabled     = true\\n        singularity.autoMounts  = true\\n        conda.enabled           = false\\n        docker.enabled          = false\\n        podman.enabled          = false\\n        shifter.enabled         = false\\n        charliecloud.enabled    = false\\n        apptainer.enabled       = false\\n    }\\n    docker {\\n        docker.enabled          = true\\n        conda.enabled           = false\\n        singularity.enabled     = false\\n        podman.enabled          = false\\n        shifter.enabled         = false\\n        charliecloud.enabled    = false\\n        apptainer.enabled       = false\\n        docker.runOptions       = \'-u $(id -u):$(id -g)\'\\n    }\\n    emulate_amd64 {\\n        // Run AMD64 containers on ARM hardware using emulation (slower but more compatible)\\n        docker.runOptions       = \'-u $(id -u):$(id -g) --platform=linux/amd64\'\\n    }\\n\\n}\\n\\nsingularity.registry = \'quay.io\'\\ndocker.registry = \'quay.io\'\\n\\nprocess{\\n    withName:FASTQC_1{\\n        cpus = 2\\n        memory = 4.GB\\n    }\\n    withName:FASTQC_2{\\n        cpus = 4\\n        memory = 8.GB\\n    }\\n    withName:FASTQC_3{\\n        cpus = 8\\n        memory = 16 .GB\\n    }\\n    withName:FASTP_1{\\n        cpus = 2\\n        memory = 4.GB\\n    }\\n    withName:FASTP_2{\\n        cpus = 4\\n        memory = 8.GB\\n    }\\n    withName:FASTP_3{\\n        cpus = 8\\n        memory = 16 .GB\\n    }\\n}\\n\\ntimeline {\\n    enabled = true\\n    file    = \\"${params.outdir}/pipeline_info/execution_timeline_${params.trace_report_suffix}.html\\"\\n}\\nreport {\\n    enabled = true\\n    file    = \\"${params.outdir}/pipeline_info/execution_report_${params.trace_report_suffix}.html\\"\\n}\\ntrace {\\n    enabled = true\\n    file    = \\"${params.outdir}/pipeline_info/execution_trace_${params.trace_report_suffix}.txt\\"\\n}\\n```\\n\\n## Input Data\\n\\nThe sample sheet below includes two RNA-seq samples, each with input files ranging from 6 to 7 GB, representing real-world data sizes.\\n\\n```bash\\nsample,fastq_1,fastq_2,strandedness\\nGM12878_REP1,s3://ngi-igenomes/test-data/rnaseq/SRX1603629_T1_1.fastq.gz,s3://ngi-igenomes/test-data/rnaseq/SRX1603629_T1_2.fastq.gz,reverse\\nGM12878_REP2,s3://ngi-igenomes/test-data/rnaseq/SRX1603630_T1_1.fastq.gz,s3://ngi-igenomes/test-data/rnaseq/SRX1603630_T1_2.fastq.gz,reverse\\n```\\n\\n## Running the Benchmark\\n\\nThe above scripts are stored in the GitHub repository: https://github.com/nttg8100/nextflow-cost-optimization\\n\\nClone the repository:\\n```bash\\ngit clone https://github.com/nttg8100/nextflow-cost-optimization.git\\ncd nextflow-cost-optimization\\n```\\n:::tip\\n+ **Requirements**: At least 8 CPUs and 16 GB RAM (adjust based on your computing resources in `nextflow.config`)\\n+ **Alternative**: If you don\'t have Docker, run with Singularity\\n:::\\n\\n\\nRun the workflow, which will execute commands using Nextflow (installed via pixi). Docker is required on your machine.\\n```bash\\n# docker\\nmake test-fastqc-docker-amd64\\nmake test-fastp-docker-amd64\\n\\n# if you use MacBook with ARM\\nmake test-fastqc-docker-arm64\\nmake test-fastp-docker-arm64\\n\\n# if you do not have Docker installed, use Singularity\\nmake test-fastqc-singularity\\nmake test-fastp-singularity\\n```\\n![run workflow](./imgs_17_01/run_simple_workflow.png)\\n\\n## How to Evaluate Performance\\n\\nWhat we need to optimize is finding the minimal CPUs and memory that can run tasks efficiently without being too slow.\\nOpen the HTML reports generated when you run the workflow to analyze performance. Below are the metrics we should use for evaluation:\\n:::info\\n- **`%cpu`**:  \\n  The average percentage of CPU usage by the process during its execution. For multi-threaded jobs, this value can exceed 100% (e.g., 400% for a process using all 4 allocated CPUs fully).\\n\\n- **`rss`**:  \\n  \\"Resident Set Size\\" \u2014 the maximum amount of physical memory (RAM) used by the process, measured in bytes. This shows how much memory your process actually consumed at peak.\\n\\n- **`realtime`**:  \\n  The total elapsed (wall-clock) time taken by the process, from start to finish, including waiting and processing time.\\n\\nThese metrics help you tune and optimize your pipeline by revealing the true resource usage for each step.\\n:::\\n\\n### FastQC\\n\\nInterestingly, the execution time barely changes even when memory and CPUs are increased. From this experiment, we should consider using the minimal setup with 2 CPUs and 4 GB RAM while achieving nearly the same performance.\\n\\n:::warning\\n+ **Note**: The module is taken directly from nf-core. There may be a bug related to FastQC that hasn\'t been identified yet.\\n+ **Finding**: With real datasets, we can use minimal resources (much cheaper) without significant impact on running time!\\n:::\\n\\n![report_fastqc](./imgs_17_01/fastqc.png)\\n\\n### FASTP\\n\\nWith fastp, increasing memory and CPUs shows a slight reduction in running time.\\n![report_fastp](./imgs_17_01/fastp.png)\\n\\n## External benchmark\\nThe simple example above can be modified appropriately for more tools. Here are external benchmarks showing that adjusting computing resources helps reduce running time while keeping costs nearly the same:\\n\\n### Trimgalore\\nReference: https://github.com/FelixKrueger/TrimGalore/blob/master/CHANGELOG.md#version-060-release-on-1-mar-2019, \\nUsing 4 CPUs is a good choice as it can reduce time by nearly 4x when compared with 1 CPU only.\\n![report_trimgalore](./imgs_17_01/trimgalore.png)\\n\\n### STAR\\nReference: https://academic.oup.com/bioinformatics/article/29/1/15/272537\\nUsing 12 threads can align nearly 2x the reads when compared with 6 threads, while memory usage remains unchanged.\\n\\n## Nf-core application\\n:::tip\\n+ If you run workflow with HPC, consider the ratio between CPUs and memory. If you spend all of memory, there is no cpus left for another jobs\\n:::\\n\\nnf-core is one of the largest and most active bioinformatics workflow communities. However, their pipelines often use generic resource labels to define CPU and memory requirements for each process. For instance, the [**nf-core/rnaseq**](https://github.com/nf-core/rnaseq/blob/master/conf/base.config) pipeline assigns default resources using labels like medium for tools such as FASTQC and FASTP.\\n\\nWithout tuning these settings for each specific tool, your workflow can waste resources and increase costs. In practice, both FASTQC and FASTP typically only need 2 CPUs and 4 GB RAM\u2014much less than the default medium allocation.\\n\\nThis is especially important when running batch jobs in the cloud (AWS, Google Cloud, Azure, etc.), where virtual machines are often configured with a CPU:Memory ratio of 1:2. Requesting resources with a higher ratio, like 1:6, can increase queue times and lead to inefficient use of resources, as you may wait longer for an available instance and not fully utilize the extra memory.\\n\\n```bash\\nwithLabel:process_single {\\n    cpus   = { 1                   }\\n    memory = { 6.GB * task.attempt }\\n    time   = { 4.h  * task.attempt }\\n}\\nwithLabel:process_low {\\n    cpus   = { 2     * task.attempt }\\n    memory = { 12.GB * task.attempt }\\n    time   = { 4.h   * task.attempt }\\n}\\nwithLabel:process_medium {\\n    cpus   = { 6     * task.attempt }\\n    memory = { 36.GB * task.attempt }\\n    time   = { 8.h   * task.attempt }\\n}\\nwithLabel:process_high {\\n    cpus   = { 12    * task.attempt }\\n    memory = { 72.GB * task.attempt }\\n    time   = { 16.h  * task.attempt }\\n}\\nwithLabel:process_long {\\n    time   = { 20.h  * task.attempt }\\n}\\nwithLabel:process_high_memory {\\n    memory = { 200.GB * task.attempt }\\n}\\n```\\n\\nTo overwrite the existing worflow process cpus and memory, use the withName for specific tasks or adjust the label resources\\n```bash\\nprocess{\\n    cpus   = { 1      * task.attempt }\\n    memory = { 2.GB   * task.attempt }\\n    time   = { 4.h    * task.attempt }\\n\\n    errorStrategy = { task.exitStatus in ((130..145) + 104 + 175) ? \'retry\' : \'finish\' }\\n    maxRetries    = 1\\n    maxErrors     = \'-1\'\\n\\n    withLabel:process_single {\\n        cpus   = { 1                   }\\n        memory = { 2.GB * task.attempt }\\n        time   = { 4.h  * task.attempt }\\n    }\\n    withLabel:process_low {\\n        cpus   = { 2     * task.attempt }\\n        memory = { 4.GB * task.attempt }\\n        time   = { 4.h   * task.attempt }\\n    }\\n    withLabel:process_medium {\\n        cpus   = { 8     * task.attempt }\\n        memory = { 16.GB * task.attempt }\\n        time   = { 8.h   * task.attempt }\\n    }\\n    withLabel:process_high {\\n        cpus   = { 16    * task.attempt }\\n        memory = { 32.GB * task.attempt }\\n        time   = { 16.h  * task.attempt }\\n    }\\n    withLabel:process_long {\\n        time   = { 20.h  * task.attempt }\\n    }\\n    withLabel:process_high_memory {\\n        memory = { 32.GB * task.attempt }\\n    }\\n    withLabel:error_ignore {\\n        errorStrategy = \'ignore\'\\n    }\\n    withLabel:error_retry {\\n        errorStrategy = \'retry\'\\n        maxRetries    = 2\\n    }\\n    withLabel: process_gpu {\\n        ext.use_gpu = { workflow.profile.contains(\'gpu\') }\\n        accelerator = { workflow.profile.contains(\'gpu\') ? 1 : null }\\n    }\\n    withName: FASTQC{\\n        cpus   = { 2     * task.attempt }\\n        memory = { 4.GB  * task.attempt }\\n        time   = { 4.h   * task.attempt }\\n    }\\n    withName: \'.*:ALIGN_STAR:STAR_ALIGN|.*:ALIGN_STAR:STAR_ALIGN_IGENOMES\'{\\n        container = \\"docker.io/nttg8100/star:2.7.11b\\"\\n    }\\n}\\n```\\n\\n## Lessons Learned\\nThe two simple modules above help us select CPU and memory more easily. To optimize based on cost, here are some tips:\\n+ For tasks shorter than 20 minutes, increasing CPUs and memory doesn\'t significantly reduce running time\u2014use minimal resources instead\\n+ Some tools benefit greatly from multiple CPUs and memory. For these, you should increase computing resources.\\n+ Using 80% of available resources is typically sufficient. Sometimes when the input is slightly larger, tools can still run without crashing."},{"id":"pixi-is-new-conda-based-era","metadata":{"permalink":"/river-docs/blog/pixi-is-new-conda-based-era","source":"@site/blog/2026-01/2026-01-17.md","title":"Pixi- New conda era","description":"Pixi is a fast, modern package management tool built on the conda ecosystem. Whether you\'re on a personal laptop or an HPC cluster, Pixi makes environment setup simple and reproducible. Here\u2019s a primer on using Pixi. For a full workflow, see: Setting up single-cell RNA-seq analysis with Pixi as collaboration between RIVERXDATA and NGS101","date":"2026-01-17T00:00:00.000Z","tags":[{"inline":true,"label":"s3","permalink":"/river-docs/blog/tags/s-3"},{"inline":true,"label":"data-analysis","permalink":"/river-docs/blog/tags/data-analysis"},{"inline":true,"label":"slurm","permalink":"/river-docs/blog/tags/slurm"},{"inline":true,"label":"hpc","permalink":"/river-docs/blog/tags/hpc"},{"inline":true,"label":"web-platform","permalink":"/river-docs/blog/tags/web-platform"}],"readingTime":2.06,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"pixi-is-new-conda-based-era","title":"Pixi- New conda era","authors":["river"],"tags":["s3","data-analysis","slurm","hpc","web-platform"],"image":"./imgs/pixi_thumbnail.svg"},"unlisted":false,"prevItem":{"title":"Bioinformatics Cost Optimization for Computing Resources Using Nextflow (Part 1)","permalink":"/river-docs/blog/bioinformatics-computing-resource-optimization-part1"},"nextItem":{"title":"The Evolution of Version Control - CI/CD in bioinformatics (Part 2)","permalink":"/river-docs/blog/how-to-version-control-git-bioinformatics-part-2"}},"content":"Pixi is a fast, modern package management tool built on the conda ecosystem. Whether you\'re on a personal laptop or an HPC cluster, Pixi makes environment setup simple and reproducible. Here\u2019s a primer on using Pixi. For a full workflow, see: [**Setting up single-cell RNA-seq analysis with Pixi**](https://ngs101.com/setting-up-single-cell-rna-seq-analysis-environment-with-pixi-10x-faster-setup-zero-version-conflicts/) as collaboration between **RIVERXDATA** and **NGS101**\\n\\n\x3c!-- truncate --\x3e\\n\\n## Installing Pixi\\n\\n### System Requirements\\n\\n- **OS**: Any modern Unix (Ubuntu, Debian, CentOS, Fedora, Arch, MacOS, etc.)\\n- **Disk Space**: 50\u2013100 MB for Pixi, plus space for packages/cache\\n- **Privileges**: No sudo/root needed\u2014Pixi installs in user space\\n\\nIdeal for shared environments without admin access.\\n\\n### One-Line Installation\\n\\nRun in your terminal:\\n\\n```bash\\ncurl -fsSL https://pixi.sh/install.sh | bash\\n```\\n\\nThis downloads Pixi, installs it to `~/.pixi/bin`, and updates your shell config. After install, restart your terminal or run:\\n\\n```bash\\nsource ~/.bashrc  # or ~/.zshrc\\n```\\n\\n### Verify Installation\\n\\nCheck Pixi is working:\\n\\n```bash\\npixi --version\\npixi --help\\n```\\n\\n## HPC Configuration\\n\\n### Custom Cache Location\\n\\nTo avoid home directory quotas, set a custom cache:\\n\\n```bash\\nexport PIXI_CACHE_DIR=/scratch/$USER/pixi-cache\\n```\\nAdd to your `~/.bashrc` or `~/.bash_profile` for persistence.\\n\\n### Proxy Settings\\n\\nIf your cluster uses a proxy, edit `$HOME/.pixi/config.toml`:\\n\\n```toml\\n[proxy-config]\\nhttp = \\"http://proxy.example.com:8080/\\"\\nhttps = \\"http://proxy.example.com:8080/\\"\\nnon-proxy-hosts = [\\".cn\\", \\"localhost\\", \\"[::1]\\"]\\n```\\nSee [Pixi configuration docs](https://pixi.prefix.dev/latest/reference/pixi_configuration/) for details.\\n\\n### Parallel Downloads\\n\\nSpeed up downloads on fast networks:\\n\\n```toml\\n[concurrency]\\ndownloads = 5\\n```\\n\\n## Project Activation\\n\\nPixi helps you create reproducible environments with lock files.\\n\\n### Initialize a Project\\n\\n```bash\\npixi init\\npixi workspace channel add conda-forge bioconda\\npixi workspace platform add osx-arm64 linux-64\\n```\\n\\nExample `pixi.toml`:\\n\\n```toml\\n[workspace]\\nchannels = [\\"bioconda\\", \\"conda-forge\\"]\\nname = \\"<your project>\\"\\nplatforms = [\\"linux-64\\", \\"osx-arm64\\"]\\nversion = \\"0.1.0\\"\\n\\n[tasks]\\n\\n[dependencies]\\n```\\n\\n### Add Tools\\n\\n```bash\\npixi add fastqc multiqc\\n```\\n\\nUpdates dependencies in `pixi.toml`:\\n\\n```toml\\n[dependencies]\\nfastqc = \\">=0.12.1,<0.13\\"\\nmultiqc = \\">=1.17,<2\\"\\n```\\n\\nA `pixi.lock` file is created for reproducibility, specifying exact package versions and checksums.\\n\\n### Run Tools\\n\\nRun tools directly:\\n\\n```bash\\npixi run fastqc --help\\n```\\n\\nOr activate the environment shell:\\n\\n```bash\\npixi shell\\nfastqc --help\\n```\\n\\n## Recap\\n\\n- Pixi is a user-space, conda-based package manager ideal for laptops and HPC clusters.\\n- Easy one-line installation, no admin rights needed.\\n- Supports custom cache and proxy settings for HPC environments.\\n- Enables reproducible project environments with lock files.\\n- Simple commands to add tools and run them in isolated environments.\\n\\nEnjoy your Pixi-powered workflow!"},{"id":"how-to-version-control-git-bioinformatics-part-2","metadata":{"permalink":"/river-docs/blog/how-to-version-control-git-bioinformatics-part-2","source":"@site/blog/2026-01/2026-01-16.md","title":"The Evolution of Version Control - CI/CD in bioinformatics (Part 2)","description":"Welcome to Part 2 of our series on version control in bioinformatics. In Part 1, we introduced Git fundamentals, branching strategies, and collaborative workflows. In this post, we\'ll dive into how Continuous Integration and Continuous Deployment (CI/CD) can transform your bioinformatics projects. If these concepts are new to you, don\'t worry\u2014this guide will walk you through managing your bioinformatics repository to ensure your work is easily reproducible on any machine. Whether your server is wiped or you need to spin up a new virtual machine, you\'ll be able to quickly rerun your pipeline. With CI/CD, every code update can automatically trigger tests on a small dataset to verify everything works before scaling up, ensuring that new changes don\'t break your results or workflows.","date":"2026-01-16T00:00:00.000Z","tags":[{"inline":true,"label":"git","permalink":"/river-docs/blog/tags/git"},{"inline":true,"label":"version-control","permalink":"/river-docs/blog/tags/version-control"},{"inline":true,"label":"bioinformatics","permalink":"/river-docs/blog/tags/bioinformatics"},{"inline":true,"label":"reproducibility","permalink":"/river-docs/blog/tags/reproducibility"},{"inline":true,"label":"pipelines","permalink":"/river-docs/blog/tags/pipelines"},{"inline":true,"label":"ci-cd","permalink":"/river-docs/blog/tags/ci-cd"},{"inline":true,"label":"github-actions","permalink":"/river-docs/blog/tags/github-actions"}],"readingTime":13.21,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"how-to-version-control-git-bioinformatics-part-2","title":"The Evolution of Version Control - CI/CD in bioinformatics (Part 2)","authors":["river"],"tags":["git","version-control","bioinformatics","reproducibility","pipelines","ci-cd","github-actions"],"image":"./imgs/github_intro.svg"},"unlisted":false,"prevItem":{"title":"Pixi- New conda era","permalink":"/river-docs/blog/pixi-is-new-conda-based-era"},"nextItem":{"title":"The Evolution of Version Control - Git\'s Role in Reproducible Bioinformatics (Part 1)","permalink":"/river-docs/blog/how-to-version-control-git-bioinformatics-part-1"}},"content":"Welcome to Part 2 of our series on version control in bioinformatics. In [Part 1](./2026-01-15.md), we introduced Git fundamentals, branching strategies, and collaborative workflows. In this post, we\'ll dive into how Continuous Integration and Continuous Deployment (CI/CD) can transform your bioinformatics projects. If these concepts are new to you, don\'t worry\u2014this guide will walk you through managing your bioinformatics repository to ensure your work is easily reproducible on any machine. Whether your server is wiped or you need to spin up a new virtual machine, you\'ll be able to quickly rerun your pipeline. With CI/CD, every code update can automatically trigger tests on a small dataset to verify everything works before scaling up, ensuring that new changes don\'t break your results or workflows.\\n\\n\x3c!--truncate--\x3e\\n- **Part 1**: History, basics, GitHub integration, and hands-on tutorials\\n- **Part 2 (This Post)**: Local testing, CI/CD implementation, automated testing, and reproducible pipelines\\n\\n:::info\\nIn this tutorial, we will:\\n+ Add code to download data\\n+ Write scripts to use FastQC and MultiQC\\n:::\\n\\n## Git commands\\nThis tutorial is fully committed via a GitHub repository. You can clone and checkout to specific branches to see how it is developed and committed.\\nFirst, clone the repository, then:\\n+ Learn how to view commit history (branches are referenced by their names)\\n+ Learn how to checkout to a specific commit\\n+ Learn how to see the difference between the current commit and a specific commit\\n\\n:::tip\\n+ To quit the git prompt, type `q`\\n+ Create your own repository for a more practical approach\\n+ Check the differences in files, then copy them to your own repository to follow the tutorial\\n:::\\n\\n```bash\\n# clone\\ngit clone git@github.com:nttg8100/ngs-quality-control-for-blog-example.git\\ncd ngs-quality-control-for-blog-example\\n\\n# checkout to branch for the relevant tutorial step, e.g. feature/add-data\\ngit checkout feature/add-data\\n```\\n\\n### How to show commit log\\nGit provides commands to view the history and check the difference between two commits.\\n```bash\\ngit log\\n```\\nIt will show details as below:\\n```bash\\ncommit ca29351295a4148bda73b6bf76a42cc7b834d1b6 (HEAD -> feature/add-data, origin/feature/add-data)\\nAuthor: Thanh-Giang (River) Tan Nguyen <nttg8100@gmail.com>\\nDate:   Mon Jan 19 23:34:58 2026 +0700\\n\\n    feat: add makefile to get data\\n\\ncommit 70d584a726fb5418274448a4f51ca07d94b104be (origin/main, origin/HEAD, main, feature/testing-rebase)\\nMerge: c731b9a 230795b\\nAuthor: Thanh Giang Nguyen Tan <64969412+nttg8100@users.noreply.github.com>\\nDate:   Sat Jan 17 16:42:11 2026 +0700\\n\\n    Merge pull request #1 from nttg8100/feature/environment-setup\\n    \\n    feat: add pixi setup\\n\\ncommit 230795b7b8b03087ef7ee938d333161c9fcc0731 (origin/feature/environment-setup)\\nAuthor: Thanh-Giang (River) Tan Nguyen <nttg8100@gmail.com>\\nDate:   Sat Jan 17 15:05:28 2026 +0700\\n\\n    feat: add pixi setup\\n\\ncommit c731b9a80dfb22936bfcf94442bdf9b488a90d3c (feature/tesing-rebase)\\nAuthor: Thanh Giang Nguyen Tan <64969412+nttg8100@users.noreply.github.com>\\nDate:   Wed Jan 14 23:09:30 2026 +0700\\n```\\n\\nTo see more precisely what you need:\\n```bash\\ngit log --oneline\\n```\\n\\nThe one-line format is good enough:\\n```bash\\nca29351 (HEAD -> feature/add-data, origin/feature/add-data) feat: add makefile to get data\\n70d584a (origin/main, origin/HEAD, main, feature/testing-rebase) Merge pull request #1 from nttg8100/feature/environment-setup\\n230795b (origin/feature/environment-setup) feat: add pixi setup\\nc731b9a (feature/tesing-rebase) Initial commit\\n```\\n\\nNow you can see what I did on the repository. Following this tutorial, you can change to a specific commit to track it.\\n\\n### How to checkout to specific commit\\nYou can simply use the commit hash. For example, if I want to see all files from commit `230795b` which is on the branch `feature/testing-rebase`:\\n```bash\\ngit checkout 230795b\\n```\\nNow your code editor will show the files with content related to this commit.\\n\\n\\n### How to view the difference between two commits or your current state\\nIf you have made changes to your files but have not committed them yet:\\n```bash\\ngit diff\\n```\\n\\nI made a change and have not committed it here:\\n```bash\\ndiff --git a/Makefile b/Makefile\\nindex ae08113..c2e5b87 100644\\n--- a/Makefile\\n+++ b/Makefile\\n@@ -10,4 +10,6 @@ data/GSE110004:\\n        wget https://raw.githubusercontent.com/nf-core/test-datasets/rnaseq/testdata/GSE110004/SRR6357070_2.fastq.gz\\n \\n clean:\\n-       rm -rf data/GSE110004\\n\\\\ No newline at end of file\\n+       rm -rf data/GSE110004\\n+\\n+Change here\\n\\\\ No newline at end of file\\n```\\n\\nIf you want to compare between two branches/commits:\\n```bash\\ngit diff ca29351..70d584a\\n```\\n\\nThe changes between the above commits:\\n```bash\\ndiff --git a/.gitignore b/.gitignore\\nindex cfca1e8..1c3ecb3 100644\\n--- a/.gitignore\\n+++ b/.gitignore\\n@@ -19,5 +19,3 @@ bin-release/\\n # pixi environments\\n .pixi/*\\n !.pixi/config.toml\\n-data\\n-\\ndiff --git a/Makefile b/Makefile\\nindex ae08113..bcaa6d7 100644\\n--- a/Makefile\\n+++ b/Makefile\\n@@ -1,13 +1,3 @@\\n ${HOME}/.pixi/bin/pixi:\\n        mkdir -p ${HOME}/.pixi/bin\\n-       curl -sSL https://pixi.dev/install.sh | sh\\n-\\n-\\n-data/GSE110004:\\n-       mkdir -p data/GSE110004\\n-       cd data/GSE110004 && \\\\\\n-       wget https://raw.githubusercontent.com/nf-core/test-datasets/rnaseq/testdata/GSE110004/SRR6357070_1.fastq.gz && \\\\\\n-       wget https://raw.githubusercontent.com/nf-core/test-datasets/rnaseq/testdata/GSE110004/SRR6357070_2.fastq.gz\\n-\\n-clean:\\n-       rm -rf data/GSE110004\\n\\\\ No newline at end of file\\n+       curl -sSL https://pixi.dev/install.sh | sh\\n\\\\ No newline at end of file\\n```\\n### How to roll back\\nIf you checkout a specific commit and add a few changes to test according to the tutorial, `git` requires you to commit your changes.\\nYou can simply roll back to the original version:\\n```bash\\ngit checkout -- .\\n```\\n\\nSometimes you want to get files from another branch. You can use:\\n```bash\\ngit checkout <branch name> <relative file path>\\n# ex: git checkout main .gitignore\\n```\\n\\n## Add data\\nWe need to create a new branch from updated main\\n```bash\\ngit checkout main\\ngit pull origin main\\ngit switch -c feature/add-data\\n```\\n\\nNow we\'ll add a few target setups to the Makefile to automate data download and cleanup tasks for your pipeline:\\n\\n- **`data/GSE110004` target**: Downloads example FASTQ files from a public test dataset into the `data/GSE110004` directory. This ensures everyone working on the project uses the same input data, improving reproducibility.\\n- **`clean` target**: Removes the downloaded data directory, making it easy to reset your environment or save disk space.\\n\\nThis approach helps standardize data management and makes it easier for collaborators (and CI/CD systems) to set up and test the pipeline with minimal manual steps.\\n\\nUpdate your `Makefile` with the following:\\n\\n```bash\\n${HOME}/.pixi/bin/pixi:\\n        mkdir -p ${HOME}/.pixi/bin\\n        curl -sSL https://pixi.dev/install.sh | sh\\n\\ndata:\\n  mkdir -p data/GSE110004\\n  wget -O data/GSE110004/SRR6351937_1.fastq.gz https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR635/007/SRR6351937/SRR6351937_1.fastq.gz\\n  wget -O data/GSE110004/SRR6351937_2.fastq.gz https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR635/007/SRR6351937/SRR6351937_2.fastq.gz\\n\\nclean:\\n  rm -rf data\\n```\\n\\nFollowing tutorial 1, you can commit, push, create a PR, and merge to main:\\n```bash\\ngit commit -m \\"feat: add makefile to get data\\"\\ngit push origin feature/add-data\\n```\\n\\n## Add workflow\\nCreate new branch\\n```bash\\ngit checkout main\\ngit pull\\ngit switch -c feature/workflow\\n```\\n### Add workflow main.sh script and testing\\nHere I create a simple bash script that will run using command lines with inputs as arguments.\\n```bash\\n# create workflow file using bash script, this runs for only paired reads of 1 sample\\ncat << EOF >> main.sh\\n#!/bin/bash\\n\\n# Usage: ./main.sh <read1.fastq.gz> <read2.fastq.gz> <output_folder>\\n\\nset -e\\n\\nif [ \\"\\\\$#\\" -ne 3 ]; then\\n    echo \\"Usage: $0 <read1.fastq.gz> <read2.fastq.gz> <output_folder>\\"\\n    exit 1\\nfi\\n\\nREAD1=\\"\\\\$1\\"\\nREAD2=\\"\\\\$2\\"\\nOUTDIR=\\"\\\\$3\\"\\n\\nmkdir -p \\"\\\\$OUTDIR\\"\\n\\necho \\"Running FastQC on \\\\$READ1 and \\\\$READ2...\\"\\nfastqc \\"\\\\$READ1\\" \\"\\\\$READ2\\" -o \\"\\\\$OUTDIR\\"\\n\\necho \\"Running MultiQC in \\\\$OUTDIR...\\"\\nmultiqc \\"\\\\$OUTDIR\\" -o \\"\\\\$OUTDIR\\"\\n\\necho \\"All done. Results are in \\\\$OUTDIR\\"\\nEOF\\n\\n# chmod to be executed\\nchmod +x main.sh\\n\\n# get help script\\nbash main.sh\\n# Usage: zsh <read1.fastq.gz> <read2.fastq.gz> <output_folder>\\n\\n# test it with your data\\n# activate pixi \\npixi shell\\n# run interactively\\nbash main.sh data/GSE110004/*_1.fastq.gz data/GSE110004/*_2.fastq.gz result\\n\\n# run with pixi directly\\npixi run bash main.sh data/GSE110004/*_1.fastq.gz data/GSE110004/*_2.fastq.gz result\\n```\\n![qc_wf](./imgs_16_01/qc_workflow.png)\\n\\n\\n### Use Makefile to add tests\\nNow we can quickly run tests after updating the `Makefile`:\\n```Makefile\\n.PHONY: install-pixi test-e2e clean\\n${HOME}/.pixi/bin/pixi:\\n\\tmkdir -p ${HOME}/.pixi/bin\\n\\tcurl -sSL https://pixi.dev/install.sh | sh\\n\\ndata/GSE110004:\\n\\tmkdir -p data/GSE110004\\n\\tcd data/GSE110004 && \\\\\\n\\twget https://raw.githubusercontent.com/nf-core/test-datasets/rnaseq/testdata/GSE110004/SRR6357070_1.fastq.gz && \\\\\\n\\twget https://raw.githubusercontent.com/nf-core/test-datasets/rnaseq/testdata/GSE110004/SRR6357070_2.fastq.gz\\n\\ntest-e2e: data/GSE110004\\n\\t${HOME}/.pixi/bin/pixi run bash main.sh data/GSE110004/*_1.fastq.gz data/GSE110004/*_2.fastq.gz result\\n\\nclean:\\n\\trm -rf data/GSE110004\\n```\\n\\n\\nQuickly run the test:\\n```bash\\nmake test-e2e\\n```\\n\\nUpdate your `.gitignore` to avoid committing the results:\\n```bash\\n# Build and Release Folders\\nbin-debug/\\nbin-release/\\n[Oo]bj/\\n[Bb]in/\\n\\n# Other files and folders\\n.settings/\\n\\n# Executables\\n*.swf\\n*.air\\n*.ipa\\n*.apk\\n\\n# Project files, i.e. `.project`, `.actionScriptProperties` and `.flexProperties`\\n# should NOT be excluded as they contain compiler settings and other important\\n# information for Eclipse / Flash Builder.\\n# pixi environments\\n.pixi/*\\n!.pixi/config.toml\\ndata\\nresult\\n```\\n\\nFollowing tutorial 1, you can commit, push, create a PR, and merge to main:\\n```bash\\ngit add .\\ngit commit -m \\"feat: add workflow\\"\\ngit push origin feature/workflow \\n```\\n\\nThen, in your repository root folder, run these commands to get the data:\\n:::tip\\n+ Currently, a few make target commands are simple and can be run directly, but in the future, it would be better to update them with more commands\\n+ Ex: `make clean` can be used to delete more files/folders\\n:::\\n```bash\\ngit checkout \\n# download data\\nmake data\\n# test that data is downloaded\\nls -lah  data/GSE110004\\n# total 4.3M\\n# drwxrwxr-x 2 giangnguyen giangnguyen 4.0K Jan 19 01:00 .\\n# drwxrwxr-x 3 giangnguyen giangnguyen 4.0K Jan 19 01:00 ..\\n# -rw-rw-r-- 1 giangnguyen giangnguyen 2.2M Jan 19 01:00 SRR6357070_1.fastq.gz\\n# -rw-rw-r-- 1 giangnguyen giangnguyen 2.2M Jan 19 01:00 SRR6357070_2.fastq.gz\\n\\n# delete data folder\\nmake clean\\n```\\n## Configure testing\\nSo far, everything has been done locally. However, when you add new features\u2014for example, adding `trimmomatic` to trim the FastQC files\u2014you need to run the tests again to ensure they can be executed without errors. This is the simplest testing practice for developing bioinformatics projects.\\n\\nGitHub provides GitHub Actions to help with this:\\n:::info\\n+ For more detailed examples on GitHub Actions, follow the official documentation: https://docs.github.com/en/actions/get-started/quickstart\\n+ Here, we introduce how it works and how we can apply it simply\\n:::\\n\\n\\n### Create GitHub Action files\\nCreate a new branch for CI/CD:\\n```bash\\ngit checkout main\\ngit pull\\ngit switch -c feature/cicd\\n```\\nConfigure the workflow:\\n```bash\\nmkdir -p .github/workflows\\n# create workflow file\\ncat << EOF >> .github/workflows/test-e2e.yaml\\nname: Testing e2e\\n\\non:\\n  push:\\n    branches: [ main ]\\n  pull_request:\\n    branches: [ main ]\\n\\njobs:\\n  clone:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - name: Checkout repository\\n        uses: actions/checkout@v4\\n\\n      - name: Running testing\\n        run: |\\n          make test-e2e\\nEOF\\n```\\n\\nWhat it does: \\n+ Runs workflow named Testing e2e\\n+ Triggers on pull requests to main\\n+ Triggers on push or merge into main\\n+ Starts a job on ubuntu-latest runner\\n+ Clones the repository using actions/checkout\\n+ Runs end-to-end tests via make test-e2e\\n+ Passes if tests succeed\\n+ Fails if tests return a non-zero exit code\\n\\n### Running GitHub Actions locally\\nIf we only run tests when creating a PR or pushing to `main`, you may make mistakes and create many commits just to trigger testing. Here, we can use `act`, a local command line tool that can help:\\n:::info\\n+ For more details on how to use `act`: https://nektosact.com\\n+ Install Docker on your local machine first\\n:::\\n\\n```bash\\n# install\\npixi add act\\n# run e2e test locally, it will run this on your local machine\\nact -W .github/workflows/test-e2e.yaml\\n```\\n![act_failed](./imgs_16_01/act_failed.png)\\n\\n:::warning\\n+ As you can see, it failed because pixi is not installed. This is because the `make test-e2e` target does not trigger pixi installation\\n+ Also, there is a wrong URL for installing pixi: `https://pixi.dev/install.sh` should be replaced with `https://pixi.sh/install.sh`\\n+ You should add pixi installation as a dependency for `test-e2e`\\n:::\\n\\nUpdate dependencies of `test-e2e` in the `Makefile`:\\n```bash\\n.PHONY: test-e2e clean\\n${HOME}/.pixi/bin/pixi:\\n\\tmkdir -p ${HOME}/.pixi/bin\\n\\tcurl -sSL https://pixi.sh/install.sh | sh\\n\\ndata/GSE110004:\\n\\tmkdir -p data/GSE110004\\n\\tcd data/GSE110004 && \\\\\\n\\twget https://raw.githubusercontent.com/nf-core/test-datasets/rnaseq/testdata/GSE110004/SRR6357070_1.fastq.gz && \\\\\\n\\twget https://raw.githubusercontent.com/nf-core/test-datasets/rnaseq/testdata/GSE110004/SRR6357070_2.fastq.gz\\n\\ntest-e2e: data/GSE110004 ${HOME}/.pixi/bin/pixi\\n\\t${HOME}/.pixi/bin/pixi run bash main.sh data/GSE110004/*_1.fastq.gz data/GSE110004/*_2.fastq.gz result\\n\\nclean:\\n\\trm -rf data/GSE110004\\n```\\n\\nNow you can rerun to test successfully, as shown below:\\n![act_success1](./imgs_16_01/act_success1.png)\\n\\n![act_success2](./imgs_16_01/act_success2.png)\\n\\n## What is CI/CD in bioinformatics?\\n### What is CI/CD in software development\\n![cicd](./imgs_16_01/cicd.webp)\\nReference: https://viblo.asia/p/cau-hinh-cicd-voi-github-phan-1-mot-it-ly-thuyet-Qbq5Q9NL5D8\\n\\nWhat exactly are CI (Continuous Integration) and CD (Continuous Delivery)? These terms are commonly used in software development.\\nIn the figure above, software can be developed with many new features, which will trigger specific jobs with actions.\\nFor example, we can the workflow to clone the repository on a GitHub Actions instance and run tests. This approach depends on many use cases.\\n\\n### How does it work in bioinformatics?\\nThat is exactly what I showed you above. If you want to develop software, follow the standard CI/CD software development process. In contrast,\\nbioinformatics typically combines these tools together. This means they will run scripts (bash, Nextflow, WDL, Snakemake, etc.) to combine tools\\nin a workflow manner. The release is the repository itself with the scripts inside (CD), while the CI is the process we set up to run\\nthe pipeline with testing datasets.\\n\\nNow you can create a PR, which will always trigger the testing that helps you validate whether your changes may fail the pipeline.\\n![PR_GHA](./imgs_16_01/PR_GHA.png)\\n\\n:::tip\\n+ Use secrets in `Settings` to work with credentials (passwords, usernames, keys)\\n+ Use the Makefile to help sync the local environment with the GitHub Actions environment. This makes it easy to change the CI/CD runner\\n+ Use matrix strategy to run tests faster\\n:::\\n\\nCheck GitHub Actions logs for debugging:\\n![GHA](./imgs_16_01/GHA_log.png)\\n\\n## Review\\n\\nIn this tutorial, we explored how to transform a basic bioinformatics project into a reproducible, testable pipeline using Git, Makefiles, and CI/CD practices. Here are the key takeaways:\\n\\n### Git Commands for Project Management\\n- Use `git log` and `git log --oneline` to view commit history\\n- Checkout to specific commits to track development progress\\n- Use `git diff` to compare changes between commits or branches\\n- Roll back to original states with `git checkout -- .`\\n- Copy files from other branches using `git checkout <branch> <file>`\\n\\n### Makefile Automation\\n- Create targets for data download (`make data`) and cleanup (`make clean`)\\n- Add test targets (`make test-e2e`) that chain dependencies together\\n- Ensure reproducibility by standardizing commands across environments\\n\\n### Workflow Scripts\\n- Create bash scripts that accept command-line arguments\\n- Combine tools like FastQC and MultiQC in a workflow\\n- Test workflows locally before pushing to remote\\n\\n### CI/CD with GitHub Actions\\n- Create workflow files in `.github/workflows/`\\n- Trigger tests on push to main or pull requests\\n- Use the same Makefile commands in CI as in local development\\n- Check GitHub Actions logs for debugging failed tests\\n\\n### Local Testing with `act`\\n- Test GitHub Actions workflows locally using `act`\\n- Catch issues before pushing to remote repositories\\n- Reduce the number of commits needed to fix CI failures\\n\\n### CI/CD in Bioinformatics Context\\n- Unlike traditional software, bioinformatics CI/CD often focuses on pipeline validation\\n- The \\"delivery\\" is often the repository itself with reproducible workflows\\n- Testing involves running entire pipelines on test datasets\\n- This ensures changes don\'t break existing analyses\\n\\nBy implementing these practices, you create a robust bioinformatics project that is:\\n- **Reproducible**: Anyone can reproduce your results on any machine\\n- **Testable**: Automated tests catch errors before they reach production\\n- **Maintainable**: Clear history and documentation make updates easier\\n- **Collaborative**: CI/CD ensures all changes are validated before merging\\n\\nReady to implement CI/CD in your own bioinformatics projects? Start by adding a simple Makefile target, then gradually build up your testing and automation capabilities."},{"id":"how-to-version-control-git-bioinformatics-part-1","metadata":{"permalink":"/river-docs/blog/how-to-version-control-git-bioinformatics-part-1","source":"@site/blog/2026-01/2026-01-15.md","title":"The Evolution of Version Control - Git\'s Role in Reproducible Bioinformatics (Part 1)","description":"In Part 1 (this post), we explore the history of Git, its integration with GitHub, and basic hands-on tutorials. Part 2 (coming soon) will cover real-world bioinformatics examples and advanced workflows with best practices.","date":"2026-01-15T00:00:00.000Z","tags":[{"inline":true,"label":"git","permalink":"/river-docs/blog/tags/git"},{"inline":true,"label":"version-control","permalink":"/river-docs/blog/tags/version-control"},{"inline":true,"label":"bioinformatics","permalink":"/river-docs/blog/tags/bioinformatics"},{"inline":true,"label":"reproducibility","permalink":"/river-docs/blog/tags/reproducibility"},{"inline":true,"label":"pipelines","permalink":"/river-docs/blog/tags/pipelines"}],"readingTime":12.08,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"how-to-version-control-git-bioinformatics-part-1","title":"The Evolution of Version Control - Git\'s Role in Reproducible Bioinformatics (Part 1)","authors":["river"],"tags":["git","version-control","bioinformatics","reproducibility","pipelines"],"image":"./imgs/github_intro.svg"},"unlisted":false,"prevItem":{"title":"The Evolution of Version Control - CI/CD in bioinformatics (Part 2)","permalink":"/river-docs/blog/how-to-version-control-git-bioinformatics-part-2"},"nextItem":{"title":"Building a Slurm HPC Cluster (Part 3) - Administration and Best Practices","permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-3"}},"content":"In Part 1 (this post), we explore the history of Git, its integration with GitHub, and basic hands-on tutorials. Part 2 (coming soon) will cover real-world bioinformatics examples and advanced workflows with best practices.\\n\\nThis part focuses on practical applications, including NGS quality control using multiqc and fastqc.\\n\\n\x3c!--truncate--\x3e\\n\\n- **Part 1 (This Post)**: History, basics, GitHub integration, and hands-on tutorials\\n- **Part 2 (Coming Soon)**: Real-world bioinformatics examples and advanced workflows with best practices\\n\\nHere I provide a real-world example of how to create a simple repository that enables reproducible work for NGS short read data quality control. I\'ll explain each step using a GitHub repository.\\n\\n## Create a Repository\\n\\nAs the creator, you are the admin of the repository. You can configure permissions to control who can read or modify it. Here are some common setup tips:\\n\\n:::tip\\n+ **Visibility**: If you want to share your work with the community, use `public`. Otherwise, use `private`\\n+ **Add README**: A README file helps others understand how your repository is structured. Beyond the standard `README.md` for general users, consider adding `README.developer.md` for developer-specific information\\n+ **.gitignore**: This file specifies which files or folders should not be pushed to GitHub\\n+ **License**: This determines how others can use your work. For example, MIT allows free usage, while other licenses may require purchase for commercial use\\n:::\\n\\n![create_repo](./imgs_16_01/create_repository.png)\\n\\n## Clone\\n\\nCloning creates a copy of your repository on your local computer. When working in a team, each member has their own local copy. GitHub serves as a central location, similar to Google Drive, that allows you to sync your source code.\\n\\nGitHub provides SSH protocol authentication for downloading resources, but it doesn\'t work like a remote server with shell access.\\n\\n:::info\\n+ After creating a repository, clone it to start working\\n+ You\'ll need to authenticate with GitHub on your local computer or HPC first. I recommend using SSH keys\\n+ Click the green \\"Code\\" button, select the \\"SSH\\" tab, copy the URL, and use it to clone the repository\\n:::\\n\\n![created](./imgs_16_01/created_clone.png)\\n\\nFor authentication, follow the official GitHub documentation: https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account\\n\\n:::tip\\n+ This site also has documentation on connecting to remote servers using SSH: [**SSH setup**](/docs/resources/administration/ssh-remote-server)\\n+ You can stop after creating the key pair\\n:::\\n\\nAdd your public key to GitHub by navigating to `Settings` \u2192 `SSH` section\\n\\n![ssh](./imgs_16_01/add_ssh_key.png)\\n\\nNow test your connection to GitHub:\\n\\n```bash\\nssh git@github.com\\n# PTY allocation request failed on channel 0\\n# Hi nttg8100! You\'ve successfully authenticated, but GitHub does not provide shell access.\\n# Connection to github.com closed.\\n```\\n\\nYou\'re ready to clone. Open your terminal, navigate to your working directory, and clone the repository:\\n\\n:::tip\\n+ To clone a specific branch or tag, add `-b <branch-name>`\\n:::\\n\\n```bash\\n# clone (equivalent to: git clone git@github.com:nttg8100/ngs-quality-control-for-blog-example.git -b main)\\ngit clone git@github.com:nttg8100/ngs-quality-control-for-blog-example.git\\ncd ngs-quality-control-for-blog-example\\n```\\n\\nOpen the code with your editor. Here I use Visual Studio Code:\\n\\n:::info\\nDownload and install Visual Studio Code: https://code.visualstudio.com/download\\n:::\\n\\n![download_vscode](./imgs_16_01/visual_studio_code.png)\\n\\n## Your Code Editor\\n\\nIn your terminal, you can open the entire repository (which is just a folder). Simply use your mouse to select the terminal icon in the footer.\\n\\n:::tip\\n+ Now you can select any file you want to edit\u2014everything is a text file\\n:::\\n\\n```bash\\ncode .\\n```\\n\\n![editor](./imgs_16_01/editor.png)\\n\\nNow follow these steps to establish good version control practices.\\n\\n## Branching Strategy\\n\\n![branch](./imgs_16_01/git_branch.png)\\n\\nA branching strategy is a workflow that helps teams manage codebase changes in a structured way. Git branches allow you to work on new features, bug fixes, or experiments without affecting the main codebase.\\n\\n**Common branching strategies:**\\n\\n- **Main (or `master`) branch:**\\n  The stable branch that always contains production-ready code\\n\\n- **Feature branches:**\\n  Create a new branch for each feature or task (e.g., `feature/quality-control`). This keeps development isolated until the feature is complete\\n\\n- **Development branch:**\\n  Some teams use a `develop` branch as an integration branch for features before merging into `main`\\n\\n- **Hotfix/bugfix branches:**\\n  For urgent fixes, create a branch from `main` (e.g., `hotfix/fix-typo`) and merge back after testing\\n\\n**Typical workflow:**\\n1. Create a new branch from `main` for your feature or fix\\n2. Work on your changes and commit them to your branch\\n3. Open a pull request (PR) to merge your branch into `main` (or `develop`)\\n4. Review, test, and merge the PR\\n\\nThis approach:\\n- Keeps the `main` branch stable\\n- Makes collaboration easier\\n- Helps track changes and review code before merging\\n\\nNow let\'s create a new branch. Our goal is to set up easy installation of fastqc and multiqc. We\'ll write a bash script for analysis later. For simplicity, we\'ll develop feature branches and merge them directly to main, which works well for small projects like this.\\n\\n```bash\\n# Create and switch to a new branch from main\\ngit switch -c feature/environment-setup\\n```\\n\\n## New Branch: Environment Setup\\n\\nInstead of conda, I prefer to install pixi, then install multiqc and fastqc. Create a Makefile for installation\u2014you can either edit a new `Makefile` manually or generate it with bash.\\n\\n:::info\\nA Makefile is a special file used by the make build automation tool. It defines rules and instructions for building and managing projects, especially in languages like C/C++. However, it can be used for any project, including Node.js, React, and TypeScript, to automate repetitive tasks.\\n\\n**Key Concepts:**\\n- **Targets**: What you want to build (e.g., a file or a label for a task)\\n- **Dependencies**: Files or targets that must be up-to-date before the target can be built\\n- **Recipes**: Shell commands to run to build the target\\n:::\\n\\n```bash\\ncat << EOF >> Makefile\\n${HOME}/.pixi/bin/pixi:\\n  mkdir -p ${HOME}/.pixi/bin\\n  curl -sSL https://pixi.dev/install.sh | sh\\nEOF\\n\\n# Verify the newly created file content\\ncat Makefile\\n```\\n\\n:::tip\\n**Pro Tip:**\\nWhen writing your `Makefile`, use `.PHONY` targets for commands that don\'t produce files with the same name. This prevents conflicts and ensures your commands always run as expected. Keep installation steps modular\u2014separate environment setup, tool installation, and analysis scripts into different targets for clarity and reusability.\\n:::\\n\\n```bash\\n# Install pixi\\nmake ~/.pixi/bin/pixi\\n\\n# Initialize pixi in your current repository\\npixi init --channel conda-forge --channel bioconda\\n\\n# Install multiqc and fastqc\\npixi add fastqc multiqc\\n\\n# Activate the pixi shell\\npixi shell\\n\\n# Verify tools are installed\\nwhich fastqc\\nwhich multiqc\\n# /home/giangnguyen/Documents/dev/docs/ngs-quality-control-for-blog-example/.pixi/envs/default/bin/fastqc\\n# /home/giangnguyen/Documents/dev/docs/ngs-quality-control-for-blog-example/.pixi/envs/default/bin/multiqc\\n```\\n\\n## Common Git Commands\\n\\n:::warning\\nYou can check git history to see who made changes\u2014useful for accountability when needed\\n:::\\n\\nCheck your current status to see uncommitted files. Use command line to add files to your commit:\\n\\n```bash\\ngit status\\n```\\n\\n![uncommited](./imgs_16_01/status_uncommit.png)\\n\\nAdd files to commit and review changes:\\n\\n```bash\\n# Add files one by one\\ngit add .gitignore\\ngit add .gitattributes\\ngit add Makefile\\ngit add pixi.lock\\ngit add pixi.toml\\n\\n# Or add all files/folders in the current repository\\ngit add .\\n\\n# Verify staged files\\ngit status\\n```\\n\\n![committed](./imgs_16_01/status_after_add.png)\\n\\nSometimes you might accidentally stage the wrong files. Remove them from the current commit:\\n\\n```bash\\n# Create a wrong file\\ntouch wrong_file.txt\\ngit add wrong_file.txt\\n\\n# Restore it to unstaged status\\ngit restore --staged wrong_file.txt\\n```\\n\\n![restore](./imgs_16_01/restore.png)\\n\\nIf you need files locally but don\'t want to commit them, add them to .gitignore. This prevents them from being committed even when using `git add .`. .gitignore can ignore both files and folders.\\n\\n:::tip\\n+ For better control, create .gitignore in the specific folder/subfolder where unwanted files/folders are located\\n+ Common .gitignore patterns: environment folders (`.env`, `.pixi`), `node_modules`, data folders (`data`), `*.tar.gz`, cache folders (`__pycache__`)\\n+ To ignore an entire folder but keep specific files, use `!<filename>` to keep only needed files\\n```\\nfolder\\n!folder/file.txt\\n```\\n:::\\n\\n```bash\\n# Add to gitignore\\necho \\"wrong_file.txt\\" > .gitignore\\n\\n# View the file\\ncat .gitignore\\n\\n# Try to add the file again\u2014it won\'t be staged now and will show a warning\\ngit add wrong_file.txt\\n\\n# Check status\\ngit status\\n```\\n\\n![gitignore](./imgs_16_01/gitignore.png)\\n\\nNow create a commit with all your staged files:\\n\\n```bash\\ngit commit -m \\"feat: add pixi setup\\"\\n```\\n\\n![commit](./imgs_16_01/commit.png)\\n\\nPush your changes to GitHub, the remote server that tracks your development progress:\\n\\n```bash\\n# Push to remote\\ngit push origin feature/environment-setup\\n\\n# View your remote origin to confirm where updates are pushed\\n# This shows I cloned using the SSH credential\\ngit config --get remote.origin.url\\n# git@github.com:nttg8100/ngs-quality-control-for-blog-example.git\\n```\\n\\n![push](./imgs_16_01/push_origin.png)\\n\\n## Check Updates on GitHub\\n\\nVisit your GitHub repository to see the new branch. In this example:\\nhttps://github.com/nttg8100/ngs-quality-control-for-blog-example\\n\\n![updated_branch](./imgs_16_01/new_branch_created.png)\\n\\nNow let\'s move on to remote collaboration with your team.\\n\\n## Pull Request\\n\\nA Pull Request (PR) compares your changes with the target branch. GitHub provides a concise PR interface that shows what\'s been added or removed. You can review your own PR, use Copilot, or have teammates review. They can comment on your PR to suggest improvements or identify issues.\\n\\n**Best practices for creating a Pull Request:**\\n+ **Keep PRs Small**: Focus on a single feature or fix. Small PRs are easier to review and merge\\n+ **Clear Description**: Write a concise summary of what the PR does, why it\'s needed, and any relevant context\\n+ **Reference Issues**: Link related issues or tickets (e.g., Closes #123)\\n+ **Self-Review**: Check your code for errors, style, and unnecessary changes before submitting\\n+ **Add Tests**: Include or update tests to cover your changes\\n+ **Follow Conventions**: Stick to your team\'s coding and commit message guidelines\\n\\nTo create a PR, select the \\"Pull requests\\" tab, click \\"New pull request\\", then select your current branch and target branch. We\'ll continue with our current branch:\\n\\n![new_pr](./imgs_16_01/new_pr.png)\\n\\nThe PR is now created:\\n\\n![created_pr](./imgs_16_01/created_pr.png)\\n\\n## Review\\n\\n**Best practices for reviewing a Pull Request:**\\n+ **Understand the Context**: Read the PR description and related issues\\n+ **Check for Clarity**: Ensure code is readable, well-structured, and commented where necessary\\n+ **Test Locally**: Run the code if possible to verify it works as intended\\n+ **Suggest Improvements**: Be constructive and specific in your feedback\\n+ **Check for Side Effects**: Make sure changes don\'t break existing functionality\\n+ **Be Respectful**: Keep feedback positive and focused on the code, not the person\\n\\nClick on \\"File changes\\" to start your review. You can comment on specific lines of changed files. In the end, comment on whether changes should be modified or merged. Here I\'m the PR creator, so GitHub disables self-approval. In best practice, another person should review your work. `LGTM` (Looks Good To Me) indicates approval and allows merging your changes into the target branch.\\n\\n:::tip\\n+ Request PR review from teammates\\n+ PR must be approved before merging\\n:::\\n\\n![review](./imgs_16_01/review.png)\\n\\n## Merge\\n\\nAlthough your PR is ready to merge without approval in this example, you should configure branch protection rules in your repository settings:\\nhttps://docs.github.com/en/repositories/configuring-branches-and-merges-in-your-repository/configuring-pull-request-merges\\n\\nWhen merging a pull request on GitHub, you can choose from several strategies. Each has advantages depending on your workflow:\\n\\n| Strategy             | Description                                                                                   | Use Case                                   |\\n| -------------------- | --------------------------------------------------------------------------------------------- | ------------------------------------------ |\\n| **Merge Commit**     | Combines all commits from the feature branch into the target branch, preserving full history. | Default; keeps detailed commit history.    |\\n| **Squash and Merge** | Combines all changes from the feature branch into a single commit on the target branch.       | Clean, concise history for small features. |\\n| **Rebase and Merge** | Reapplies your branch\'s commits on top of the target branch, creating a linear history.       | Linear history, avoids merge commits.      |\\n\\n![merge](./imgs_16_01/merge.png)\\n\\nWhen working with a large team, using a merge commit directly integrates your branch into the target branch. This works well if the target branch hasn\'t changed significantly since you started. For example, if you branched off `main` when it only had `feature A` and `feature B`, and you\'re adding `feature C`, merging is straightforward.\\n\\nHowever, if other team members have merged new features (e.g., `feature C1`) into `main` that modify the same files you\'re working on, you may encounter conflicts. In such cases, it\'s best to use rebase to update your branch with the latest changes from the target branch before merging. If your branch contains many small, incremental commits, consider using squash and merge to combine them into a single, clean commit for a tidier history.\\n\\n![merge_type](./imgs_16_01/merge-types.png)\\n\\nNow the main branch shows the update from your PR:\\n\\n![updated_main](./imgs_16_01/merged_to_main.png)\\n\\n## Rebase\\n\\nWhen working in a team, the `main` branch (or your target branch) may receive updates from other contributors while you\'re developing your feature branch. To keep your branch up-to-date and avoid conflicts later, regularly update your branch with the latest changes from `main`. This is where `rebase` is useful.\\n\\n**Use case:**\\nSuppose you started working on a feature branch (`feature/testing-rebase`) based on an older version of `main`. Meanwhile, new commits have been added to `main` by others. Rebasing allows you to \\"replay\\" your changes on top of the latest `main`, resulting in a linear and clean history.\\n\\n**How to update your feature branch after `main` is updated:**\\n\\n```bash\\n# Make sure you\'re on your feature branch\\ngit checkout feature/testing-rebase\\n\\n# Fetch the latest changes from the remote repository\\ngit fetch origin\\n\\n# Rebase your branch onto the updated main branch\\ngit rebase origin/main\\n```\\n\\nDuring the rebase, if there are conflicts, Git will pause and let you resolve them. After fixing conflicts, continue the rebase:\\n\\n```bash\\ngit add <resolved-files>\\ngit rebase --continue\\n```\\n\\nIf you want to abort the rebase at any point:\\n\\n```bash\\ngit rebase --abort\\n```\\n\\n**Why use rebase?**\\n- Keeps your branch history clean and linear\\n- Makes it easier to review and merge changes\\n- Reduces the chance of complex merge conflicts later\\n\\n**Tip:**\\nAfter rebasing and before pushing, you may need to force-push your branch since the commit history has changed:\\n\\n```bash\\ngit push --force-with-lease\\n```\\n\\nUse force-push with care, especially on shared branches.\\n\\n![rebase](./imgs_16_01/rebase_on_local.png)\\n\\n## New branch again\\nNow you can sync with your team, next step is creating the new branch again to have more updates (features, fix bug, documentation, testing, etc)\\n```bash\\n# check out on base branch where feature is updated\\ngit checkout main\\n\\n# pull to update, add rebase for better organization on commits\\ngit pull --rebase origin main\\n\\n# create new branch\\ngit switch -c feature/add-cicd-setup\\n```\\n\\n*This concludes Part 1 of the RiverXData series on version control in bioinformatics. Stay tuned for Part 2!*"},{"id":"how-to-build-slurm-hpc-part-3","metadata":{"permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-3","source":"@site/blog/2026-01/2026-01-14.md","title":"Building a Slurm HPC Cluster (Part 3) - Administration and Best Practices","description":"In Part 1 and Part 2, we built a complete Slurm HPC cluster from a single node to a production-ready multi-node system. Now let\'s learn how to manage, maintain, and secure it effectively.","date":"2026-01-14T00:00:00.000Z","tags":[{"inline":true,"label":"slurm","permalink":"/river-docs/blog/tags/slurm"},{"inline":true,"label":"hpc","permalink":"/river-docs/blog/tags/hpc"},{"inline":true,"label":"administration","permalink":"/river-docs/blog/tags/administration"},{"inline":true,"label":"security","permalink":"/river-docs/blog/tags/security"},{"inline":true,"label":"best-practices","permalink":"/river-docs/blog/tags/best-practices"}],"readingTime":12.14,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"how-to-build-slurm-hpc-part-3","title":"Building a Slurm HPC Cluster (Part 3) - Administration and Best Practices","authors":["river"],"tags":["slurm","hpc","administration","security","best-practices"],"image":"./imgs/hpc_3.svg"},"unlisted":false,"prevItem":{"title":"The Evolution of Version Control - Git\'s Role in Reproducible Bioinformatics (Part 1)","permalink":"/river-docs/blog/how-to-version-control-git-bioinformatics-part-1"},"nextItem":{"title":"Building a Slurm HPC Cluster (Part 2) - Scaling to Production with Ansible","permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-2"}},"content":"In [Part 1](/blog/how-to-build-slurm-hpc-part-1) and [Part 2](/blog/how-to-build-slurm-hpc-part-2), we built a complete Slurm HPC cluster from a single node to a production-ready multi-node system. Now let\'s learn how to manage, maintain, and secure it effectively.\\n\\nThis final post covers daily administration tasks, troubleshooting, security hardening, and integration with data processing frameworks.\\n\\n\x3c!--truncate--\x3e\\n\\n## Series Overview\\n\\n- **[Part 1](/blog/how-to-build-slurm-hpc-part-1)**: Introduction, Architecture, and Single Node Setup\\n- **[Part 2](/blog/how-to-build-slurm-hpc-part-2)**: Scaling to Production with Ansible\\n- **Part 3 (This Post)**: Administration and Best Practices\\n\\n## Administration Overview\\n\\nManaging a Slurm cluster involves several key areas:\\n\\n- **Cluster Management**: Build, maintain, and update the cluster via Ansible\\n- **User Management**: Synchronize users across nodes with proper permissions\\n- **Login Security**: Implement SSH hardening with 2FA or key pairs\\n- **Resource Management**: Enforce limits and fair-share policies\\n- **Monitoring**: Track performance and resource utilization\\n- **Troubleshooting**: Diagnose and resolve issues\\n\\n## User and Resource Management\\n\\n### Adding Users and Groups\\n\\nSlurm uses accounts (groups) to organize users and apply resource policies:\\n\\n```bash\\n# Add a new account/group\\nsacctmgr add account research_team Description=\\"Research Team\\"\\n\\n# Add a user to an account\\nsacctmgr add user john account=research_team\\n\\n# Add user with multiple accounts\\nsacctmgr add user alice account=research_team,dev_team DefaultAccount=research_team\\n\\n# View accounts\\nsacctmgr show account\\n\\n# View users\\nsacctmgr show user\\n```\\n\\n### Setting Resource Limits\\n\\n#### Account-Level Limits\\n\\nControl resources for entire groups:\\n\\n```bash\\n# Limit CPU minutes (prevents monopolizing cluster)\\nsacctmgr modify account research_team set GrpCPUMins=100000\\n\\n# Limit memory (in MB)\\nsacctmgr modify account research_team set GrpMem=500000\\n\\n# Limit concurrent jobs\\nsacctmgr modify account research_team set GrpJobs=50\\n\\n# Limit concurrent running jobs\\nsacctmgr modify account research_team set GrpJobsRun=20\\n\\n# Limit number of nodes\\nsacctmgr modify account research_team set GrpNodes=10\\n```\\n\\n#### User-Level Limits\\n\\nControl individual user behavior:\\n\\n```bash\\n# Limit jobs in queue\\nsacctmgr modify user john set MaxJobs=10\\n\\n# Limit running jobs\\nsacctmgr modify user john set MaxJobsRun=5\\n\\n# Limit wall time (in minutes)\\nsacctmgr modify user john set MaxWall=1440  # 24 hours\\n\\n# Limit CPUs per job\\nsacctmgr modify user john set MaxCPUs=32\\n\\n# View user limits\\nsacctmgr show user john withassoc format=user,account,maxjobs,maxsubmit,maxwall\\n```\\n\\n### Quality of Service (QoS)\\n\\nQoS allows you to create service tiers with different priorities:\\n\\n```bash\\n# Create QoS levels\\nsacctmgr add qos normal priority=100\\nsacctmgr add qos high priority=500 MaxWall=2-00:00:00 MaxJobs=5\\nsacctmgr add qos low priority=50\\n\\n# Assign QoS to account\\nsacctmgr modify account research_team set qos=normal,high\\n\\n# Users can specify QoS when submitting\\nsbatch --qos=high job_script.sh\\n```\\n\\n### Fair-Share Scheduling\\n\\nEnsure equitable resource distribution:\\n\\n```bash\\n# Set fair-share values (higher = more priority)\\nsacctmgr modify account research_team set fairshare=100\\nsacctmgr modify account dev_team set fairshare=50\\n\\n# View fair-share tree\\nsshare -a\\n\\n# View detailed fair-share info\\nsshare -A research_team --all\\n```\\n\\n## Node Management\\n\\n### Checking Node Status\\n\\n```bash\\n# View all nodes\\nsinfo\\n\\n# Detailed node information\\nsinfo -Nel\\n\\n# Show node states\\nsinfo -N -o \\"%N %T %C %m %e %f\\"\\n\\n# View specific node details\\nscontrol show node worker-01\\n```\\n\\n<figure markdown=\\"span\\">\\n    ![Job sinfo](./imgs/job_sinfo.png)\\n</figure>\\n\\nNode states you\'ll encounter:\\n- **IDLE**: Available for jobs\\n- **ALLOCATED**: Running jobs\\n- **MIXED**: Partially allocated\\n- **DRAIN**: Won\'t accept new jobs (draining)\\n- **DRAINED**: Fully drained\\n- **DOWN**: Not responding\\n\\n### Node Maintenance\\n\\n#### Draining a Node\\n\\nWhen you need to perform maintenance:\\n\\n```bash\\n# Drain node (won\'t accept new jobs, allows running jobs to finish)\\nscontrol update NodeName=worker-01 State=drain Reason=\\"Hardware upgrade\\"\\n\\n# Force drain (terminate running jobs)\\nscontrol update NodeName=worker-01 State=drain Reason=\\"Emergency maintenance\\"\\n\\n# Check drain reason\\nsinfo -R\\n```\\n\\n#### Resuming a Node\\n\\nAfter maintenance:\\n\\n```bash\\n# Resume node\\nscontrol update NodeName=worker-01 State=resume\\n\\n# Verify it\'s back\\nsinfo -n worker-01\\n```\\n\\n#### Forcing Node Down\\n\\nIf a node is misbehaving:\\n\\n```bash\\n# Mark node as down\\nscontrol update NodeName=worker-01 State=down Reason=\\"Hardware failure\\"\\n\\n# When fixed, resume\\nscontrol update NodeName=worker-01 State=resume\\n```\\n\\n### Adding New Compute Nodes\\n\\n1. Update Ansible inventory (`inventories/hosts`):\\n\\n```ini\\n[slurm_worker]\\nworker-01 ansible_host=192.168.58.11\\nworker-02 ansible_host=192.168.58.12\\nworker-03 ansible_host=192.168.58.13  # NEW\\n```\\n\\n2. Run Ansible playbook:\\n\\n```bash\\nansible-playbook -i inventories/hosts river_cluster.yml --limit worker-03\\n```\\n\\n3. Update `slurm.conf` on controller and all nodes (Ansible handles this)\\n\\n4. Restart slurmctld:\\n\\n```bash\\nsudo systemctl restart slurmctld\\n```\\n\\n5. Verify the new node:\\n\\n```bash\\nsinfo\\nscontrol show node worker-03\\n```\\n\\n## Monitoring and Troubleshooting\\n\\n### Checking Slurm Logs\\n\\nLogs are essential for diagnosing issues:\\n\\n```bash\\n# Controller logs\\nsudo tail -f /var/log/slurm/slurmctld.log\\n\\n# Worker node logs (on compute nodes)\\nsudo tail -f /var/log/slurm/slurmd.log\\n\\n# Database logs\\nsudo tail -f /var/log/slurm/slurmdbd.log\\n\\n# Filter for errors\\nsudo grep \\"error\\" /var/log/slurm/*.log\\n\\n# Filter for specific node\\nsudo grep \\"worker-01\\" /var/log/slurm/slurmctld.log\\n\\n# Last 100 lines with context\\nsudo tail -100 /var/log/slurm/slurmctld.log\\n```\\n\\n### Common Issues and Solutions\\n\\n#### Issue: Node Shows as DOWN\\n\\n**Diagnosis:**\\n```bash\\nsinfo\\n# OUTPUT: worker-01    down   ...\\n\\nscontrol show node worker-01\\n# Check \\"Reason\\" field\\n```\\n\\n**Solutions:**\\n```bash\\n# 1. Check if slurmd is running\\nssh worker-01 \\"sudo systemctl status slurmd\\"\\n\\n# 2. Restart slurmd\\nssh worker-01 \\"sudo systemctl restart slurmd\\"\\n\\n# 3. Check network connectivity\\nping worker-01\\n\\n# 4. Check logs\\nssh worker-01 \\"sudo tail -50 /var/log/slurm/slurmd.log\\"\\n\\n# 5. Resume the node\\nscontrol update NodeName=worker-01 State=resume\\n```\\n\\n#### Issue: Jobs Stuck in Pending\\n\\n**Diagnosis:**\\n```bash\\nsqueue\\n# See jobs in PD (pending) state\\n\\n# Check why job is pending\\nsqueue --start -j JOB_ID\\n\\n# View detailed job info\\nscontrol show job JOB_ID\\n```\\n\\n**Common reasons:**\\n- `Resources`: Not enough resources available\\n- `Priority`: Lower priority than other jobs\\n- `Dependency`: Waiting for another job to complete\\n- `QOSMaxJobsPerUser`: User has too many jobs running\\n\\n**Solutions:**\\n```bash\\n# 1. Check available resources\\nsinfo -o \\"%P %a %l %D %N %C\\"\\n\\n# 2. View job requirements\\nscontrol show job JOB_ID | grep -E \\"Partition|NumNodes|MinMemory\\"\\n\\n# 3. Cancel job if needed\\nscancel JOB_ID\\n\\n# 4. Modify pending job\\nscontrol update JobId=JOB_ID NumNodes=1\\n```\\n\\n#### Issue: Jobs Failing Immediately\\n\\n**Diagnosis:**\\n```bash\\n# Check job status\\nsacct -j JOB_ID\\n\\n# View job output files\\ncat slurm-JOB_ID.out\\ncat slurm-JOB_ID.err\\n```\\n\\n**Common causes:**\\n- Script errors (check shebang line)\\n- Missing executables\\n- Resource limits exceeded\\n- Permission issues\\n\\n#### Issue: Accounting Database Not Working\\n\\n**Diagnosis:**\\n```bash\\n# Check slurmdbd status\\nsudo systemctl status slurmdbd\\n\\n# Test database connection\\nsudo mysql -u slurm -p slurm_acct_db -e \\"SHOW TABLES;\\"\\n\\n# Check slurmdbd logs\\nsudo tail -50 /var/log/slurm/slurmdbd.log\\n```\\n\\n**Solutions:**\\n```bash\\n# 1. Restart slurmdbd\\nsudo systemctl restart slurmdbd\\n\\n# 2. Verify database credentials in slurmdbd.conf\\nsudo cat /etc/slurm-llnl/slurmdbd.conf\\n\\n# 3. Check database permissions\\nsudo mysql -e \\"SHOW GRANTS FOR \'slurm\'@\'localhost\';\\"\\n\\n# 4. Restart slurmctld to reconnect\\nsudo systemctl restart slurmctld\\n```\\n\\n### System Logs with rsyslog\\n\\nOur Ansible setup configures centralized logging:\\n\\n```bash\\n# On controller (rsyslog server)\\nsudo tail -f /var/log/syslog\\n\\n# Filter by hostname\\nsudo grep \\"worker-01\\" /var/log/syslog\\n\\n# Filter by service\\nsudo grep \\"slurmd\\" /var/log/syslog\\n\\n# Check authentication logs\\nsudo tail -f /var/log/auth.log\\n```\\n\\n## Security Best Practices\\n\\n### SSH Hardening\\n\\n:::warning\\nSecure your login nodes! HPC clusters are attractive targets for attackers.\\n:::\\n\\nFor detailed SSH security setup, see our [SSH Remote Server documentation](/docs/resources/administration/ssh-remote-server).\\n\\nKey recommendations:\\n\\n1. **Disable Password Authentication**:\\n```bash\\n# /etc/ssh/sshd_config\\nPasswordAuthentication no\\nPubkeyAuthentication yes\\n```\\n\\n2. **Implement 2FA** with Google Authenticator or similar\\n\\n3. **Use SSH Key Pairs**:\\n```bash\\n# Generate key on your machine\\nssh-keygen -t ed25519 -C \\"your_email@example.com\\"\\n\\n# Copy to cluster\\nssh-copy-id user@controller-node\\n```\\n\\n4. **Limit SSH Access**:\\n```bash\\n# /etc/ssh/sshd_config\\nAllowUsers alice bob charlie\\nAllowGroups cluster_users\\n\\n# Or deny specific users\\nDenyUsers baduser\\n```\\n\\n5. **Change Default Port** (security through obscurity):\\n```bash\\n# /etc/ssh/sshd_config\\nPort 2222\\n```\\n\\n### Munge Authentication\\n\\nMunge provides authentication between Slurm components:\\n\\n```bash\\n# Verify munge is running\\nsudo systemctl status munge\\n\\n# Test munge\\nmunge -n | unmunge\\n\\n# Generate new key (do this on controller, then distribute)\\nsudo /usr/sbin/create-munge-key\\n\\n# Copy key to all nodes (Ansible does this automatically)\\nsudo scp /etc/munge/munge.key worker-01:/etc/munge/\\n\\n# Restart munge on all nodes\\nsudo systemctl restart munge\\n```\\n\\n:::warning\\n**Critical**: The munge key must be identical on all nodes and have proper permissions (0400, owned by munge:munge).\\n:::\\n\\n### Docker Security\\n\\n:::danger\\n**Critical Security Issue**: Users in the `docker` group can gain root privileges!\\n\\n```bash\\n# DON\'T DO THIS (unless they\'re admins)\\nsudo usermod -aG docker regular_user\\n```\\n\\nWhy? Because they can run:\\n```bash\\ndocker run -v /:/hostfs --privileged -it ubuntu bash\\n# Now they have root access to the host filesystem!\\n```\\n:::\\n\\n**Solutions**:\\n\\n1. **Use Docker Rootless Mode**:\\n```bash\\n# Install rootless docker\\ncurl -fsSL https://get.docker.com/rootless | sh\\n```\\n\\n2. **Use Apptainer/Singularity** (designed for HPC):\\n```bash\\n# Install Apptainer\\nsudo apt-get install apptainer\\n\\n# Run containers without root\\napptainer run docker://ubuntu:latest\\n```\\n\\n3. **Restrict Docker Group**: Only add administrators to docker group\\n\\n### Firewall Configuration\\n\\nRestrict access to Slurm ports:\\n\\n```bash\\n# Allow Slurm ports only from cluster network\\nsudo ufw allow from 192.168.58.0/24 to any port 6817  # slurmctld\\nsudo ufw allow from 192.168.58.0/24 to any port 6818  # slurmd\\nsudo ufw allow from 192.168.58.0/24 to any port 6819  # slurmdbd\\n\\n# Allow SSH from anywhere\\nsudo ufw allow 22/tcp\\n\\n# Enable firewall\\nsudo ufw enable\\n```\\n\\n## Shared Storage Best Practices\\n\\n### NFS Performance Tuning\\n\\nOptimize NFS for your workload:\\n\\n```bash\\n# /etc/fstab on compute nodes\\ncontroller-01:/home /home nfs4 rw,soft,rsize=262144,wsize=262144,timeo=14,intr 0 0\\n```\\n\\nParameters explained:\\n- `soft`: Timeout after retry (vs `hard` which waits forever)\\n- `rsize/wsize`: Read/write buffer size (larger = better performance)\\n- `timeo`: Timeout value\\n- `intr`: Allow interrupts\\n\\n### Storage Layout\\n\\nRecommended directory structure:\\n\\n```\\n/home/          # User home directories (SSD/NVMe)\\n  \u251c\u2500 alice/\\n  \u251c\u2500 bob/\\n  \u2514\u2500 charlie/\\n\\n/mnt/data/      # Large datasets (HDD or object storage)\\n  \u251c\u2500 shared/    # Common datasets\\n  \u251c\u2500 projects/  # Project-specific data\\n  \u2514\u2500 scratch/   # Temporary data (auto-cleanup)\\n\\n/opt/           # Shared software/modules\\n  \u251c\u2500 anaconda/\\n  \u251c\u2500 modules/\\n  \u2514\u2500 apps/\\n```\\n\\n### Quotas\\n\\nPrevent users from filling up shared storage:\\n\\n```bash\\n# Set user quotas\\nsudo setquota -u alice 50G 60G 0 0 /home\\nsudo setquota -u alice 500G 550G 0 0 /mnt/data\\n\\n# Check quotas\\nquota -u alice\\n\\n# View all quotas\\nsudo repquota -a\\n```\\n\\n## Integration with Data Processing Frameworks\\n\\nOne of Slurm\'s greatest strengths is integration with modern computing frameworks:\\n\\n### Apache Spark\\n\\nSubmit Spark jobs to Slurm:\\n\\n```bash\\n#!/bin/bash\\n#SBATCH --job-name=spark-job\\n#SBATCH --nodes=4\\n#SBATCH --ntasks-per-node=1\\n#SBATCH --cpus-per-task=8\\n#SBATCH --mem=32G\\n\\n# Load Spark module\\nmodule load spark/3.5.0\\n\\n# Run Spark application\\nspark-submit \\\\\\n  --master yarn \\\\\\n  --num-executors 4 \\\\\\n  --executor-cores 8 \\\\\\n  --executor-memory 28G \\\\\\n  my_spark_app.py\\n```\\n\\n### Ray (Distributed ML)\\n\\n```bash\\n#!/bin/bash\\n#SBATCH --job-name=ray-job\\n#SBATCH --nodes=2\\n#SBATCH --ntasks-per-node=1\\n#SBATCH --cpus-per-task=16\\n#SBATCH --gpus-per-node=2\\n\\n# Start Ray cluster\\nray start --head --port=6379\\nsrun --nodes=1 --ntasks=1 ray start --address=$HEAD_NODE:6379\\n\\n# Run Ray application\\npython ray_train.py\\n```\\n\\n### Dask\\n\\n```python\\nfrom dask_jobqueue import SLURMCluster\\nfrom dask.distributed import Client\\n\\ncluster = SLURMCluster(\\n    cores=8,\\n    memory=\\"16GB\\",\\n    processes=2,\\n    walltime=\\"02:00:00\\",\\n    queue=\\"compute\\"\\n)\\n\\ncluster.scale(jobs=10)  # Request 10 jobs\\nclient = Client(cluster)\\n\\n# Your Dask code here\\n```\\n\\n### Nextflow (Bioinformatics)\\n\\n```groovy\\n// nextflow.config\\nprocess {\\n    executor = \'slurm\'\\n    queue = \'compute\'\\n    memory = \'8 GB\'\\n    time = \'2h\'\\n}\\n```\\n\\nRun with:\\n```bash\\nnextflow run nf-core/rnaseq -profile slurm\\n```\\n\\n## Maintenance Tasks\\n\\n### Regular Updates\\n\\n```bash\\n# Update cluster via Ansible\\nansible-playbook -i inventories/hosts river_cluster.yml --tags update\\n\\n# Update specific nodes\\nansible-playbook -i inventories/hosts river_cluster.yml --limit worker-01,worker-02\\n```\\n\\n### Backup Critical Data\\n\\n```bash\\n# Backup Slurm configuration\\nsudo cp /etc/slurm-llnl/slurm.conf /backup/slurm.conf.$(date +%Y%m%d)\\n\\n# Backup accounting database\\nsudo mysqldump -u slurm -p slurm_acct_db > slurm_acct_backup_$(date +%Y%m%d).sql\\n\\n# Backup user data (use rsync for efficiency)\\nsudo rsync -av /home/ /backup/home/\\n```\\n\\n### Monitoring Disk Space\\n\\n```bash\\n# Check disk usage on all nodes\\nansible all -i inventories/hosts -m shell -a \\"df -h\\"\\n\\n# Check specific directory\\nansible all -i inventories/hosts -m shell -a \\"du -sh /var/log/slurm\\"\\n\\n# Find large files\\nfind /home -type f -size +1G -exec ls -lh {} \\\\;\\n```\\n\\n## Performance Optimization Tips\\n\\n### 1. Tune Scheduler Parameters\\n\\n```bash\\n# /etc/slurm-llnl/slurm.conf\\n\\n# Increase scheduling frequency\\nSchedulerTimeSlice=30\\n\\n# Prioritize recent submitters less\\nPriorityWeightAge=1000\\nPriorityWeightFairshare=10000\\n\\n# Enable backfill scheduling with larger window\\nSchedulerType=sched/backfill\\nbf_window=1440  # 24 hours\\n```\\n\\n### 2. Optimize Job Packing\\n\\n```bash\\n# Use CR_CPU for CPU-bound jobs\\nSelectType=select/cons_tres\\nSelectTypeParameters=CR_CPU\\n\\n# Or CR_Memory for memory-bound jobs\\nSelectTypeParameters=CR_Memory\\n\\n# Or CR_Core for mixed workloads\\nSelectTypeParameters=CR_Core\\n```\\n\\n### 3. Create Multiple Partitions\\n\\n```bash\\n# /etc/slurm-llnl/slurm.conf\\n\\n# Fast partition for short jobs\\nPartitionName=quick Nodes=worker-[01-02] Default=NO MaxTime=01:00:00 State=UP Priority=100\\n\\n# Standard partition\\nPartitionName=standard Nodes=worker-[01-04] Default=YES MaxTime=2-00:00:00 State=UP Priority=50\\n\\n# Long partition for extended jobs\\nPartitionName=long Nodes=worker-[03-04] Default=NO MaxTime=7-00:00:00 State=UP Priority=25\\n\\n# GPU partition\\nPartitionName=gpu Nodes=gpu-[01-02] Default=NO MaxTime=1-00:00:00 State=UP Priority=75\\n```\\n\\n### 4. Enable Job Arrays for Batch Processing\\n\\n```bash\\n#!/bin/bash\\n#SBATCH --array=1-100%10  # 100 tasks, max 10 concurrent\\n\\n# Process task based on array index\\npython process.py --input data_${SLURM_ARRAY_TASK_ID}.txt\\n```\\n\\n## Conclusion\\n\\nCongratulations! You now have the knowledge to build, deploy, and manage a production Slurm HPC cluster. Let\'s recap the journey:\\n\\n### Part 1: Foundations\\n- Understanding Slurm architecture\\n- Single-node setup for learning\\n- Critical cgroup configuration\\n- Job accounting basics\\n\\n### Part 2: Production Deployment\\n- Ansible automation\\n- Multi-node cluster setup\\n- Monitoring with Grafana\\n- Slack alerting\\n\\n### Part 3: Administration (This Post)\\n- User and resource management\\n- Node maintenance and troubleshooting\\n- Security hardening\\n- Performance optimization\\n- Framework integration\\n\\n## Key Takeaways\\n\\n1. **Start Simple, Scale Smart**: Master single-node before going multi-node\\n2. **Automate Everything**: Use Ansible for reproducible deployments\\n3. **Monitor Proactively**: Set up alerting before problems occur\\n4. **Security First**: SSH hardening, proper permissions, Docker caution\\n5. **Regular Maintenance**: Backups, updates, and log monitoring\\n6. **Documentation**: Document your cluster configuration and procedures\\n\\n## What\'s Next?\\n\\nConsider these advanced topics:\\n\\n- **High Availability**: Redundant controllers with failover\\n- **LDAP Integration**: Centralized authentication for large organizations\\n- **GPU Scheduling**: Optimize for machine learning workloads\\n- **Cloud Bursting**: Expand to cloud resources during peak demand\\n- **Custom Plugins**: Extend Slurm with custom scheduling policies\\n\\n## Resources\\n\\n- **Part 1**: [Single Node Setup](/blog/how-to-build-slurm-hpc-part-1)\\n- **Part 2**: [Production Deployment](/blog/how-to-build-slurm-hpc-part-2)\\n- **Documentation**: [Administration Guide](/docs/resources/high-performance-computing/how-to-build-slurm-scalable-using-ansible/administration)\\n- **GitHub**: [RiverXData Slurm Ansible](https://github.com/riverxdata/river-slurm)\\n- **Official Docs**: [SchedMD Slurm Documentation](https://slurm.schedmd.com/)\\n\\n## Contact\\n\\nHave questions or need help with your cluster? Reach out at: nttg8100@gmail.com\\n\\n---\\n\\n*This concludes the RiverXData series on building Slurm HPC clusters. Thank you for following along! We hope this guide helps you build and manage effective HPC infrastructure.*"},{"id":"how-to-build-slurm-hpc-part-2","metadata":{"permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-2","source":"@site/blog/2026-01/2026-01-12.md","title":"Building a Slurm HPC Cluster (Part 2) - Scaling to Production with Ansible","description":"In Part 1, we learned the fundamentals by building a single-node Slurm cluster. Now it\'s time to scale up to a production-ready, multi-node cluster with automated deployment, monitoring, and alerting.","date":"2026-01-12T00:00:00.000Z","tags":[{"inline":true,"label":"slurm","permalink":"/river-docs/blog/tags/slurm"},{"inline":true,"label":"hpc","permalink":"/river-docs/blog/tags/hpc"},{"inline":true,"label":"ansible","permalink":"/river-docs/blog/tags/ansible"},{"inline":true,"label":"automation","permalink":"/river-docs/blog/tags/automation"},{"inline":true,"label":"devops","permalink":"/river-docs/blog/tags/devops"}],"readingTime":8.87,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"how-to-build-slurm-hpc-part-2","title":"Building a Slurm HPC Cluster (Part 2) - Scaling to Production with Ansible","authors":["river"],"tags":["slurm","hpc","ansible","automation","devops"],"image":"./imgs/hpc_2.svg"},"unlisted":false,"prevItem":{"title":"Building a Slurm HPC Cluster (Part 3) - Administration and Best Practices","permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-3"},"nextItem":{"title":"Building a Slurm HPC Cluster (Part 1) - Single Node Setup and Fundamentals","permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-1"}},"content":"In [Part 1](/blog/how-to-build-slurm-hpc-part-1), we learned the fundamentals by building a single-node Slurm cluster. Now it\'s time to scale up to a production-ready, multi-node cluster with automated deployment, monitoring, and alerting.\\n\\nIn this post, we\'ll use Ansible to automate the entire deployment process, making it reproducible and maintainable.\\n\\n\x3c!--truncate--\x3e\\n\\n## Series Overview\\n\\n- **[Part 1](/blog/how-to-build-slurm-hpc-part-1)**: Introduction, Architecture, and Single Node Setup\\n- **Part 2 (This Post)**: Scaling to Production with Ansible\\n- **[Part 3](/blog/how-to-build-slurm-hpc-part-3)**: Administration and Best Practices\\n\\n## Why Ansible for HPC Clusters?\\n\\nMoving from a single-node setup to a multi-node production cluster involves:\\n\\n- Configuring multiple machines identically\\n- Managing dependencies and installation order\\n- Keeping configurations synchronized\\n- Handling user management across nodes\\n- Setting up monitoring and logging infrastructure\\n\\nDoing this manually is error-prone and time-consuming. Infrastructure automation tools like Ansible, Puppet, or Terraform solve this problem. We chose **Ansible** because:\\n\\n- **Agentless**: No software to install on managed nodes\\n- **Declarative**: Describe the desired state, not the steps\\n- **Idempotent**: Safe to run multiple times\\n- **YAML-based**: Easy to read and version control\\n- **Large ecosystem**: Many pre-built roles available\\n\\n### What is Ansible? (Quick Primer)\\n\\nIf you\'re new to Ansible, watch this excellent 100-second introduction:\\n\\n<div style={{ position: \\"relative\\", paddingBottom: \\"56.25%\\", height: 0, overflow: \\"hidden\\", maxWidth: \\"100%\\", background: \\"#000\\" }}>\\n  <iframe \\n    src=\\"https://www.youtube.com/embed/xRMPKQweySE\\" \\n    frameBorder=\\"0\\" \\n    allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" \\n    allowFullScreen\\n    style={{ position: \\"absolute\\", top: 0, left: 0, width: \\"100%\\", height: \\"100%\\" }}\\n  />\\n</div>\\n\\n## Production Cluster Architecture\\n\\nOur production setup includes these components:\\n\\n<figure markdown=\\"span\\">\\n    ![Small HPC Cluster](./imgs/small_HPC.jpg)\\n</figure>\\nSource: https://cs.phenikaa-uni.edu.vn/vi/post/gioi-thieu/co-so-vat-chat/he-thong-tinh-toan-hieu-nang-cao-phenikaa-hpc\\n\\n### Head Node (Controller + Login)\\n\\nThe head node manages the cluster and provides user access:\\n\\n- **slurmctld**: Job scheduling and resource management\\n- **slurmdbd**: Accounting database\\n- **NFS Server**: Shares directories to compute nodes\\n- **Prometheus**: Metrics collection\\n- **Grafana**: Monitoring dashboards\\n- **Alertmanager**: Slack notifications\\n\\n### Compute Nodes\\n\\nWorker nodes that execute jobs:\\n\\n- **slurmd**: Job execution daemon\\n- **NFS Client**: Mounts shared storage\\n- **Node Exporter**: Exposes system metrics\\n- **Slurm Exporter**: Exposes Slurm-specific metrics\\n\\n### Shared Storage (NFS)\\n\\nNFS provides unified file system access across all nodes:\\n\\n<figure markdown=\\"span\\">\\n    ![NFS Architecture](./imgs/NFS.png)\\n</figure>\\nSource: https://thuanbui.me/cai-dat-nfs-server-va-nfs-client-tren-ubuntu-22-04/\\n\\n- `/home`: User home directories (fast SSD/NVMe storage)\\n- `/mnt/data`: Large datasets (high-capacity HDD)\\n\\n### Monitoring Stack\\n\\nComplete observability for your cluster:\\n\\n<figure markdown=\\"span\\">\\n    ![Grafana Dashboard](./imgs/grafana.png)\\n</figure>\\nSource: https://swsmith.cc/posts/grafana-slurm.html\\n\\n- **Prometheus**: Time-series metrics database\\n- **Grafana**: Beautiful dashboards for visualization\\n- **Alertmanager**: Sends alerts to Slack when issues occur\\n- **Node Exporter**: System-level metrics (CPU, memory, disk)\\n- **Slurm Exporter**: Slurm-specific metrics (jobs, partitions, nodes)\\n\\n## Setting Up the RiverXData Slurm Cluster\\n\\nWe\'ve created a comprehensive Ansible playbook that automates everything. Let\'s get started!\\n\\n<figure markdown=\\"span\\">\\n    ![Slurm Deployment Architecture](./imgs/slurm.svg)\\n</figure>\\n\\n### Prerequisites\\n\\n- Multiple Ubuntu 20.04 or 24.04 machines (or VMs)\\n- SSH access to all nodes\\n- Sudo privileges on all nodes\\n- A Slack workspace (for alerts)\\n\\n### Step 1: Clone the Repository\\n\\n```bash\\ngit clone https://github.com/riverxdata/river-slurm.git -b 1.0.0\\ncd river-slurm\\n```\\n\\n### Step 2: Install Ansible and Dependencies\\n\\n```bash\\n# Ubuntu 24.04, without Vagrant\\nbash scripts/setup.sh 24.04 false\\n\\n# For Ubuntu 20.04\\nbash scripts/setup.sh 20.04 false\\n\\n# For developers: Install with Vagrant support\\nbash scripts/setup.sh 24.04 true\\n```\\n\\nThis script installs:\\n- Ansible and required Python packages\\n- Community Ansible collections\\n- Galaxy roles (geerlingguy.docker, etc.)\\n\\n### Step 3: Set Up Slack Alerts\\n\\n:::info\\nIn production environments, even small teams benefit from proactive monitoring. Slack is perfect for this - you\'ll get notifications when nodes go down, jobs fail, or resources run low.\\n:::\\n\\n#### Create a Slack App\\n\\n1. Go to [Slack API](https://api.slack.com/apps) and create a new app\\n2. Choose \\"From scratch\\"\\n3. Name it (e.g., \\"Slurm Cluster Monitor\\")\\n4. Select your workspace\\n\\n<figure markdown=\\"span\\">\\n    ![Create Slack App](./imgs/create_app.png)\\n</figure>\\n\\n#### Enable Incoming Webhooks\\n\\n1. Navigate to \\"Incoming Webhooks\\" in your app settings\\n2. Activate incoming webhooks\\n3. Click \\"Add New Webhook to Workspace\\"\\n4. Select the channel for notifications (e.g., `#cluster-alerts`)\\n5. Copy the webhook URL\\n\\n<figure markdown=\\"span\\">\\n    ![Slack Config Step 1](./imgs/config_1.png)\\n</figure>\\n\\n<figure markdown=\\"span\\">\\n    ![Slack Config Step 2](./imgs/config_2.png)\\n</figure>\\n\\n<figure markdown=\\"span\\">\\n    ![Slack Config Step 3](./imgs/config_3.png)\\n</figure>\\n\\n#### Test Your Webhook\\n\\n```bash\\ncurl -X POST -H \'Content-type: application/json\' \\\\\\n  --data \'{\\"text\\":\\"Hello from Slurm cluster!\\"}\' \\\\\\n  https://hooks.slack.com/services/YOUR/WEBHOOK/URL\\n```\\n\\nYou should see the message appear in your Slack channel!\\n\\n### Step 4: Configure Your Inventory\\n\\nCreate `inventories/hosts` (or copy from `inventories/hosts.example`):\\n\\n```ini\\n[slurm_master]\\ncontroller-01 ansible_host=192.168.58.10\\n\\n[slurm_worker]\\nworker-01 ansible_host=192.168.58.11\\nworker-02 ansible_host=192.168.58.12\\n\\n[slurm:children]\\nslurm_master\\nslurm_worker\\n\\n[all:vars]\\nansible_user=your_username\\nslurm_password=secure_munge_password\\nslurm_account_db_pass=secure_db_password\\nslack_api_url=https://hooks.slack.com/services/YOUR/WEBHOOK/URL\\nslack_channel=#cluster-alerts\\nadmin_user=admin\\nadmin_password=secure_grafana_password\\n```\\n\\n:::warning\\n**Security Best Practice**: Use [Ansible Vault](https://docs.ansible.com/ansible/latest/user_guide/vault.html) to encrypt sensitive variables like passwords and API keys.\\n\\n```bash\\n# Create encrypted vault\\nansible-vault create inventories/vault.yml\\n\\n# Or encrypt existing file\\nansible-vault encrypt inventories/hosts\\n```\\n:::\\n\\n#### Optional Parameters\\n\\n```ini\\ndefault_password=temporary_user_password  # Forces change on first login\\nusers=alice,bob,charlie                   # Comma-separated list\\n```\\n\\n### Step 5: Deploy the Cluster\\n\\nNow for the magic moment - deploy your entire cluster with one command!\\n\\n```bash\\n# If you have passwordless sudo configured\\nansible-playbook -i inventories/hosts river_cluster.yml\\n\\n# If you need to enter sudo password\\nansible-playbook -i inventories/hosts river_cluster.yml --ask-become-pass\\n```\\n\\nWhat this playbook does:\\n\\n1. **Prepares all nodes**:\\n   - Updates packages\\n   - Installs dependencies\\n   - Configures firewalls\\n\\n2. **Sets up the controller**:\\n   - Installs slurmctld and slurmdbd\\n   - Configures MariaDB for accounting\\n   - Sets up NFS server\\n   - Installs monitoring stack\\n\\n3. **Configures compute nodes**:\\n   - Installs slurmd\\n   - Mounts NFS shares\\n   - Configures metrics exporters\\n\\n4. **Deploys monitoring**:\\n   - Prometheus for metrics collection\\n   - Grafana with pre-configured dashboards\\n   - Alertmanager with Slack integration\\n\\n5. **Synchronizes configurations**:\\n   - Copies slurm.conf to all nodes\\n   - Sets up Munge authentication\\n   - Configures log aggregation\\n\\n### Step 6: Add Users\\n\\n```bash\\nansible-playbook -i inventories/hosts river_users.yml\\n```\\n\\nThis creates Linux users on all nodes with:\\n- Synchronized UID/GID across nodes\\n- Home directories on shared NFS\\n- Slurm accounting associations\\n\\n:::info\\n**Note on User Management**: For production, consider integrating with LDAP or Active Directory. However, NIS and LDAP setup can be complex on Ubuntu. Our Ansible approach provides a simpler alternative that works well for small to medium clusters.\\n:::\\n\\n### Step 7: Verify the Setup\\n\\nSSH into the controller node and run:\\n\\n```bash\\n# Check cluster status\\nsinfo\\n\\n# Expected output:\\n# PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\\n# compute*     up   infinite      2   idle worker-01,worker-02\\n\\n# View job queue\\nsqueue\\n\\n# Submit a test job\\nsrun --nodes=1 --ntasks=1 hostname\\n\\n# Check accounting\\nsacct\\n\\n# View cluster configuration\\nscontrol show config | head -20\\n```\\n\\n### Step 8: Access Grafana Dashboards\\n\\nGrafana runs on the controller node at port 3000. To access it securely from your local machine:\\n\\n```bash\\n# Create SSH tunnel\\nssh -N -L 3001:localhost:3000 your_user@controller_ip\\n\\n# Now open in browser: http://localhost:3001\\n# Login: admin / your_grafana_password\\n```\\n\\nYou\'ll see pre-configured dashboards showing:\\n\\n#### Node Metrics Dashboard\\n- CPU usage per node\\n- Memory utilization\\n- Disk I/O\\n- Network traffic\\n- System load\\n\\n<figure markdown=\\"span\\">\\n    ![Node Grafana Dashboard](./imgs/node_grafana.png)\\n</figure>\\n\\n#### Slurm Metrics Dashboard\\n- Active jobs\\n- Job queue length\\n- Node states (idle, allocated, down)\\n- CPU allocation\\n- Memory usage\\n- Job completion rates\\n\\n<figure markdown=\\"span\\">\\n    ![Slurm Grafana Dashboard](./imgs/slurm_grafana.png)\\n</figure>\\n\\n### What About Alerts?\\n\\nAlertmanager is configured to send Slack notifications for:\\n\\n- **Node down**: When a compute node becomes unresponsive\\n- **Node resumed**: When a node comes back online\\n- **High CPU usage**: Sustained high CPU across cluster\\n- **High memory usage**: Memory pressure warnings\\n- **Disk space low**: Storage running out\\n\\nExample alert in Slack when a node goes down:\\n\\n<figure markdown=\\"span\\">\\n    ![Node Resume Alert](./imgs/resume_node.png)\\n</figure>\\n\\nFor detailed information, check the Grafana dashboard:\\n\\n<figure markdown=\\"span\\">\\n    ![Grafana Node Down](./imgs/grafana_down.png)\\n</figure>\\n\\n## Testing Your Cluster\\n\\nLet\'s run some tests to ensure everything works:\\n\\n### Test 1: Simple Job\\n\\n```bash\\nsrun hostname\\n```\\n\\n### Test 2: Multi-node Job\\n\\n```bash\\nsrun --nodes=2 --ntasks=2 hostname\\n```\\n\\n### Test 3: Interactive Session\\n\\n```bash\\nsrun --nodes=1 --cpus-per-task=2 --mem=2G --pty bash\\n\\n# Inside the session\\nhostname\\nnproc\\nfree -h\\nexit\\n```\\n\\n### Test 4: Batch Job\\n\\nCreate `test_job.sh`:\\n\\n```bash\\n#!/bin/bash\\n#SBATCH --job-name=test\\n#SBATCH --output=test_%j.out\\n#SBATCH --error=test_%j.err\\n#SBATCH --nodes=1\\n#SBATCH --ntasks=1\\n#SBATCH --cpus-per-task=2\\n#SBATCH --mem=1G\\n#SBATCH --time=00:05:00\\n\\necho \\"Job started at $(date)\\"\\necho \\"Running on node: $(hostname)\\"\\necho \\"CPUs allocated: $SLURM_CPUS_PER_TASK\\"\\necho \\"Memory allocated: $SLURM_MEM_PER_NODE MB\\"\\n\\n# Do some work\\nsleep 60\\n\\necho \\"Job finished at $(date)\\"\\n```\\n\\nSubmit it:\\n\\n```bash\\nsbatch test_job.sh\\n\\n# Check status\\nsqueue\\n\\n# When done, view output\\ncat test_*.out\\n```\\n\\n### Test 5: Resource Limits\\n\\n```bash\\n# Submit job requesting more resources than available\\nsrun --mem=999999 --pty bash\\n\\n# Should fail with:\\n# srun: error: Unable to allocate resources: Requested node configuration is not available\\n```\\n\\n### Test 6: Accounting\\n\\n```bash\\n# View your jobs\\nsacct\\n\\n# Detailed accounting info\\nsacct --format=JobID,JobName,User,State,Start,End,Elapsed,CPUTime,MaxRSS\\n\\n# Cluster usage summary\\nsreport cluster utilization\\n```\\n\\n## Architecture Diagram\\n\\nHere\'s what you\'ve built:\\n\\n```\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502           Users SSH to Controller           \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n                  \u2502\\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n    \u2502   Controller Node (Head)     \u2502\\n    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\\n    \u2502  \u2502 slurmctld              \u2502  \u2502  Job Scheduling\\n    \u2502  \u2502 slurmdbd + MariaDB     \u2502  \u2502  Accounting\\n    \u2502  \u2502 NFS Server             \u2502  \u2502  Shared Storage\\n    \u2502  \u2502 Prometheus + Grafana   \u2502  \u2502  Monitoring\\n    \u2502  \u2502 Alertmanager           \u2502  \u2502  Alerts\\n    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\\n    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n         \u2502                  \u2502\\n    \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n    \u2502 worker-01\u2502       \u2502 worker-02\u2502\\n    \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502       \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\\n    \u2502 \u2502slurmd\u2502 \u2502       \u2502 \u2502slurmd\u2502 \u2502\\n    \u2502 \u2502NFS \u2191 \u2502 \u2502       \u2502 \u2502NFS \u2191 \u2502 \u2502\\n    \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502       \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\n## For Developers: Local Testing with Vagrant\\n\\nIf you want to test the deployment locally using VMs:\\n\\n```bash\\n# Install Vagrant with libvirt provider\\nbash scripts/setup.sh 24.04 true\\n\\n# Create local VMs and deploy cluster\\nvagrant up\\n\\n# SSH to controller\\nvagrant ssh controller-01\\n\\n# Destroy VMs when done\\nvagrant destroy -f\\n```\\n\\n## Key Takeaways\\n\\nIn this post, we\'ve covered:\\n\\n1. **Why Automation**: The benefits of using Ansible for cluster management\\n2. **Production Architecture**: Multi-node setup with monitoring and alerting\\n3. **Slack Integration**: Proactive monitoring with notifications\\n4. **Automated Deployment**: One command to deploy the entire cluster\\n5. **Verification**: Testing your cluster thoroughly\\n\\n:::info\\n**What\'s Next?**\\n\\nIn [Part 3](/blog/how-to-build-slurm-hpc-part-3), we\'ll cover daily administration tasks, troubleshooting, security best practices, and advanced resource management.\\n:::\\n\\n## Resources\\n\\n- **GitHub Repository**: [RiverXData Slurm Ansible](https://github.com/riverxdata/river-slurm)\\n- **Deployment Docs**: [Scalable Slurm Deployment](/docs/resources/high-performance-computing/how-to-build-slurm-scalable-using-ansible/deployment)\\n- **Architecture Overview**: [Slurm Architecture](/docs/resources/high-performance-computing/how-to-build-slurm-scalable-using-ansible/overview)\\n- **Ansible Documentation**: [docs.ansible.com](https://docs.ansible.com/)\\n\\n## Contact\\n\\nQuestions about the deployment? Reach out at: nttg8100@gmail.com\\n\\n---\\n\\n*This is Part 2 of the RiverXData series on building Slurm HPC clusters. Continue to [Part 3](/blog/how-to-build-slurm-hpc-part-3) for administration and best practices.*"},{"id":"how-to-build-slurm-hpc-part-1","metadata":{"permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-1","source":"@site/blog/2026-01/2026-01-09.md","title":"Building a Slurm HPC Cluster (Part 1) - Single Node Setup and Fundamentals","description":"Building a High-Performance Computing (HPC) cluster can seem daunting, but with the right approach, you can create a robust system for managing computational workloads. This is Part 1 of a 3-part series where we\'ll build a complete Slurm cluster from scratch.","date":"2026-01-09T00:00:00.000Z","tags":[{"inline":true,"label":"slurm","permalink":"/river-docs/blog/tags/slurm"},{"inline":true,"label":"hpc","permalink":"/river-docs/blog/tags/hpc"},{"inline":true,"label":"tutorial","permalink":"/river-docs/blog/tags/tutorial"},{"inline":true,"label":"getting-started","permalink":"/river-docs/blog/tags/getting-started"}],"readingTime":7.65,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"how-to-build-slurm-hpc-part-1","title":"Building a Slurm HPC Cluster (Part 1) - Single Node Setup and Fundamentals","authors":["river"],"tags":["slurm","hpc","tutorial","getting-started"],"image":"./imgs/hpc_1.svg"},"unlisted":false,"prevItem":{"title":"Building a Slurm HPC Cluster (Part 2) - Scaling to Production with Ansible","permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-2"},"nextItem":{"title":"RIVER- A Web Application to Run Nf-Core","permalink":"/river-docs/blog/river-platform-and-nextflow"}},"content":"Building a High-Performance Computing (HPC) cluster can seem daunting, but with the right approach, you can create a robust system for managing computational workloads. This is **Part 1** of a 3-part series where we\'ll build a complete Slurm cluster from scratch.\\n\\nIn this first post, we\'ll cover the fundamentals by setting up a single-node Slurm cluster and understanding the core concepts.\\n\\n\x3c!--truncate--\x3e\\n\\n## Series Overview\\n\\n- **Part 1 (This Post)**: Introduction, Architecture, and Single Node Setup\\n- **[Part 2](/blog/how-to-build-slurm-hpc-part-2)**: Scaling to Production with Ansible\\n- **[Part 3](/blog/how-to-build-slurm-hpc-part-3)**: Administration and Best Practices\\n\\n## Why Slurm?\\n\\nWhen it comes to job scheduling in HPC environments, several options exist including PBS, Grid Engine, and IBM\'s LSF. However, Slurm (Simple Linux Utility for Resource Management) stands out for several compelling reasons:\\n\\n- **Open Source**: Free to use with a large, active community\\n- **Scalability**: Designed to scale from small clusters to the world\'s largest supercomputers\\n- **Flexibility**: Fine-grained control over job scheduling, resource allocation, and priority settings\\n- **Integration**: Works seamlessly with MPI, distributed computing frameworks (Spark, Ray, Dask), and monitoring tools\\n- **Performance**: Optimized for high throughput with minimal overhead\\n\\n## Understanding Slurm Architecture\\n\\nBefore diving into the implementation, it\'s crucial to understand the key components of a Slurm cluster:\\n\\n<figure markdown=\\"span\\">\\n    ![HPC Architecture](./imgs/HPC_architecture.png)\\n</figure>\\nsource: https://www.marquette.edu/high-performance-computing/architecture.php\\n\\n### Core Components\\n\\n1. **slurmctld (Controller Daemon)**: The brain of the cluster, running on the controller node. It handles job scheduling, resource tracking, and communicates with compute nodes.\\n\\n2. **slurmd (Node Daemon)**: Runs on compute nodes to execute jobs and report status back to the controller.\\n\\n3. **slurmdbd (Database Daemon)**: Optional but recommended for storing job accounting data, resource usage tracking, and fair-share scheduling.\\n\\n### Node Types\\n\\n| Node Type  | Services                | Purpose                              |\\n| ---------- | ----------------------- | ------------------------------------ |\\n| Controller | slurmctld               | Manages job scheduling and resources |\\n| Compute    | slurmd                  | Executes submitted jobs              |\\n| Login      | Slurm clients           | User access point for job submission |\\n| Database   | slurmdbd, MySQL/MariaDB | Stores accounting data               |\\n\\n<figure markdown=\\"span\\">\\n    ![Slurm Architecture](./imgs/slurm_arch.gif)\\n</figure>\\nsource: https://www.schedmd.com/\\n\\nFor a deeper understanding of Slurm architecture, check our [Slurm Architecture documentation](/docs/resources/high-performance-computing/slurm-architecture).\\n\\n## Single Node Setup - Understanding the Fundamentals\\n\\nStarting with a single-node setup helps you understand how Slurm works before scaling up. This approach is perfect for learning and local development.\\n\\n<div align=\\"center\\">\\n    <figure markdown=\\"span\\">\\n        ![Slurm Logo](./imgs/Slurm_logo.svg)\\n    </figure>\\n</div>\\n\\n:::info\\nThis setup runs on Ubuntu 20.04 and includes all standard Slurm features. Note that this configuration is for learning purposes - for production environments, you\'ll want the multi-node setup covered in Part 2.\\n:::\\n\\n### Basic Installation\\n\\nFirst, install the required Slurm components:\\n\\n```bash\\nsudo apt-get update -y && sudo apt-get install -y slurmd slurmctld\\n```\\n\\nVerify the installation:\\n\\n```bash\\n# Locate slurmd and slurmctld\\nwhich slurmd\\n# Output: /usr/sbin/slurmd\\n\\nwhich slurmctld\\n# Output: /usr/sbin/slurmctld\\n```\\n\\n### Configuring slurm.conf\\n\\nThe `slurm.conf` file is the heart of your Slurm configuration. This file must be identical across all nodes in a cluster (but for now, we just have one node).\\n\\nCreate your `slurm.conf`:\\n\\n```bash\\ncat <<EOF > slurm.conf\\n# slurm.conf for a single-node Slurm cluster\\nClusterName=localcluster\\nSlurmctldHost=localhost\\nMpiDefault=none\\nProctrackType=proctrack/linuxproc\\nReturnToService=2\\nSlurmctldPidFile=/run/slurmctld.pid\\nSlurmctldPort=6817\\nSlurmdPidFile=/run/slurmd.pid\\nSlurmdPort=6818\\nSlurmdSpoolDir=/var/lib/slurm-llnl/slurmd\\nSlurmUser=slurm\\nStateSaveLocation=/var/lib/slurm-llnl/slurmctld\\nSwitchType=switch/none\\nTaskPlugin=task/none\\n\\n# TIMERS\\nInactiveLimit=0\\nKillWait=30\\nMinJobAge=300\\nSlurmctldTimeout=120\\nSlurmdTimeout=300\\nWaittime=0\\n\\n# SCHEDULING\\nSchedulerType=sched/backfill\\nSelectType=select/cons_tres\\nSelectTypeParameters=CR_Core\\n\\n# ACCOUNTING (not enabled yet)\\nAccountingStorageType=accounting_storage/none\\nJobAcctGatherType=jobacct_gather/none\\nJobAcctGatherFrequency=30\\n\\n# LOGGING\\nSlurmctldDebug=info\\nSlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log\\nSlurmdDebug=info\\nSlurmdLogFile=/var/log/slurm-llnl/slurmd.log\\n\\n# COMPUTE NODES (adjust CPUs and RealMemory to match your system)\\nNodeName=localhost CPUs=2 Sockets=1 CoresPerSocket=2 ThreadsPerCore=1 RealMemory=1024 State=UNKNOWN\\n\\n# PARTITION CONFIGURATION\\nPartitionName=LocalQ Nodes=ALL Default=YES MaxTime=INFINITE State=UP\\nEOF\\n\\nsudo mv slurm.conf /etc/slurm-llnl/slurm.conf\\n```\\n\\n### Starting Services\\n\\nStart the Slurm daemons:\\n\\n```bash\\n# Start slurmd (compute daemon)\\nsudo service slurmd start\\nsudo service slurmd status\\n\\n# Start slurmctld (controller daemon)\\nsudo service slurmctld start\\nsudo service slurmctld status\\n```\\n\\n<figure markdown=\\"span\\">\\n    ![slurmd status](./imgs/slurmd_status.png)\\n</figure>\\n\\n<figure markdown=\\"span\\">\\n    ![slurmctld status](./imgs/slurmctld_status.png)\\n</figure>\\n\\nTest your setup by submitting a simple interactive job:\\n\\n```bash\\nsrun --mem 500MB -c 1 --pty bash\\n\\n# Check job details\\nsqueue -o \\"%i %P %u %T %M %l %D %C %m %R %Z %N\\" | column -t\\n```\\n\\n### Critical: Resource Limiting with cgroups\\n\\n:::warning\\n**This is a critical step that\'s often overlooked!**\\n\\nWithout proper cgroup configuration, jobs can exceed their allocated resources, potentially causing system instability or crashes. The job scheduler will accept resource limits, but won\'t actually enforce them.\\n:::\\n\\nLet\'s test this problem first. Submit a job requesting 500MB and try to allocate much more:\\n\\n```bash\\nsrun --mem 500MB -c 1 --pty bash\\n\\n# Try to allocate 1GB of memory (exceeding the 500MB limit)\\ndeclare -a mem\\ni=0\\nwhile :; do\\n    mem[$i]=$(head -c 100M </dev/zero | tr \'\\\\000\' \'x\') \\n    ((i++))\\n    echo \\"Allocated: $((i * 100)) MB\\"\\ndone\\n```\\n\\nBefore submitting the job, memory usage is less than 200MB:\\n\\n<figure markdown=\\"span\\">\\n    ![Memory before stress](./imgs/memory_before_stress.png)\\n</figure>\\n\\nAfter allocating 1GB, the job is not killed due to missing control group (cgroup) configuration:\\n\\n<figure markdown=\\"span\\">\\n    ![Over resource limit](./imgs/overresource_limit.png)\\n</figure>\\n\\nYou\'ll notice the job continues running even after exceeding 500MB - that\'s the problem!\\n\\nNow let\'s fix it with cgroups:\\n\\n```bash\\ncat <<EOF > cgroup.conf\\nCgroupAutomount=yes\\nCgroupMountpoint=/sys/fs/cgroup\\nConstrainCores=yes\\nConstrainRAMSpace=yes\\nConstrainDevices=yes\\nConstrainSwapSpace=yes\\nMaxSwapPercent=5\\nMemorySwappiness=0\\nEOF\\n\\nsudo mv cgroup.conf /etc/slurm-llnl/cgroup.conf\\n```\\n\\nUpdate `slurm.conf` to use cgroup plugins:\\n\\n```bash\\nsudo sed -i -e \\"s|ProctrackType=proctrack/linuxproc|ProctrackType=proctrack/cgroup|\\" \\\\\\n            -e \\"s|TaskPlugin=task/none|TaskPlugin=task/cgroup|\\" /etc/slurm-llnl/slurm.conf\\n```\\n\\nEnable cgroup in GRUB and reboot:\\n\\n```bash\\nsudo sed -i \'s/^GRUB_CMDLINE_LINUX=\\"/GRUB_CMDLINE_LINUX=\\"cgroup_enable=memory swapaccount=1 /\' /etc/default/grub\\nsudo update-grub\\nsudo reboot\\n```\\n\\nAfter reboot, restart Slurm services:\\n\\n```bash\\nsudo service slurmctld restart\\nsudo service slurmd restart\\n```\\n\\nNow test again with the same memory allocation script - this time, the job will be killed when it exceeds the limit!\\n\\n<figure markdown=\\"span\\">\\n    ![Out of Memory](./imgs/oom.png)\\n</figure>\\n\\n### Enabling Accounting\\n\\nJob accounting is essential for:\\n- Tracking who is using resources\\n- Monitoring job completion and failures\\n- Enforcing resource limits per user/group\\n- Fair-share scheduling\\n\\n<figure markdown=\\"span\\">\\n    ![Accounting disabled](./imgs/sacct_disable.png)\\n</figure>\\n\\nInstall the required packages:\\n\\n```bash\\nsudo apt-get install slurmdbd mariadb-server -y\\n```\\n\\nCreate the database and user:\\n\\n```bash\\nsudo service mysql start\\n\\nsudo mysql -e \\"CREATE DATABASE slurm_acct_db;\\"\\nsudo mysql -e \\"CREATE USER \'slurm\'@\'localhost\' IDENTIFIED BY \'slurm\';\\"\\nsudo mysql -e \\"GRANT ALL PRIVILEGES ON slurm_acct_db.* TO \'slurm\'@\'localhost\';\\"\\nsudo mysql -e \\"FLUSH PRIVILEGES;\\"\\n```\\n\\nVerify the database was created:\\n\\n```bash\\nsudo mysql -e \\"SHOW DATABASES;\\" \\nsudo mysql -e \\"SELECT User, Host FROM mysql.user;\\"\\n```\\n\\n<figure markdown=\\"span\\">\\n    ![Add database](./imgs/add_db.png)\\n</figure>\\n\\nConfigure `slurmdbd`:\\n\\n```bash\\ncat <<EOF > slurmdbd.conf\\nPidFile=/run/slurmdbd.pid\\nLogFile=/var/log/slurm/slurmdbd.log\\nDebugLevel=error\\nDbdHost=localhost\\nDbdPort=6819\\n\\n# DB connection data\\nStorageType=accounting_storage/mysql\\nStorageHost=localhost\\nStoragePort=3306\\nStorageUser=slurm\\nStoragePass=slurm\\nStorageLoc=slurm_acct_db\\nSlurmUser=slurm\\nEOF\\n\\nsudo mv slurmdbd.conf /etc/slurm-llnl/slurmdbd.conf\\nsudo service slurmdbd start\\n```\\n\\nUpdate `slurm.conf` to enable accounting:\\n\\n```bash\\nsudo sed -i -e \\"s|AccountingStorageType=accounting_storage/none|AccountingStorageType=accounting_storage/slurmdbd\\\\nAccountingStorageEnforce=associations,limits,qos\\\\nAccountingStorageHost=localhost\\\\nAccountingStoragePort=6819|\\" /etc/slurm-llnl/slurm.conf \\n\\nsudo sed -i -e \\"s|JobAcctGatherType=jobacct_gather/none|JobAcctGatherType=jobacct_gather/cgroup|\\" /etc/slurm-llnl/slurm.conf\\n\\nsudo systemctl restart slurmctld slurmd\\n```\\n\\nAdd your cluster and user to accounting:\\n\\n```bash\\n# Add cluster\\nsudo sacctmgr -i add cluster localcluster\\n\\n# Add account for your user\\nsudo sacctmgr -i add account $USER Cluster=localcluster\\n\\n# Add your user to the account\\nsudo sacctmgr -i add user $USER account=$USER DefaultAccount=$USER\\n\\nsudo systemctl restart slurmctld slurmd\\n```\\n\\n<figure markdown=\\"span\\">\\n    ![Add account](./imgs/add_account.png)\\n</figure>\\n\\nNow test accounting by submitting a job and viewing its details:\\n\\n```bash\\n# Submit a test job\\nsrun --mem 500MB -c 1 hostname\\n\\n# View accounting information\\nsacct\\n```\\n\\n<figure markdown=\\"span\\">\\n    ![Account usage](./imgs/account_usage.png)\\n</figure>\\n\\n## Key Takeaways\\n\\nIn this first part of our series, we\'ve covered:\\n\\n1. **Why Slurm**: Understanding the advantages of Slurm over alternatives\\n2. **Architecture**: Core components (slurmctld, slurmd, slurmdbd) and their roles\\n3. **Basic Setup**: Installing and configuring a single-node cluster\\n4. **Critical cgroups**: Why resource limiting is essential (and how to enable it)\\n5. **Accounting**: Setting up job tracking and resource monitoring\\n\\n:::info\\n**What\'s Next?**\\n\\nIn [Part 2](/blog/how-to-build-slurm-hpc-part-2), we\'ll take this knowledge and scale to a multi-node production cluster using Ansible automation. We\'ll add monitoring with Grafana, alerting via Slack, and shared storage with NFS.\\n:::\\n\\n## Resources\\n\\n- **Full Documentation**: [Single Node Slurm Setup](/docs/resources/high-performance-computing/how-to-build-slurm-single-node-with-full-functions)\\n- **Architecture Details**: [Slurm Architecture](/docs/resources/high-performance-computing/slurm-architecture)\\n- **HPC Overview**: [High-Performance Computing Overview](/docs/resources/high-performance-computing/high-performance-computing-overview)\\n- **Official Docs**: [SchedMD Slurm Documentation](https://slurm.schedmd.com/)\\n\\n---\\n\\n*This is Part 1 of the RiverXData series on building Slurm HPC clusters. Continue to [Part 2](/blog/how-to-build-slurm-hpc-part-2) to learn about production deployment with Ansible.*"},{"id":"river-platform-and-nextflow","metadata":{"permalink":"/river-docs/blog/river-platform-and-nextflow","source":"@site/blog/2026-01/2026-01-08.md","title":"RIVER- A Web Application to Run Nf-Core","description":"Simply, I just want a web application. This is an all-in-one application similar to Google Drive, but more advanced. It allows me to select files and put them into a workflow obtained directly from GitHub. For example, rnaseq from nf-core. The web application allows me to run and monitor Nextflow jobs. Then, it puts the results back to Google Drive, where I can view the results. It is useful for sharing results with my team or external teams. I can develop pipelines independently. My data can be stored on the cloud for backup. It is a simple and standard procedure for bioinformatics analysis. But I have not found any solution that fits these requirements is EASY, FREE, OPEN-SOURCE","date":"2026-01-08T00:00:00.000Z","tags":[{"inline":true,"label":"s3","permalink":"/river-docs/blog/tags/s-3"},{"inline":true,"label":"data-analysis","permalink":"/river-docs/blog/tags/data-analysis"},{"inline":true,"label":"slurm","permalink":"/river-docs/blog/tags/slurm"},{"inline":true,"label":"hpc","permalink":"/river-docs/blog/tags/hpc"},{"inline":true,"label":"web-platform","permalink":"/river-docs/blog/tags/web-platform"}],"readingTime":2.14,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang Tan Nguyen","title":"Founder at RIVER","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"river-platform-and-nextflow","title":"RIVER- A Web Application to Run Nf-Core","authors":["river"],"tags":["s3","data-analysis","slurm","hpc","web-platform"],"image":"./imgs/intro.png"},"unlisted":false,"prevItem":{"title":"Building a Slurm HPC Cluster (Part 1) - Single Node Setup and Fundamentals","permalink":"/river-docs/blog/how-to-build-slurm-hpc-part-1"}},"content":"Simply, I just want a web application. This is an all-in-one application similar to Google Drive, but more advanced. It allows me to select files and put them into a workflow obtained directly from GitHub. For example, rnaseq from nf-core. The web application allows me to run and monitor Nextflow jobs. Then, it puts the results back to Google Drive, where I can view the results. It is useful for sharing results with my team or external teams. I can develop pipelines independently. My data can be stored on the cloud for backup. It is a simple and standard procedure for bioinformatics analysis. But I have not found any solution that fits these requirements is `EASY`, `FREE`, `OPEN-SOURCE`\\n\\nIf you feel the same, **RIVER** may be your choice.\\n\\n\x3c!-- truncate --\x3e\\n:::info\\nThis blog post is intended for the general community, show cases only. I\'ll focus on the big picture rather than diving deep into technical details.\\n:::\\n\\nTo use it, you can follow the below tutorial\\n\\n## Quick review\\nFirst, you can take a look at how it work first. In breif, it will allows the users to interact with S3 bucket, select inputs from S3 buckets, run and put back results to s3 that\\nRIVER platform can visualize. After watching this, you are able to configure the RIVER platform as contributor, add your interested workflows and allow you to run tools\\n\\n\\n\\n<div style={{ position: \\"relative\\", paddingBottom: \\"56.25%\\", height: 0, overflow: \\"hidden\\", maxWidth: \\"100%\\", background: \\"#000\\" }}>\\n  <iframe \\n    src=\\"https://www.youtube.com/embed/boabEFNIkNA\\" \\n    frameBorder=\\"0\\" \\n    allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" \\n    allowFullScreen\\n    style={{ position: \\"absolute\\", top: 0, left: 0, width: \\"100%\\", height: \\"100%\\" }}\\n  />\\n</div> \\n\\n## Get started\\nYour raw data should be uploaded to the platform. The fastq files are downloaded from the below url, then upload to the platform\\n\\nThe samplesheet.csv on test profile\\n```bash\\nsample,fastq_1,fastq_2\\nSAMPLE1_PE,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample1_R1.fastq.gz,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample1_R2.fastq.gz\\nSAMPLE2_PE,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample2_R1.fastq.gz,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample2_R2.fastq.gz\\n```\\n\\nThe samplesheet.csv on RIVER\\n```bash\\nsample,fastq_1,fastq_2\\ntest,s3://genomics/demo/resources/fastq_files/sample1_R1.fastq.gz,s3://genomics/demo/resources/fastq_files/sample1_R2.fastq.gz\\n```\\n\\n<div style={{ position: \\"relative\\", paddingBottom: \\"56.25%\\", height: 0, overflow: \\"hidden\\", maxWidth: \\"100%\\", background: \\"#000\\" }}>\\n  <iframe \\n    src=\\"https://www.youtube.com/embed/Fujhi6ZnHQQ\\" \\n    frameBorder=\\"0\\" \\n    allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" \\n    allowFullScreen\\n    style={{ position: \\"absolute\\", top: 0, left: 0, width: \\"100%\\", height: \\"100%\\" }}\\n  />\\n</div>"}]}}')}}]);