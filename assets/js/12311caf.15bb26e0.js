"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[6959],{2543:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"setup-shell-remote-server","metadata":{"permalink":"/river-docs/blog/setup-shell-remote-server","source":"@site/blog/2025-03-30/setup-remote-server.md","title":"Hack your shell environment","description":"Setting up a shell environment on a remote server can be a tedious process, especially when dealing with multiple dependencies. This one also can be used for your local machine.","date":"2025-03-30T00:00:00.000Z","tags":[{"inline":true,"label":"zsh","permalink":"/river-docs/blog/tags/zsh"},{"inline":true,"label":"river","permalink":"/river-docs/blog/tags/river"},{"inline":true,"label":"micromamba","permalink":"/river-docs/blog/tags/micromamba"}],"readingTime":4.14,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang (River) Tan Nguyen","title":"Software and bioinformatics engineer","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"setup-shell-remote-server","title":"Hack your shell environment","authors":["river"],"tags":["zsh","river","micromamba"]},"unlisted":false,"nextItem":{"title":"SSH remote tunnel","permalink":"/river-docs/blog/ssh-remote-tunnel"}},"content":"Setting up a shell environment on a remote server can be a tedious process, especially when dealing with multiple dependencies. This one also can be used for your local machine.\\nThis guide walks you through installing **micromamba**, setting up a River environment, and configuring useful tools like **goofys** to work with cloud storage  (S3) and zsh (amazing shell)\\n![zsh](./zsh.png)\\n\\n\x3c!-- truncate --\x3e\\n## Set up variables\\nIt will define a hard version for installing goofys, micromamba. You can adjust to update latest version \\n```bash\\necho \\"River software dependencies setup\\"\\nMICROMAMBA_VERSION=2.0.5\\nGOOFYS_VERSION=0.24.0\\nRIVER_BIN=$HOME/.river/bin\\nmkdir -p $RIVER_BIN\\n```\\n\\n## Install micromamba\\nMicromamba is a lightweight, fast alternative to Conda for managing environments and packages. \\nIt is a small, standalone executable that provides the same package management features as Conda but with much lower overhead. Unlike Conda, Micromamba does not require a full Anaconda installation, making it ideal for minimal setups, CI/CD pipelines, and remote servers.\\n\\n:::info\\nKey Features:\\n\\n+ Fast and lightweight: Much smaller than Conda, with a quick installation process.\\n+ Standalone executable: No need for a full Conda installation.\\n+ Supports Conda environments: Fully compatible with Conda packages and repositories.\\n+ Works in remote/cloud environments: Ideal for automation and scripting\\n:::\\n\\nCreate an environmen call `river` that helps to install common software without required `sudo` permision, install based on user space:\\n+ Python (from Anaconda) for scripting and development.\\n+ R-base for statistical computing and bioinformatics applications.\\n+ Singularity (v3.8.6) for containerized workflows. The singularity has the image is similar to a file, it is portable, has a great integration with the host system\\n+ Nextflow to enable scalable and reproducible scientific workflows.\\n+ Zsh for a better shell experience.\\n\\nAWS CLI to interact with AWS services like S3.\\n```bash\\n# Base softwares\\n# micromamba\\nexport HOME=$HOME\\nexport MICROMAMBA_EXECUTE=$HOME/.river/bin\\nexport PATH=$HOME/.river/bin:$PATH\\nmkdir -p $MICROMAMBA_EXECUTE\\nif [ ! -f \\"$RIVER_BIN/micromamba\\"  ]; then\\n    echo \\"Installing micromamba...\\"\\n    curl -Ls https://micro.mamba.pm/api/micromamba/linux-64/$MICROMAMBA_VERSION | tar -xvj bin/micromamba\\n    mv bin/micromamba $RIVER_BIN/micromamba\\n    rm -rf bin\\nelse\\n    echo \\"micromamba already exists at: $HOME/.river/bin\\"\\nfi\\n# Start up\\nexport PATH=\\"$HOME/.river/bin:$PATH\\"\\nexport MAMBA_ROOT_PREFIX=\\"$HOME/.river/images/micromamba\\"\\n# Create the singularity dir\\nmkdir -p $HOME/.river/images/singularities/images\\n\\n# Create micromamba environment and install river-utils\\nmicromamba create -n river \\\\\\n    anaconda::python \\\\\\n    conda-forge::r-base \\\\\\n    conda-forge::singularity=3.8.6 \\\\\\n    bioconda::nextflow \\\\\\n    conda-forge::zsh \\\\\\n    conda-forge::awscli \\\\\\n    -y\\n```\\n\\n### Activate environment\\nTo install additional software, you can install it in the `river` environment or create a new environment. Below is an example of how to activate the `river` environment and install additional software.\\nIt is based on conda, what you need to do is searching for your softwares. Almost are hosted on the [**conda**](https://anaconda.org/). Find the command and replace `conda` with `micromamba`.\\n\\n```bash\\n# Activate the river environment\\neval \\"$(micromamba shell hook --shell bash)\\"\\nmicromamba activate river\\n\\n# Install additional software\\nmicromamba install -n river \\\\\\n    conda-forge::htop \\\\\\n    conda-forge::jq \\\\\\n    conda-forge::tree \\\\\\n    -y\\n```\\n### Create new environment\\n```bash\\n# Create a new environment and install Python\\nmicromamba create -n new_env \\\\\\n    anaconda::python=3.9 \\\\\\n    -y\\n\\n# Activate the new environment\\nmicromamba activate new_env\\n\\n# Verify Python installation\\npython --version\\n```\\n![micromamba](./micromamba.png)\\n\\n**Figure 1:** Micromamba helps to setup your perfect working environments on remote server\\n\\nIn this example:\\n- `new_env`: The name of the new environment.\\n- `python=3.9`: Specifies the version of Python to install. You can adjust the version as needed.\\n\\nThis allows you to create isolated environments for different projects or dependencies. It is good practice to install tools in different environemnts.\\nIf you develop web application, install `npm`, `python`, `java`, etc on different environments\\n\\n##  Goofys fuse file system for S3\\nFollow blog with tag goofys for more information. It is simply making a cloud storage (compatible S3) to be worked as local file system with nearly full POSIX support\\n```bash\\n# goofys\\nif [ ! -f \\"$RIVER_BIN/goofys\\"  ]; then\\n    echo \\"Installing goofys...\\"\\n    curl -L https://github.com/kahing/goofys/releases/download/v${GOOFYS_VERSION}/goofys -o $RIVER_BIN/goofys\\n    chmod +x $RIVER_BIN/goofys\\nelse\\n    echo \\"Goofys already exists at: $HOME/.river/bin\\"\\n    goofys --help 2> /dev/null\\nfi\\n```\\n\\n## Improve your shell experience\\nInstall the zsh and related extensions. Here is the standard ones, but you can find more about **zsh**[https://ohmyz.sh/]\\n\\n```bash\\neval \\"$(micromamba shell hook --shell bash)\\"\\nmicromamba activate river\\nsh -c \\"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\\" \\"\\" --unattended\\n\\n# plugins\\ngit clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions\\ngit clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting\\n\\n# Update .zshrc\\necho \\"Updating .zshrc...\\"\\nsed -i \\"s|plugins=(git)|plugins=(\\\\n    git\\\\n    docker\\\\n    docker-compose\\\\n    history\\\\n    rsync\\\\n    safe-paste\\\\n    zsh-autosuggestions\\\\n    zsh-syntax-highlighting\\\\n)\\\\n|\\" ~/.zshrc\\nsource ~/.zshrc\\n```\\n\\nNow you can see how it improve your experiences;\\n+ Your systax is highlighted to show it correctly work\\n+ Your history can be quickly reused\\n  \\n\\n![zsh](./zsh.png)\\n\\n**Figure 2:** zsh improves your shell experiences\\n\\n## Activate all by `.river.sh`\\nCreate the file called `.river.sh` then you can activate all standard setup\\n\\n```bash\\n# Create .river.sh for environment variables\\ncat <<EOF > $HOME/.river.sh\\nexport HOME=${HOME}\\nexport HOME_TOOLS=\\\\${HOME}/.river/bin\\nexport MAMBA_ROOT_PREFIX=\\\\${HOME}/.river/images/micromamba\\nexport SINGULARITY_CACHE_DIR=\\\\${HOME}/.river/images/singularities\\nexport NXF_SINGULARITY_CACHEDIR=\\\\$SINGULARITY_CACHE_DIR/images\\nexport PATH=\\\\${HOME_TOOLS}:\\\\$PATH\\neval \\"\\\\$(micromamba shell hook -s posix)\\"\\nmicromamba activate -n river\\nzsh \\nsource ~/.zshrc\\nEOF\\n```\\n\\nYou can activate absolutely when you are login by `~/.bashrc`\\n```bash\\nsource ~/.river.sh\\n```"},{"id":"ssh-remote-tunnel","metadata":{"permalink":"/river-docs/blog/ssh-remote-tunnel","source":"@site/blog/2025-03-29/ssh-remote-tunnelling.md","title":"SSH remote tunnel","description":"When working on a remote High-Performance Computing (HPC) cluster or a cloud server, accessing development tools locally can be challenging.","date":"2025-03-29T00:00:00.000Z","tags":[{"inline":true,"label":"hpc","permalink":"/river-docs/blog/tags/hpc"},{"inline":true,"label":"ssh","permalink":"/river-docs/blog/tags/ssh"},{"inline":true,"label":"network","permalink":"/river-docs/blog/tags/network"},{"inline":true,"label":"slurm","permalink":"/river-docs/blog/tags/slurm"}],"readingTime":1.48,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang (River) Tan Nguyen","title":"Software and bioinformatics engineer","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"ssh-remote-tunnel","title":"SSH remote tunnel","authors":["river"],"tags":["hpc","ssh","network","slurm"]},"unlisted":false,"prevItem":{"title":"Hack your shell environment","permalink":"/river-docs/blog/setup-shell-remote-server"},"nextItem":{"title":"Python thread: deep dive","permalink":"/river-docs/blog/python-thread"}},"content":"When working on a remote High-Performance Computing (HPC) cluster or a cloud server, accessing development tools locally can be challenging. \\nOne effective approach is to use an SSH tunnel to securely access a galaxy server-a web platform as if it were running on your local machine.\\n![Processes in Computer](./remote_server.webp)\\n\\n\\n\x3c!-- truncate --\x3e\\n\\n:::info\\n+ Replace with your host and port for ssh\\n+ Run any web on specific port, then forward this web service via ssh to access at your local machine\\n+ Require to install [**docker**](https://docs.docker.com/engine/install/). For more details on, please visit [**galaxy docker**](https://github.com/bgruening/docker-galaxy)\\n:::\\n## Why Use SSH Tunneling?\\nSSH tunneling allows you to securely forward ports from a remote server to your local system. This is useful for accessing services that are running on the remote machine without exposing them to the internet.\\n\\n\\n\\n\\n## Login to your remote server\\n```bash\\nssh river@platform.riverxdata.com\\n```\\n\\n## Run a web service\\nIt can be accessed at localhost 8080\\nRun galaxy server- a web platform for bioinformatics:\\n```bash\\ndocker run -d -p 8080:80 \\\\\\n    -v ./galaxy_storage/:/export/ \\\\\\n    quay.io/bgruening/galaxy\\n```\\n\\nThe web is now available on a remote system on the port 8080, docker bind the port of the web is running at 80 on the container.\\n\\n## Test the web service\\nTest the web service at 8080\\n```bash\\ncurl localhost -p 8080\\n```\\n![Processes in Computer](./login_and_run_web.png)\\n\\n**Figure 1:** Login and start galaxy server using docker on remote machine\\n\\nNow you can access the galaxy at your local computer via ssh. With the local machine will access this network at port 8081 which is remoted from 8080 of the `platform.riverxdata.com`\\n```bash\\nssh -N -L 8081:localhost:8080 platform.riverxdata.com\\n```\\n\\nOpen your web brower to see how it work\\n![Processes in Computer](./galaxy_local.png)\\n**Figure 2:** Access your web service at local machine"},{"id":"python-thread","metadata":{"permalink":"/river-docs/blog/python-thread","source":"@site/blog/2025-03-28/stop-using-python-thread.md","title":"Python thread: deep dive","description":"Modern computers are designed to handle multitasking, enabling you to run multiple programs simultaneously. But have you ever wondered how computers manage this complexity?","date":"2025-03-28T00:00:00.000Z","tags":[{"inline":true,"label":"python","permalink":"/river-docs/blog/tags/python"}],"readingTime":5.41,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang (River) Tan Nguyen","title":"Software and bioinformatics engineer","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"python-thread","title":"Python thread: deep dive","authors":["river"],"tags":["python"]},"unlisted":false,"prevItem":{"title":"SSH remote tunnel","permalink":"/river-docs/blog/ssh-remote-tunnel"},"nextItem":{"title":"Single Node Slurm Setup","permalink":"/river-docs/blog/single-node-slurm-setup"}},"content":"Modern computers are designed to handle multitasking, enabling you to run multiple programs simultaneously. But have you ever wondered how computers manage this complexity? \\n\\nIn programming, Python is one of the most popular languages, and it supports multitasking through multiple processes and threads. However, Python has a unique feature that might lead to inefficient usage if not understood properly. Let\u2019s dive in and explore.\\n\\n![Processes in Computer](./process_by_htop.png)  \\n**Figure 1:** A system monitor displaying process IDs (PIDs), user ownership, and resource consumption (e.g., memory and CPU).\\n\\n\x3c!-- truncate --\x3e\\n\\n## What Is a Process in Computing?\\n\\nA process is an instance of a program being executed by a computer. It includes the program\'s code, data, and allocated system resources. Each process operates **independently** in its own memory space and can spawn multiple threads for concurrent execution. \\n\\nThe operating system is responsible for managing processes, enabling them to communicate, **schedule CPU time**, and **share resources** efficiently.\\n\\n\\n## Thread in python\\nIn Python, a thread runs within a single process and is managed by that process. A single process can run multiple threads, and these threads share computing\\nresources such as CPU and memory. However, due to Python\u2019s Global Interpreter Lock (GIL), threads are restricted to executing on a single CPU core at a time, \\neven if multiple CPU cores are available. \\n:::info\\nWhat, why and how python requires GIL ?\\n+ The Global Interpreter Lock (GIL) is a mechanism in CPython that ensures only one thread executes Python bytecode at a time, even on multi-core processors.\\n+ It prevents race conditions in memory management but limits true parallel execution for CPU-bound tasks. To achieve real parallelism, multiprocessing should be used instead of threading.\\n:::\\n\\n## What is GIL and how thread in python work ?\\n:::warning\\nThis is the proof of concept for the python3 lower than 3.13. From python 3.13, the GIL can be disabled, with evenly higher performance with cpu-bound tasks while sharing the same resource (memory,etc)\\n:::\\n\\nThe **Global Interpreter Lock (GIL)** is a mechanism in Python that manages thread execution by allowing only **one thread to run at a time**, even on multi-core processors. \\nAlthough multiple threads can be created, they do not achieve true parallelism for CPU-bound tasks due to the GIL.\\n\\nHowever, Python threads can still improve performance for **I/O-bound tasks** like downloading or uploading files because the CPU spends most of its time waiting for external resources.\\nThe Python interpreter quickly **switches between threads**, saving and restoring their states, making it appear as if multiple threads are running simultaneously.\\nIn reality, the **CPU rapidly cycles through threads**, ensuring that tasks that do not require significant computation feel like they are running in parallel.\\n\\n\\n## Simple process with thread in python\\n### Simple program\\n+ A simple function in python, create 2 files as below `single_worker.py`. Then you can duplicate the `worker` step, it is similar in real world problem when you have IO tasks likes\\ndownloading multiple files\\n+ Here to make it easier, I uses `sleep` which is similar.\\n```bash\\ndownload_file(<file_url_1>)\\ndownload_file(<file_url_2>)\\n```\\n\\n**single_worker.py**\\n```python\\nimport time\\n\\ndef worker(task_id, duration):\\n\\n    \\"\\"\\"Simulate work by sleeping for `duration` seconds.\\"\\"\\"\\n    print(f\\"Thread-{task_id} started\\")\\n    time.sleep(duration)\\n    print(f\\"Thread-{task_id} finished\\")\\n    print\\n# execute\\nstart_time = time.time()   \\nworker(1,1)\\nend_time = time.time()\\nprint(f\\"Total execution time: {end_time - start_time:.2f} seconds\\")\\n```\\n\\n**sequential_worker.py**\\n```python\\nimport time\\n\\ndef worker(task_id, duration):\\n\\n    \\"\\"\\"Simulate work by sleeping for `duration` seconds.\\"\\"\\"\\n    print(f\\"Thread-{task_id} started\\")\\n    time.sleep(duration)\\n    print(f\\"Thread-{task_id} finished\\")\\n    print\\n# execute\\nstart_time = time.time()   \\nworker(1,1)\\n# duplicate\\nworker(1,1)\\nend_time = time.time()\\nprint(f\\"Total execution time: {end_time - start_time:.2f} seconds\\")\\n```\\n\\nRun and test the time to executes\\n```bash\\npython3 single_worker.py\\npython3 sequential_worker.py\\n```\\n\\n![Simple program](./simple.png)\\n\\n**Figure 2:** Execute the simple programs\\n\\n:::info\\n+ For these functions, it is bad practice run them sequentially. You may automatically to use the `loop`, it reduces number of lines in script,\\nbut it takes your times while you can do it **much more better**\\n:::\\n\\n### Multiple threads\\n+ Create the file with content below, name it with `multiple_thread.py`\\n```python\\nimport sys\\nimport threading\\nimport time\\n\\ndef worker(task_id, duration):\\n    \\"\\"\\"Simulate work by sleeping for `duration` seconds.\\"\\"\\"\\n    print(f\\"Thread-{task_id} started\\")\\n    time.sleep(duration)\\n    print(f\\"Thread-{task_id} finished\\")\\n\\ndef run_threads(num_threads, duration):\\n    \\"\\"\\"Create and start multiple threads.\\"\\"\\"\\n    threads = []\\n    \\n    start_time = time.time()\\n    \\n    for i in range(num_threads):\\n        thread = threading.Thread(target=worker, args=(i, duration))\\n        threads.append(thread)\\n        thread.start()\\n    \\n    for thread in threads:\\n        thread.join()  # Wait for all threads to finish\\n    \\n    end_time = time.time()\\n    print(f\\"Total execution time: {end_time - start_time:.2f} seconds\\")\\n\\nif __name__ == \\"__main__\\":   \\n    run_threads(sys.argv[1], 1)\\n```\\n+ Run with threads\\n```bash\\npython3 multiple_thread.py <number of threads>\\n\\n```\\n![2 threads](./thread.png)\\n\\n**Figure 3:** Multiple threads that you don\'t need to wait them sequentially\\n\\n\\n### Number of cpus and number of threads ?\\n+ It\u2019s useful to see how multiple threads can improve execution time, but the performance gain does not depend on the number of CPUs.\\nI ran the program with 1000 threads, and the results showed that for non-CPU-bound tasks, using many threads is fine\u2014but excessive threads should be used with caution.\\n\\n+ Even with more threads, they still need to share resources like internet bandwidth, and switching between threads adds overhead.\\nAs a result, the execution time was 1.23s instead of 1s, showing that too many threads can slow things down.\\n\\n+ In general, for CPU-bound tasks, the number of threads should match the number of CPUs.\\nHowever, for I/O-bound tasks like network requests, you can experiment with more threads to maximize efficiency.\\nInstead of waiting 1000 seconds, the task completed in 1.23s\u2014a significant speedup! \ud83d\ude80\\n\\n![1000 threads](./1k_threads.png)\\n\\n**Figure 3:** 1000 threads in 1.23s\\n\\n\\n### Bump! When you use threads with cpus tasks ?\\nIt does not reduces time to execute. Because it requires process to use a cpu to compute. Overall, it should not be used in cpu-bound tasks.\\nCreate a file with content as below, name it with `cpu_thread.py`.\\n:::warning\\nAvoid using threads for CPU-bound tasks, as they often increase execution time without providing linear performance improvements.\\n:::\\n```python\\nimport threading\\nimport time\\nimport sys\\n\\ndef cpu_task(n):\\n    \\"\\"\\"Simulate a CPU-bound task by performing heavy computations.\\"\\"\\"\\n    print(f\\"Thread-{n} started\\")\\n    count = 0\\n    for _ in range(10**7):  # Simulate CPU-intensive work\\n        count += 1\\n    print(f\\"Thread-{n} finished\\")\\n\\ndef run_threads(num_threads):\\n    \\"\\"\\"Create and start multiple CPU-bound threads.\\"\\"\\"\\n    threads = []\\n    \\n    start_time = time.time()\\n    \\n    for i in range(num_threads):\\n        thread = threading.Thread(target=cpu_task, args=(i,))\\n        threads.append(thread)\\n        thread.start()\\n    \\n    for thread in threads:\\n        thread.join()  # Wait for all threads to finish\\n    \\n    end_time = time.time()\\n    print(f\\"Total execution time: {end_time - start_time:.2f} seconds\\")\\n\\nif __name__ == \\"__main__\\":\\n    run_threads(int(sys.argv[1]))\\n```\\n\\n![cpu thread](./cpu_thread.png)\\n\\n**Figure 4:** Use threads are crazy in python with cpu bound tasks"},{"id":"single-node-slurm-setup","metadata":{"permalink":"/river-docs/blog/single-node-slurm-setup","source":"@site/blog/2025-03-14/how-to-build-slurm-single-node-with-full-functions.md","title":"Single Node Slurm Setup","description":"cluster","date":"2025-03-14T00:00:00.000Z","tags":[{"inline":true,"label":"slurm","permalink":"/river-docs/blog/tags/slurm"},{"inline":true,"label":"hpc","permalink":"/river-docs/blog/tags/hpc"}],"readingTime":5.4,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang (River) Tan Nguyen","title":"Software and bioinformatics engineer","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"single-node-slurm-setup","title":"Single Node Slurm Setup","authors":["river"],"tags":["slurm","hpc"]},"unlisted":false,"prevItem":{"title":"Python thread: deep dive","permalink":"/river-docs/blog/python-thread"},"nextItem":{"title":"Welcome to RiverXData","permalink":"/river-docs/blog/welcome-to-riverxdata"}},"content":"![cluster](./img/Slurm_logo.svg)\\n\\nSLURM (Simple Linux Utility for Resource Management) is a powerful open-source workload manager commonly used in high-performance computing (HPC) environments. While it is typically deployed across multi-node clusters, setting up SLURM on a single node can be highly beneficial for testing, development, or running lightweight workloads. This guide will help you understand the fundamental concepts of how a scheduler operates and provide a step-by-step walkthrough to configure SLURM with full functionality on a single-node setup.\\n\\n\x3c!-- truncate --\x3e\\n\\n**How to Set Up a Fully Functional SLURM Cluster on a Single Node (with Proper Resource Constraints)**\\nSLURM is a powerful job scheduler widely used in HPC environments, but configuring it properly\u2014especially on a single-node cluster\u2014can be tricky. Many tutorials cover the basics, like installing SLURM and running jobs, but they often overlook a critical aspect: cgroups (control groups).\\n\\nWithout proper cgroup configuration, SLURM may fail to enforce CPU and memory constraints, allowing jobs to consume more resources than allocated. This can lead to performance degradation, system crashes, or unexpected behavior.\\n\\nIn this guide, I\u2019ll walk you through setting up a fully functional SLURM cluster on a single node, ensuring that CPU and memory limits are properly enforced using cgroups. Whether you\'re setting up a test environment or a lightweight HPC system, this tutorial will help you avoid common pitfalls and ensure that SLURM effectively manages resources as expected.\\n\\nLet\u2019s dive in\\n\\n:::info\\n- This architecture is designed for a single node running Ubuntu 20.04.\\n- It supports all standard Slurm features.\\n- The setup is manual to help you understand how Slurm works. Note that users can utilize resources without submitting jobs, so this configuration is not recommended for production environments.\\n:::\\n\\n## **Install `slurmd` and `slurmctld`**\\n\\nInstall the required software:\\n\\n```bash\\nsudo apt-get update -y && sudo apt-get install -y slurmd slurmctld\\n```\\n\\nVerify the installation:\\n\\n```bash\\n# Locate slurmd and slurmctld\\nwhich slurmd\\n# Output: /usr/sbin/slurmd\\nwhich slurmctld\\n# Output: /usr/sbin/slurmctld\\n```\\n\\n## **Prepare `slurm.conf`**\\n\\n:::info\\n- This configuration applies to all nodes.\\n:::\\n\\nCreate the `slurm.conf` file:\\n\\n```bash\\ncat <<EOF > slurm.conf\\n# slurm.conf for a single-node Slurm cluster with accounting\\nClusterName=localcluster\\nSlurmctldHost=localhost\\nMpiDefault=none\\nProctrackType=proctrack/linuxproc\\nReturnToService=2\\nSlurmctldPidFile=/run/slurmctld.pid\\nSlurmctldPort=6817\\nSlurmdPidFile=/run/slurmd.pid\\nSlurmdPort=6818\\nSlurmdSpoolDir=/var/lib/slurm-llnl/slurmd\\nSlurmUser=slurm\\nStateSaveLocation=/var/lib/slurm-llnl/slurmctld\\nSwitchType=switch/none\\nTaskPlugin=task/none\\n\\n# TIMERS\\nInactiveLimit=0\\nKillWait=30\\nMinJobAge=300\\nSlurmctldTimeout=120\\nSlurmdTimeout=300\\nWaittime=0\\n\\n# SCHEDULING\\nSchedulerType=sched/backfill\\nSelectType=select/cons_tres\\nSelectTypeParameters=CR_Core\\n\\n# ACCOUNTING (slurmdbd, not enabled now)\\nAccountingStorageType=accounting_storage/none\\nJobAcctGatherType=jobacct_gather/none\\nJobAcctGatherFrequency=30\\n\\n# LOGGING\\nSlurmctldDebug=info\\nSlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log\\nSlurmdDebug=info\\nSlurmdLogFile=/var/log/slurm-llnl/slurmd.log\\n\\n# COMPUTE NODES (Single-node configuration)\\nNodeName=localhost CPUs=2 Sockets=1 CoresPerSocket=2 ThreadsPerCore=1 RealMemory=1024 State=UNKNOWN\\n\\n# PARTITION CONFIGURATION\\nPartitionName=LocalQ Nodes=ALL Default=YES MaxTime=INFINITE State=UP\\nEOF\\n```\\n\\nMove the file to the correct location:\\n\\n```bash\\nsudo mv slurm.conf /etc/slurm-llnl/slurm.conf\\n```\\n\\n## **Start Basic Slurm Services**\\n\\nStart the `slurmd` service:\\n\\n```bash\\n# Start the service\\nsudo service slurmd start\\n# Check its status\\nsudo service slurmd status\\n```\\n\\n<figure markdown=\\"span\\">\\n    ![cluster](./img/slurmd_status.png)\\n</figure>\\n\\nStart the `slurmctld` service:\\n\\n```bash\\n# Start the service\\nsudo service slurmctld start\\n# Check its status\\nsudo service slurmctld status\\n```\\n\\n<figure markdown=\\"span\\">\\n    ![cluster](./img/slurmctld_status.png)\\n</figure>\\n\\nSubmit a small job (adjust CPUs and memory as needed):\\n\\n```bash\\nsrun --mem 500MB -c 1 --pty bash\\n# Check details of submitted jobs\\nsqueue -o \\"%i %P %u %T %M %l %D %C %m %R %Z %N\\" | column -t\\n```\\n\\nBefore submitting the job, memory usage is less than 200MB:\\n\\n<figure markdown=\\"span\\">\\n    ![cluster](./img/memory_before_stress.png)\\n</figure>\\n\\nAllocate 100MB of memory repeatedly:\\n\\n```bash\\ndeclare -a mem\\ni=0\\n\\nwhile :; do\\n    mem[$i]=$(head -c 100M </dev/zero | tr \'\\\\000\' \'x\') \\n    ((i++))\\n    echo \\"Allocated: $((i * 100)) MB\\"\\ndone\\n```\\n\\nAfter allocating 1GB, the job is not killed due to missing control group (cgroup) configuration:\\n\\n<figure markdown=\\"span\\">\\n    ![cluster](./img/overresource_limit.png)\\n</figure>\\n\\n## **Limit Resources Using cgroups**\\n\\nCreate a `cgroup.conf` file to restrict resource usage:\\n\\n```bash\\ncat <<EOF >cgroup.conf\\nCgroupAutomount=yes\\nCgroupMountpoint=/sys/fs/cgroup\\nConstrainCores=yes\\nConstrainRAMSpace=yes\\nConstrainDevices=yes\\nConstrainSwapSpace=yes\\nMaxSwapPercent=5\\nMemorySwappiness=0\\nEOF\\n```\\n\\nMove the file to the correct directory:\\n\\n```bash\\nsudo mv cgroup.conf /etc/slurm-llnl/cgroup.conf\\n```\\n\\nUpdate `slurm.conf` to enable cgroup plugins:\\n\\n```bash\\nsudo sed -i -e \\"s|ProctrackType=proctrack/linuxproc|ProctrackType=proctrack/cgroup|\\" \\\\\\n            -e \\"s|TaskPlugin=task/none|TaskPlugin=task/cgroup|\\" /etc/slurm-llnl/slurm.conf\\n```\\n\\nEnable cgroup in GRUB and reboot:\\n\\n```bash\\nsudo sed -i \'s/^GRUB_CMDLINE_LINUX=\\"/GRUB_CMDLINE_LINUX=\\"cgroup_enable=memory swapaccount=1 /\' /etc/default/grub\\nsudo update-grub\\nsudo reboot\\n```\\n\\nRestart Slurm services:\\n\\n```bash\\nsudo service slurmctld restart\\nsudo service slurmd restart\\n```\\n\\nRerun the memory allocation job. This time, the job will be killed when it exceeds the memory limit:\\n\\n```bash\\nsrun --mem 500MB -c 1 --pty bash\\ndeclare -a mem\\ni=0\\n\\nwhile :; do\\n    mem[$i]=$(head -c 100M </dev/zero | tr \'\\\\000\' \'x\') \\n    ((i++))\\n    echo \\"Allocated: $((i * 100)) MB\\"\\ndone\\n```\\n\\n<figure markdown=\\"span\\">\\n    ![cluster](./img/oom.png)\\n</figure>\\n\\n## **Enable Slurm Accounting**\\n\\nAccounting allows monitoring of jobs, resource allocation, and permissions.\\n\\n<figure markdown=\\"span\\">\\n    ![cluster](./img/sacct_disable.png)\\n</figure>\\n\\n### Install `slurmdbd`\\n\\n```bash\\nsudo apt-get install slurmdbd mariadb-server -y\\n```\\n\\n### Configure `slurmdbd.conf`\\n\\n:::info\\n- Enables the accounting plugin to store account information.\\n- Maps Linux users to Slurm accounts. Users cannot submit jobs without being added.\\n- Useful for monitoring jobs and optimizing resource usage.\\n:::\\n\\nCreate the `slurmdbd.conf` file:\\n\\n```bash\\ncat <<EOF >slurmdbd.conf\\nPidFile=/run/slurmdbd.pid\\nLogFile=/var/log/slurm/slurmdbd.log\\nDebugLevel=error\\nDbdHost=localhost\\nDbdPort=6819\\n\\n# DB connection data\\nStorageType=accounting_storage/mysql\\nStorageHost=localhost\\nStoragePort=3306\\nStorageUser=slurm\\nStoragePass=slurm\\nStorageLoc=slurm_acct_db\\nSlurmUser=slurm\\nEOF\\n```\\n\\nMove the file to the correct location:\\n\\n```bash\\nsudo mv slurmdbd.conf /etc/slurm-llnl/slurmdbd.conf\\n```\\n\\n### Create the Database\\n\\nCreate the database and user:\\n\\n```bash\\nsudo service mysql start\\nsudo mysql -e \\"CREATE DATABASE slurm_acct_db;\\" && \\\\\\nsudo mysql -e \\"CREATE USER \'slurm\'@\'localhost\' IDENTIFIED BY \'slurm\';\\" && \\\\\\nsudo mysql -e \\"GRANT ALL PRIVILEGES ON slurm_acct_db.* TO \'slurm\'@\'localhost\';\\" && \\\\\\nsudo mysql -e \\"FLUSH PRIVILEGES;\\"\\n```\\n\\nVerify the database and user:\\n\\n```bash\\nsudo mysql -e \\"SHOW DATABASES;\\" \\nsudo mysql -e \\"SELECT User, Host FROM mysql.user;\\"\\nsudo mysql -e \\"SHOW GRANTS FOR \'slurm\'@\'localhost\';\\"\\n```\\n\\n<figure markdown=\\"span\\">\\n    ![cluster](./img/add_db.png)\\n</figure>\\n\\n### Start `slurmdbd` Service\\n\\n```bash\\nsudo service slurmdbd start\\n```\\n\\nUpdate `slurm.conf` to enable accounting:\\n\\n```bash\\nsudo sed -i -e \\"s|AccountingStorageType=accounting_storage/none|AccountingStorageType=accounting_storage/slurmdbd\\\\nAccountingStorageEnforce=associations,limits,qos\\\\nAccountingStorageHost=localhost\\\\nAccountingStoragePort=6819|\\" /etc/slurm-llnl/slurm.conf \\nsudo sed -i -e \\"s|JobAcctGatherType=jobacct_gather/none|JobAcctGatherType=jobacct_gather/cgroup|\\" /etc/slurm-llnl/slurm.conf\\nsudo systemctl restart slurmctl slurmd\\n```\\n\\nAdd Linux users to Slurm accounting:\\n\\n```bash\\nsudo sacctmgr -i add cluster localcluster\\nsudo sacctmgr -i --quiet add account $USER Cluster=localcluster\\nsudo sacctmgr -i --quiet add user $USER account=$USER DefaultAccount=root\\nsudo systemctl restart slurmctl slurmd\\n```\\n\\n<figure markdown=\\"span\\">\\n    ![cluster](./img/add_account.png)\\n</figure>\\n\\n### Submit a Job and View Metrics\\n\\nSubmit a job:\\n\\n```bash\\nsrun --mem 500MB -c 1 --pty bash\\n```\\n\\n<figure markdown=\\"span\\">\\n    ![cluster](./img/add_account.png)\\n</figure>\\n\\n## **Conclusion**\\n\\n:::info\\n- Slurm is widely used in academic and industrial settings for orchestrating distributed jobs across multiple nodes.\\n- While Slurm is relatively easy to set up, critical steps like resource limits and accounting are often overlooked.\\n- Slurm integrates seamlessly with distributed computing frameworks like Spark, Ray, Dask, and Flink, enabling efficient resource utilization for local development.\\n:::"},{"id":"welcome-to-riverxdata","metadata":{"permalink":"/river-docs/blog/welcome-to-riverxdata","source":"@site/blog/2025-01-08-welcome/2025-01-08-index.md","title":"Welcome to RiverXData","description":"Unlock the Power of Cloud-Based Data Analysis with RiverXData","date":"2025-01-08T00:00:00.000Z","tags":[{"inline":true,"label":"cloud","permalink":"/river-docs/blog/tags/cloud"},{"inline":true,"label":"data-analysis","permalink":"/river-docs/blog/tags/data-analysis"},{"inline":true,"label":"slurm","permalink":"/river-docs/blog/tags/slurm"},{"inline":true,"label":"hpc","permalink":"/river-docs/blog/tags/hpc"},{"inline":true,"label":"web-platform","permalink":"/river-docs/blog/tags/web-platform"}],"readingTime":0.975,"hasTruncateMarker":true,"authors":[{"name":"Thanh-Giang (River) Tan Nguyen","title":"Software and bioinformatics engineer","url":"https://www.facebook.com/nttg8100","page":{"permalink":"/river-docs/blog/authors/river"},"email":"nttg8100@gmail.com","socials":{"linkedin":"https://www.linkedin.com/in/thanh-giang-tan-nguyen-761b28190/","github":"https://github.com/nttg8100"},"imageURL":"https://avatars.githubusercontent.com/u/64969412?v=4","key":"river"}],"frontMatter":{"slug":"welcome-to-riverxdata","title":"Welcome to RiverXData","authors":["river"],"tags":["cloud","data-analysis","slurm","hpc","web-platform"]},"unlisted":false,"prevItem":{"title":"Single Node Slurm Setup","permalink":"/river-docs/blog/single-node-slurm-setup"}},"content":"## Unlock the Power of Cloud-Based Data Analysis with RiverXData\\n![RiverXData](./riverxdata.svg)\\n\\nWelcome to **RiverXData**, a data platform designed for scalable and efficient data analysis. Built on top of **SLURM**, RiverXData brings the power of high-performance computing (HPC) to a user-friendly web-based interface, enabling researchers, engineers, and data scientists to run complex computational tasks with ease.\\n\\n\x3c!-- truncate --\x3e\\n\\n### Why RiverXData?\\n\\nModern data analysis requires powerful computation, but traditional HPC environments can be challenging to access and manage. RiverXData simplifies this by providing:\\n\\n- **Seamless Web-Based Access**: Run jobs, manage workloads, and analyze results all from a web browser\u2014no command-line expertise required.\\n- **Scalability & Performance**: Harness the power of SLURM for efficient job scheduling and resource management.\\n- **Cloud Flexibility**: Deploy, scale, and optimize computational tasks without worrying about infrastructure.\\n- **User-Friendly Interface**: Modern web based job in slurm can be accessed interactively\\n\\n### Get Started Today\\n\\nWhether you\'re analyzing large datasets, running simulations, or training machine learning models, RiverXData empowers you to leverage the full potential of cloud-based HPC with minimal setup. \\n\\nStay tuned for tutorials, feature updates, and real-world use cases as we continue to improve RiverXData.\\n\\n\ud83d\udd17 **Explore more at [RiverXData](#)**"}]}}')}}]);